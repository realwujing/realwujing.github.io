---
title: 'å¤§æ¨¡å‹ä»0åˆ°1ï½œç¬¬åå››è¯¾ï¼šå®æˆ˜æ•°æ®è¿‡æ»¤å’Œå»é‡'
date: '2025/12/14 16:19:09'
updated: '2025/12/14 16:19:09'
---

# å¤§æ¨¡å‹ä»0åˆ°1ï½œç¬¬åå››è¯¾ï¼šå®æˆ˜æ•°æ®è¿‡æ»¤å’Œå»é‡

> è¯¾ç¨‹é“¾æ¥ï¼š[Stanford CS336 Spring 2025 - Lecture 14](https://stanford-cs336.github.io/spring2025-lectures/?trace=var/traces/lecture_14.json)

ä¸Šä¸€è®²ï¼šè®­ç»ƒæ•°æ®ç­–ç•¥
æœ¬è®²ï¼šæ·±å…¥æ¢è®¨æœºåˆ¶

## 1. æ¦‚è§ˆ (Overview)

ä¸Šä¸€è®²ï¼šç”¨äºè®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ•°æ®é›†æ¦‚è§ˆ
*   åœ¨çº¿æœåŠ¡ (GitHub) â†’ è½¬å‚¨/æŠ“å– (GH Archive) â†’ å¤„ç†åçš„æ•°æ® (The Stack)
*   å¤„ç†ï¼šHTML è½¬æ–‡æœ¬ï¼Œè¯­è¨€/è´¨é‡/æ¯’æ€§è¿‡æ»¤ï¼Œå»é‡

æœ¬è®²ï¼šæ·±å…¥æ¢è®¨æœºåˆ¶
*   è¿‡æ»¤ç®—æ³•ï¼ˆä¾‹å¦‚ï¼šåˆ†ç±»å™¨ï¼‰
*   è¿‡æ»¤çš„åº”ç”¨ï¼ˆä¾‹å¦‚ï¼šè¯­è¨€ï¼Œè´¨é‡ï¼Œæ¯’æ€§ï¼‰
*   å»é‡ï¼ˆä¾‹å¦‚ï¼šBloom filters, MinHash, LSHï¼‰

## 2. è¿‡æ»¤ç®—æ³• (Filtering algorithms)

**ç®—æ³•æ„å»ºå—ï¼š**
*   ç»™å®šä¸€äº› **ç›®æ ‡æ•°æ®** T å’Œå¤§é‡ **åŸå§‹æ•°æ®** Rï¼Œæ‰¾åˆ° R ä¸­ä¸ T ç›¸ä¼¼çš„å­é›† T'ã€‚

![Raw target schema](https://stanford-cs336.github.io/spring2025-lectures/images/raw-target-schema.png)

**è¿‡æ»¤ç®—æ³•çš„æœŸæœ›ï¼š**
*   ä»ç›®æ ‡æ•°æ®æ³›åŒ–ï¼ˆå¸Œæœ› T å’Œ T' ä¸åŒï¼‰
*   æå¿«ï¼ˆå¿…é¡»åœ¨å·¨å¤§çš„ R ä¸Šè¿è¡Œï¼‰

### KenLM
**å¸¦ Kneser-Ney å¹³æ»‘çš„ n-gram æ¨¡å‹** [Article](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing)
*   KenLMï¼šæœ€åˆç”¨äºæœºå™¨ç¿»è¯‘çš„å¿«é€Ÿå®ç° [code](https://kheafield.com/code/kenlm/)
*   ç”¨äºæ•°æ®è¿‡æ»¤çš„é€šç”¨è¯­è¨€æ¨¡å‹
*   æå…¶ç®€å• / å¿«é€Ÿ - åªæ˜¯è®¡æ•°å’Œå½’ä¸€åŒ–

**æ¦‚å¿µ**
n-gram è¯­è¨€æ¨¡å‹çš„æå¤§ä¼¼ç„¶ä¼°è®¡ï¼š
*   n = 3: p(in | the cat) = count(the cat in) / count(the cat)
é—®é¢˜ï¼šç¨€ç–è®¡æ•°ï¼ˆå¯¹äºå¤§ nï¼Œè®¸å¤š n-grams è®¡æ•°ä¸º 0ï¼‰
è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ Kneser-Ney å¹³æ»‘æ¥å¤„ç†æœªè§è¿‡çš„ n-grams [Article](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing)
*   p(in | the cat) ä¹Ÿä¾èµ–äº p(in | cat)

**ä½¿ç”¨ KenLM è¯­è¨€æ¨¡å‹**
```python
# Download a KenLM language model
model_url = "https://huggingface.co/edugp/kenlm/resolve/main/wikipedia/en.arpa.bin"
model = kenlm.Model(model_path)

def compute(content: str):
    # Hacky preprocessing
    content = "<s> " + content.replace(",", " ,").replace(".", " .") + " </s>"
    # log p(content)
    score = model.score(content)
    # Perplexity normalizes by number of tokens to avoid favoring short documents
    num_tokens = len(list(model.full_scores(content)))
    perplexity = math.exp(-score / num_tokens)
    return score, perplexity

# Examples
compute("Stanford University was founded in 1885...") # Low perplexity (good)
compute("asdf asdf asdf asdf asdf") # High perplexity (bad)
```

**CCNet** [Link](https://arxiv.org/pdf/1911.00359)
*   é¡¹ç›®æ˜¯æ–‡æœ¬æ®µè½
*   æŒ‰å›°æƒ‘åº¦ (perplexity) é€’å¢æ’åºæ®µè½
*   ä¿ç•™å‰ 1/3
*   æ›¾ç”¨äº LLaMA

æ€»ç»“ï¼šKneser-Ney n-gram è¯­è¨€æ¨¡å‹ï¼ˆä½¿ç”¨ KenLM å®ç°ï¼‰å¾ˆå¿«ä½†å¾ˆç²—ç³™ã€‚

### fastText
fastText åˆ†ç±»å™¨ [Link](https://arxiv.org/pdf/1607.01759)
*   ä»»åŠ¡ï¼šæ–‡æœ¬åˆ†ç±»ï¼ˆä¾‹å¦‚æƒ…æ„Ÿåˆ†ç±»ï¼‰
*   ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªç”¨äºæ–‡æœ¬åˆ†ç±»çš„å¿«é€Ÿåˆ†ç±»å™¨
*   ä»–ä»¬å‘ç°å®ƒå’Œæ…¢å¾—å¤šçš„ç¥ç»ç½‘ç»œåˆ†ç±»å™¨ä¸€æ ·å¥½

**åŸºçº¿ï¼šè¯è¢‹ (Baselin: bag of words)ï¼ˆå¹¶éä»–ä»¬æ‰€åšï¼‰**
```python
L = 32                              # Length of input
V = 8192                            # Vocabulary size
K = 64                              # Number of classes
W = nn.Embedding(V, K)              # Embedding parameters (V x K)
x = torch.randint(V, (L,))          # Input tokens (L)
y = softmax(W(x).mean(dim=0))       # Output probabilities (K)
```
é—®é¢˜ï¼šV*K å‚æ•°ï¼ˆå¯èƒ½å·¨å¤§ï¼‰

**fastText åˆ†ç±»å™¨ï¼šè¯åµŒå…¥è¢‹ (Bag of word embeddings)**
```python
H = 16                              # Hidden dimension
W = nn.Embedding(V, H)              # Embedding parameters (V x H)
U = nn.Linear(H, K)                 # Head parameters (H x K)
y = softmax(U(W(x).mean(dim=0)))    # Output probabilities (K)
```
åªæœ‰ H*(V + K) å‚æ•°ã€‚

**å®ç°ï¼š**
*   å¹¶è¡ŒåŒ–ï¼Œå¼‚æ­¥ SGD
*   å­¦ä¹ ç‡ï¼šä» [æŸä¸ªæ•°å­—] çº¿æ€§æ’å€¼åˆ° 0 [Article](https://github.com/facebookresearch/fastText/blob/main/src/fasttext.cc#L653)

**n-grams è¢‹ (Bag of n-grams)**
```python
x = ["the cat", "cat in", "in the", "the hat"]
```
é—®é¢˜ï¼šbigrams çš„æ•°é‡å¯èƒ½å¾ˆå¤§ï¼ˆå¹¶ä¸”ä¹Ÿå¯èƒ½æ˜¯æ— ç•Œçš„ï¼‰
è§£å†³æ–¹æ¡ˆï¼šå“ˆå¸ŒæŠ€å·§ (hashing trick)
```python
num_bins = 10000000 
hashed_x = [hash(bigram) % num_bins for bigram in x]
```

*   å¯¹äºè´¨é‡è¿‡æ»¤ï¼Œæˆ‘ä»¬æœ‰ K = 2 ä¸ªç±»ï¼ˆå¥½ vs åï¼‰
*   åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒfastText åªæ˜¯ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨ (H = K = 2)

ä¸€èˆ¬æ¥è¯´ï¼Œå¯ä»¥ä½¿ç”¨ä»»ä½•åˆ†ç±»å™¨ï¼ˆä¾‹å¦‚ BERT, Llamaï¼‰ï¼Œåªæ˜¯æ›´æ…¢ã€‚

### DSIR
é€šè¿‡é‡è¦æ€§é‡é‡‡æ ·è¿›è¡Œè¯­è¨€æ¨¡å‹æ•°æ®é€‰æ‹© (DSIR) [Link](https://arxiv.org/abs/2302.03169)

![DSIR](https://www.jinghong-chen.net/content/images/size/w1200/2023/12/Screenshot-2023-12-24-at-17.41.38.png)

**è®¾ç½®ï¼š**
*   ç›®æ ‡æ•°æ®é›† D_pï¼ˆå°ï¼‰
*   æè®®ï¼ˆåŸå§‹ï¼‰æ•°æ®é›† D_qï¼ˆå¤§ï¼‰

**å°è¯• 1ï¼š**
*   åœ¨ D_p ä¸Šæ‹Ÿåˆç›®æ ‡åˆ†å¸ƒ p
*   åœ¨ D_q ä¸Šæ‹Ÿåˆæè®®åˆ†å¸ƒ q
*   ä½¿ç”¨ p, q å’ŒåŸå§‹æ ·æœ¬ D_q è¿›è¡Œé‡è¦æ€§é‡é‡‡æ ·
é—®é¢˜ï¼šç›®æ ‡æ•°æ® D_p å¤ªå°ï¼Œæ— æ³•ä¼°è®¡ä¸€ä¸ªå¥½çš„æ¨¡å‹

**å°è¯• 2ï¼šä½¿ç”¨å“ˆå¸Œ n-grams**
```python
# Hash the n-grams
num_bins = 10000
def get_hashed_ngrams(text: str):
    ngrams = text.split(" ") 
    return [hash(ngram) % num_bins for ngram in ngrams]

# Learn unigram model
probs = [count(training_hashed_ngrams, x) / len(training_hashed_ngrams) for x in range(num_bins)]

# Evaluate probability of any sentence
prob = np.prod([probs[x] for x in hashed_ngrams])
```
ç»“æœï¼šåœ¨ [GLUE](https://gluebenchmark.com/) åŸºå‡†æµ‹è¯•ä¸Šï¼ŒDSIR ç•¥ä¼˜äºå¯å‘å¼åˆ†ç±»ï¼ˆfastTextï¼‰

![DSIR results](https://stanford-cs336.github.io/spring2025-lectures/images/dsir-results.png)

**ä¸ fastText çš„æ¯”è¾ƒï¼š**
*   å»ºæ¨¡åˆ†å¸ƒæ˜¯ä¸€ä¸ªæ•æ‰å¤šæ ·æ€§çš„æ›´åŸåˆ™æ€§çš„æ–¹æ³•
*   ç±»ä¼¼çš„è®¡ç®—å¤æ‚åº¦
*   ä¸¤è€…éƒ½å¯ä»¥é€šè¿‡æ›´å¥½çš„å»ºæ¨¡æ¥æ”¹è¿›

### æ€»ç»“
**ä¸€èˆ¬æ¡†æ¶**
ç»™å®šç›®æ ‡ T å’ŒåŸå§‹ Rï¼Œæ‰¾åˆ° R ä¸­ä¸ T ç›¸ä¼¼çš„å­é›†
1.  åŸºäº R å’Œ T ä¼°è®¡æŸä¸ªæ¨¡å‹å¹¶æ¨å¯¼è¯„åˆ†å‡½æ•°
2.  åŸºäºåˆ†æ•°ä¿ç•™ R ä¸­çš„ç¤ºä¾‹

**æ¡†æ¶çš„å®ä¾‹åŒ–**
T çš„ç”Ÿæˆæ¨¡å‹ (KenLM):
1.  score(x) = p_T(x)
2.  éšæœºä¿ç•™ score(x) >= threshold çš„ç¤ºä¾‹ x

åˆ¤åˆ«å¼åˆ†ç±»å™¨ (fastText):
1.  score(x) = p(T | x)
2.  éšæœºä¿ç•™ score(x) >= threshold çš„ç¤ºä¾‹ x

é‡è¦æ€§é‡é‡‡æ · (DSIR):
1.  score(x) = p_T(x) / p_R(x)
2.  ä»¥æ­£æ¯”äº score(x) çš„æ¦‚ç‡é‡é‡‡æ ·ç¤ºä¾‹ x

æ•°æ®é€‰æ‹©è°ƒæŸ¥è®ºæ–‡ [Link](https://arxiv.org/abs/2402.16827)

---

## 3. è¿‡æ»¤åº”ç”¨ (Filtering applications)

åŒæ ·çš„æ•°æ®è¿‡æ»¤æœºåˆ¶å¯ç”¨äºä¸åŒçš„è¿‡æ»¤ä»»åŠ¡ã€‚

### è¯­è¨€è¯†åˆ« (Language identification)
è¯­è¨€è¯†åˆ«ï¼šæŸ¥æ‰¾ç‰¹å®šè¯­è¨€ï¼ˆä¾‹å¦‚è‹±è¯­ï¼‰çš„æ–‡æœ¬

**ä¸ºä»€ä¹ˆä¸ç›´æ¥æå¤šè¯­è¨€ï¼Ÿ**
*   æ•°æ®ï¼šéš¾ä»¥å¯¹ä»»ä½•ç»™å®šè¯­è¨€çš„é«˜è´¨é‡æ•°æ®è¿›è¡Œç­–åˆ’/å¤„ç†
*   è®¡ç®—ï¼šåœ¨è®¡ç®—å—é™çš„åˆ¶åº¦ä¸‹ï¼Œè‡´åŠ›äºä»»ä½•ç»™å®šè¯­è¨€çš„è®¡ç®—/token æ›´å°‘
æ¨¡å‹åœ¨å¤šè¯­è¨€æ€§æ–¹é¢æœ‰æ‰€ä¸åŒï¼š
*   English ä»…å  BLOOMï¼ˆè®­ç»ƒä¸è¶³ï¼‰çš„ 30%ï¼Œè‹±è¯­æ€§èƒ½å—æŸ [Link](https://arxiv.org/pdf/2303.03915)
*   å¤§å¤šæ•°å‰æ²¿æ¨¡å‹ï¼ˆGPT-4, Claude, Gemini, Llama, Qwenï¼‰éƒ½æ˜¯é«˜åº¦å¤šè¯­è¨€çš„ï¼ˆå……åˆ†è®­ç»ƒï¼‰

**fastText è¯­è¨€è¯†åˆ«** [Article](https://fasttext.cc/docs/en/language-identification.html)
*   ç°æˆçš„åˆ†ç±»å™¨
*   æ”¯æŒ 176 ç§è¯­è¨€
*   åœ¨å¤šè¯­è¨€ç½‘ç«™ä¸Šè®­ç»ƒï¼šWikipedia, Tatoebaï¼ˆç¿»è¯‘ç½‘ç«™ï¼‰å’Œ SETimesï¼ˆä¸œå—æ¬§æ–°é—»ï¼‰

**ç¤ºä¾‹ï¼šDolma ä¿ç•™ p(English) >= 0.5 çš„é¡µé¢** [Link](https://arxiv.org/abs/2402.00159)

```python
# Make predictions
model.predict(["The quick brown fox..."])  # English
model.predict(["OMG that movie was ğŸ”¥ğŸ”¥!"])  # Informal English
model.predict(["Auf dem Wasser zu singen"])  # German
model.predict(["for (int i = 0; i < 10; i++)"])  # C++ (may be misclassified?)
```

**æ³¨æ„äº‹é¡¹ï¼š**
*   çŸ­åºåˆ—å›°éš¾
*   ä½èµ„æºè¯­è¨€å›°éš¾
*   å¯èƒ½ä¼šæ„å¤–è¿‡æ»¤æ‰è‹±è¯­æ–¹è¨€
*   ç›¸ä¼¼è¯­è¨€ï¼ˆé©¬æ¥è¯­å’Œå°å°¼è¯­ï¼‰å›°éš¾
*   è¯­ç è½¬æ¢ï¼ˆä¾‹å¦‚è¥¿ç­ç‰™è¯­ + è‹±è¯­ï¼‰å®šä¹‰ä¸æ˜ç¡®

**OpenMathText** [Link](https://arxiv.org/pdf/2310.06786)
*   ç›®æ ‡ï¼šä» CommonCrawl ç­–åˆ’å¤§å‹æ•°å­¦æ–‡æœ¬è¯­æ–™åº“
*   ä½¿ç”¨è§„åˆ™è¿‡æ»¤ï¼ˆä¾‹å¦‚åŒ…å« latex å‘½ä»¤ï¼‰
*   KenLM åœ¨ ProofPile ä¸Šè®­ç»ƒï¼Œä¿ç•™å›°æƒ‘åº¦ < 15000
*   è®­ç»ƒ fastText åˆ†ç±»å™¨é¢„æµ‹æ•°å­¦å†™ä½œï¼Œå¦‚æœæ˜¯æ•°å­¦é˜ˆå€¼ä¸º 0.17ï¼Œå¦‚æœæ— æ•°å­¦åˆ™ä¸º 0.8
ç»“æœï¼šäº§ç”Ÿäº† 14.7B tokensï¼Œç”¨äºè®­ç»ƒ 1.4B æ¨¡å‹ï¼Œå…¶è¡¨ç°ä¼˜äºåœ¨ 20 å€æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹

### è´¨é‡è¿‡æ»¤ (Quality filtering)
*   ä¸€äº›æ•…æ„ä¸ä½¿ç”¨åŸºäºæ¨¡å‹çš„è¿‡æ»¤ (C4, Gopher, RefinedWeb, FineWeb, Dolma)
*   ä¸€äº›ä½¿ç”¨åŸºäºæ¨¡å‹çš„è¿‡æ»¤ (GPT-3, LLaMA, DCLM) [æ­£æˆä¸ºå¸¸æ€]

**GPT-3** [Link](https://arxiv.org/pdf/2005.14165)
*   æ­£ä¾‹ï¼šæ¥è‡ª {Wikipedia, WebText2, Books1, Books2} çš„æ ·æœ¬
*   è´Ÿä¾‹ï¼šæ¥è‡ª CommonCrawl çš„æ ·æœ¬
![Pareto](https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Probability_density_function_of_Pareto_distribution.svg/325px-Probability_density_function_of_Pareto_distribution.svg.png)
åŸºäºå•è¯ç‰¹å¾è®­ç»ƒçº¿æ€§åˆ†ç±»å™¨ [Article](https://spark.apache.org/docs/latest/ml-features#tokenizer)
æ ¹æ®åˆ†æ•°éšæœºä¿ç•™æ–‡æ¡£ï¼š
```python
def keep_document(score: float) -> bool:
    return np.random.pareto(9) > 1 - score
```

**LLaMA/RedPajama** [Link](https://arxiv.org/pdf/2302.13971)
*   æ­£ä¾‹ï¼šWikipedia **å¼•ç”¨**é¡µé¢çš„æ ·æœ¬
*   è´Ÿä¾‹ï¼šCommonCrawl çš„æ ·æœ¬
*   ä¿ç•™å½’ç±»ä¸ºæ­£çš„æ–‡æ¡£

**phi-1** [Link](https://arxiv.org/pdf/2306.11644)
å“²å­¦ï¼šéå¸¸é«˜è´¨é‡çš„æ•°æ®ï¼ˆæ•™ç§‘ä¹¦ï¼‰æ¥è®­ç»ƒå°æ¨¡å‹ (1.5B)
åŒ…æ‹¬æ¥è‡ª GPT 3.5 çš„åˆæˆæ•°æ®ï¼ˆåæ¥ï¼šGPT-4ï¼‰å’Œè¿‡æ»¤æ•°æ®

*   R = "The Stack çš„ Python å­é›†" # åŸå§‹æ•°æ®
*   prompt = "determine its educational value..."
*   T = "ä½¿ç”¨ GPT-4 å¯¹ 100K R å­é›†è¿›è¡Œåˆ†ç±»ä»¥è·å¾—æ­£ä¾‹"
*   ä½¿ç”¨é¢„è®­ç»ƒ codegen æ¨¡å‹çš„è¾“å‡ºåµŒå…¥åœ¨ T ä¸Šè®­ç»ƒéšæœºæ£®æ—åˆ†ç±»å™¨
*   ä» R ä¸­é€‰æ‹©è¢«åˆ†ç±»å™¨åˆ†ç±»ä¸ºæ­£çš„æ•°æ®

[HumanEval](https://huggingface.co/datasets/openai_humaneval) ç»“æœï¼š
*   åœ¨ The Stack çš„ Python å­é›†ä¸Šè®­ç»ƒ 1.3B LMï¼ˆæ€§èƒ½ï¼š96K æ­¥å 12.19%ï¼‰
*   åœ¨æ–°çš„è¿‡æ»¤å­é›†ä¸Šè®­ç»ƒ 1.3B LMï¼ˆæ€§èƒ½ï¼š36K æ­¥å 17.68%ï¼‰ - æ›´å¥½ï¼

### æ¯’æ€§è¿‡æ»¤ (Toxicity filtering)
**è­¦å‘Šï¼šä»¥ä¸‹å¯èƒ½åŒ…å«å†’çŠ¯æ€§å†…å®¹**

**Dolma ä¸­çš„æ¯’æ€§è¿‡æ»¤** [Link](https://arxiv.org/abs/2402.00159)
æ•°æ®é›†ï¼šJigsaw æœ‰æ¯’è¯„è®ºæ•°æ®é›† (2018) [dataset](https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge)
*   é¡¹ç›®ç›®æ ‡ï¼šå¸®åŠ©äººä»¬åœ¨ç½‘ä¸Šè¿›è¡Œæ›´å¥½çš„è®¨è®º [Article](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/discussion/46064)
*   æ•°æ®ï¼šWikipedia è®¨è®ºé¡µä¸Šçš„è¯„è®ºï¼Œæ ‡æ³¨ä¸º {toxic, severe_toxic, obscene, threat, insult, identity_hate}

è®­ç»ƒäº† 2 ä¸ª fastText åˆ†ç±»å™¨ï¼š
*   hate: æ­£ä¾‹ = {unlabeled, obscene}, è´Ÿä¾‹ = å…¶ä»–æ‰€æœ‰
*   NSFW: æ­£ä¾‹ = {obscene}, è´Ÿä¾‹ = å…¶ä»–æ‰€æœ‰

```python
# Examples
train_examples = [
    Example(label=0, text="Are you threatening me...?"),
    Example(label=1, text="Stupid peace of shit..."),
]
model.predict(["I love strawberries"]) # OK
model.predict(["I hate strawberries"]) # OK
```

---

## 4. å»é‡ (Deduplication)

**ä¸¤ç§ç±»å‹çš„é‡å¤ï¼š**
*   ç²¾ç¡®é‡å¤ï¼ˆé•œåƒç«™ç‚¹ï¼ŒGitHub forksï¼‰ [Gutenberg mirrors](https://www.gutenberg.org/MIRRORS.ALL)
*   è¿‘ä¼¼é‡å¤ï¼šåªæœ‰å‡ ä¸ª tokens ä¸åŒçš„ç›¸åŒæ–‡æœ¬

**è¿‘ä¼¼é‡å¤çš„ä¾‹å­ï¼š**
*   æœåŠ¡æ¡æ¬¾å’Œè®¸å¯è¯ [MIT license](https://opensource.org/license/mit)
*   å…¬å¼åŒ–å†™ä½œï¼ˆå¤åˆ¶/ç²˜è´´æˆ–ä»æ¨¡æ¿ç”Ÿæˆï¼‰
![Template](https://d3i71xaburhd42.cloudfront.net/4566c0d22ebf3c31180066ab23b6c445aeec78d5/5-Table1-1.png)
*   å¤åˆ¶/ç²˜è´´ä¸­çš„å¾®å°æ ¼å¼å·®å¼‚

**åœ¨ C4 ä¸­ï¼Œäº§å“æè¿°é‡å¤äº† 61,036 æ¬¡ï¼š**
â€œby combining fantastic ideas, interesting arrangements...â€
[Example page](https://www.amazon.co.uk/suryagede-100-Graffiti-Gas-Mask/dp/B07CRHT3RG)

**å»é‡è®­ç»ƒæ•°æ®ä½¿è¯­è¨€æ¨¡å‹æ›´å¥½** [Link](https://arxiv.org/pdf/2107.06499)
*   è®­ç»ƒæ›´æœ‰æ•ˆç‡ï¼ˆå› ä¸º tokens æ›´å°‘ï¼‰
*   é¿å…æ­»è®°ç¡¬èƒŒï¼ˆå¯ä»¥å‡è½»ç‰ˆæƒã€éšç§é—®é¢˜ï¼‰

**è®¾è®¡ç©ºé—´ï¼š**
1.  ä»€ä¹ˆæ˜¯é¡¹ç›®ï¼ˆå¥å­ï¼Œæ®µè½ï¼Œæ–‡æ¡£ï¼‰ï¼Ÿ
2.  å¦‚ä½•åŒ¹é…ï¼ˆç²¾ç¡®åŒ¹é…ï¼Œå­˜åœ¨å…¬å…±å­é¡¹ï¼Œå…¬å…±å­é¡¹çš„æ¯”ä¾‹ï¼‰ï¼Ÿ
3.  é‡‡å–ä»€ä¹ˆè¡ŒåŠ¨ï¼ˆå…¨éƒ¨åˆ é™¤ï¼Œé™¤ä¸€ä¸ªå¤–å…¨éƒ¨åˆ é™¤ï¼‰ï¼Ÿ

**å…³é”®æŒ‘æˆ˜ï¼š**
*   å»é‡æ ¹æœ¬ä¸Šæ˜¯å…³äºå°†é¡¹ç›®ä¸å…¶ä»–é¡¹ç›®è¿›è¡Œæ¯”è¾ƒ
*   éœ€è¦çº¿æ€§æ—¶é—´ç®—æ³•æ¥æ‰©å±•

### å“ˆå¸Œå‡½æ•° (Hash functions)
*   å“ˆå¸Œå‡½æ•° h å°†é¡¹ç›®æ˜ å°„åˆ°å“ˆå¸Œå€¼ï¼ˆæ•´æ•°æˆ–å­—ç¬¦ä¸²ï¼‰
*   å“ˆå¸Œå€¼æ¯”é¡¹ç›®å°å¾—å¤š
*   å“ˆå¸Œå†²çªï¼šå¯¹äº x â‰  yï¼Œh(x) = h(y)

**æ•ˆç‡ä¸æŠ—å†²çªæ€§ä¹‹é—´çš„æƒè¡¡** [Article](https://softwareengineering.stackexchange.com/questions/49550/which-hashing-algorithm-is-best-for-uniqueness-and-speed)
*   åŠ å¯†å“ˆå¸Œå‡½æ•° (SHA-256)ï¼šæŠ—å†²çªï¼Œæ…¢ï¼ˆç”¨äºæ¯”ç‰¹å¸ï¼‰
*   DJB2, MurmurHash, CityHash: ä¸æŠ—å†²çªï¼Œå¿«ï¼ˆç”¨äºå“ˆå¸Œè¡¨ï¼‰
*   æˆ‘ä»¬å°†ä½¿ç”¨ MurmurHash

### ç²¾ç¡®å»é‡ (Exact deduplication)
**ç®€å•ä¾‹å­**
1.  é¡¹ç›®ï¼šå­—ç¬¦ä¸²
2.  å¦‚ä½•åŒ¹é…ï¼šç²¾ç¡®åŒ¹é…
3.  æ“ä½œï¼šé™¤ä¸€ä¸ªå¤–å…¨éƒ¨åˆ é™¤

```python
items = ["Hello!", "hello", "hello there", "hello", "hi", "bye"]
# Group by hash
hash_items = itertools.groupby(sorted(items, key=mmh3.hash), key=mmh3.hash)
# Keep one
deduped_items = [next(group) for h, group in hash_items]
```

*   ä¼˜ç‚¹ï¼šç®€å•ï¼Œè¯­ä¹‰æ¸…æ™°ï¼Œé«˜ç²¾åº¦
*   ç¼ºç‚¹ï¼šä¸å»é‡è¿‘ä¼¼é‡å¤
*   æ­¤ä»£ç æ˜¯ä»¥ MapReduce æ–¹å¼ç¼–å†™çš„ï¼Œå¯ä»¥è½»æ¾å¹¶è¡ŒåŒ–å’Œæ‰©å±•

**C4** [Link](https://arxiv.org/pdf/1910.10683v4)
1.  é¡¹ç›®ï¼š3 å¥è¯çš„è·¨åº¦
2.  å¦‚ä½•åŒ¹é…ï¼šä½¿ç”¨ç²¾ç¡®åŒ¹é…
3.  æ“ä½œï¼šé™¤ä¸€ä¸ªå¤–å…¨éƒ¨åˆ é™¤
è­¦å‘Šï¼šå½“ä»æ–‡æ¡£ä¸­é—´åˆ é™¤ 3 å¥è¯çš„è·¨åº¦æ—¶ï¼Œç”Ÿæˆçš„æ–‡æ¡£å¯èƒ½ä¸è¿è´¯

### Bloom filter
ç›®æ ‡ï¼šç”¨äºæµ‹è¯•é›†åˆæˆå‘˜èµ„æ ¼çš„é«˜æ•ˆã€è¿‘ä¼¼æ•°æ®ç»“æ„

**Bloom filters çš„ç‰¹ç‚¹**
*   å†…å­˜é«˜æ•ˆ
*   å¯ä»¥æ›´æ–°ï¼Œä½†ä¸èƒ½åˆ é™¤
*   å¦‚æœè¿”å› 'no'ï¼Œè‚¯å®šæ˜¯ 'no'
*   å¦‚æœè¿”å› 'yes'ï¼Œå¾ˆå¯èƒ½æ˜¯ 'yes'ï¼Œä½†æœ‰å°æ¦‚ç‡æ˜¯ 'no'
*   å¯ä»¥é€šè¿‡æ›´å¤šçš„æ—¶é—´/è®¡ç®—ä»¥æŒ‡æ•°æ–¹å¼é™ä½å‡é˜³æ€§ç‡

**æ„å»º**
é¦–å…ˆï¼Œä½¿å“ˆå¸Œå‡½æ•°çš„èŒƒå›´å˜å°ï¼ˆå°‘é‡çš„ binsï¼‰ã€‚
é—®é¢˜ï¼šå° bins ä¼šå¯¼è‡´å‡é˜³æ€§ã€‚
æœ´ç´ è§£å†³æ–¹æ¡ˆï¼šå¢åŠ  bins æ•°é‡ã€‚é”™è¯¯æ¦‚ç‡æ˜¯ O(1/num_bins)ã€‚
æ›´å¥½çš„è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æ›´å¤šå“ˆå¸Œå‡½æ•° (k)ã€‚

**å‡é˜³æ€§ç‡åˆ†æ** [Article](https://en.wikipedia.org/wiki/Bloom_filter)
å‡è®¾å“ˆå¸Œå‡½æ•°å’Œé¡¹ç›®ç‹¬ç«‹ã€‚
*   m = bins æ•°é‡
*   k = å“ˆå¸Œå‡½æ•°æ•°é‡
*   n = æ’å…¥çš„é¡¹ç›®æ•°é‡

æœ€ä¼˜ k å€¼ï¼ˆç»™å®šå›ºå®šçš„ m / n æ¯”ç‡ï¼‰ï¼šk = ln(2) * m / n
ç»“æœå‡é˜³æ€§ç‡ï¼šf = 0.5^k
è®¡ç®— (k)ã€å†…å­˜ (m) å’Œå‡é˜³æ€§ç‡ (f) ä¹‹é—´çš„æƒè¡¡ [Lecture notes](https://people.eecs.berkeley.edu/~daw/teaching/cs170-s03/Notes/lecture10.pdf)

**ç¤ºä¾‹ï¼šDolma**
*   è®¾ç½®å‡é˜³æ€§ç‡ä¸º 1e-15
*   åœ¨é¡¹ç›® = æ®µè½ ä¸Šæ‰§è¡Œ

### Jaccard similarity & MinHash
ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹è¿‘ä¼¼é›†åˆæˆå‘˜èµ„æ ¼ã€‚

**Jaccard ç›¸ä¼¼åº¦**
å®šä¹‰ï¼šJaccard(A, B) = |A intersect B| / |A union B|
å®šä¹‰ï¼šå¦‚æœä¸¤ä¸ªæ–‡æ¡£çš„ Jaccard ç›¸ä¼¼åº¦ >= é˜ˆå€¼ï¼Œåˆ™å®ƒä»¬æ˜¯ **è¿‘ä¼¼é‡å¤**ã€‚

ç®—æ³•æŒ‘æˆ˜ï¼šåœ¨çº¿æ€§æ—¶é—´å†…æ‰¾åˆ°è¿‘ä¼¼é‡å¤ã€‚

**MinHash**
MinHash: ä¸€ä¸ªéšæœºå“ˆå¸Œå‡½æ•° hï¼Œä½¿å¾— Pr[h(A) = h(B)] = Jaccard(A, B)
*   é€šå¸¸ï¼Œä½ å¸Œæœ›ä¸åŒçš„é¡¹ç›®å“ˆå¸Œåˆ°ä¸åŒçš„å“ˆå¸Œå€¼
*   ...ä½†åœ¨è¿™é‡Œï¼Œä½ å¸Œæœ›å†²çªæ¦‚ç‡å–å†³äºç›¸ä¼¼åº¦

ç‰¹å¾çŸ©é˜µè¡¨ç¤ºï¼š
```
item | A | B
1    | 1 | 1
2    | 1 | 1
3    | 1 | 1
4    | 1 | 0
5    | 0 | 1
```
éšæœºå“ˆå¸Œå‡½æ•°åœ¨é¡¹ç›®ä¸Šå¼•å…¥ä¸€ä¸ªæ’åˆ—ã€‚
æŸ¥çœ‹ A ä¸­ç¬¬ä¸€ä¸ªé¡¹ç›®å’Œ B ä¸­ç¬¬ä¸€ä¸ªé¡¹ç›®ã€‚
Pr[MinHash(A) == MinHash(B)] = Jaccard(A, B)

### å±€éƒ¨æ•æ„Ÿå“ˆå¸Œ (Locality sensitive hashing)
[Book chapter](http://infolab.stanford.edu/~ullman/mmds/ch3n.pdf)

å‡è®¾æˆ‘ä»¬åªç”¨ä¸€ä¸ª MinHash å‡½æ•°å¯¹ç¤ºä¾‹è¿›è¡Œå“ˆå¸Œã€‚
P[A å’Œ B å†²çª] = Jaccard(A, B)
å¹³å‡è€Œè¨€ï¼Œæ›´ç›¸ä¼¼çš„é¡¹ç›®ä¼šå†²çªï¼Œä½†éå¸¸éšæœº...

ç›®æ ‡ï¼šå¦‚æœ Jaccard(A, B) > é˜ˆå€¼ï¼Œåˆ™è®© A å’Œ B å†²çªã€‚
æˆ‘ä»¬å¿…é¡»ä»¥æŸç§æ–¹å¼é”åŒ–æ¦‚ç‡...

**è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ n ä¸ªå“ˆå¸Œå‡½æ•°**
åˆ†æˆ b ä¸ªå¸¦ (bands)ï¼Œæ¯ä¸ªå¸¦ r ä¸ªå“ˆå¸Œå‡½æ•° (n = b * r)

```
h1 h2 h3 h4  |  h5 h6 h7 h8  |  h9 h10 h11 h12
```

å…³é”®ï¼šå¦‚æœå¯¹äº *æŸä¸ª* å¸¦ï¼Œå…¶ *æ‰€æœ‰* å“ˆå¸Œå‡½æ•°éƒ½è¿”å›ç›¸åŒçš„å€¼ï¼Œåˆ™ A å’Œ B å†²çªã€‚
å¸¦çš„ and-or ç»“æ„é”åŒ–äº†é˜ˆå€¼ã€‚

ç»™å®š Jaccard(A, B)ï¼ŒA å’Œ B å†²çªçš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿ
prob_match = sim^r
prob_collision = 1 - (1 - prob_match)^b

**ç¤ºä¾‹**
![LSH Graph](https://cdn.sanity.io/images/vr8gru94/production/b470799575b8e77911bacb8500977afef06d6c85-1280x720.png)

*   å¢åŠ  r ä¼šé”åŒ–é˜ˆå€¼å¹¶å°†æ›²çº¿å‘å³ç§»åŠ¨ï¼ˆæ›´éš¾åŒ¹é…ï¼‰
*   å¢åŠ  b ä¼šå°†æ›²çº¿å‘å·¦ç§»åŠ¨ï¼ˆæ›´å®¹æ˜“åŒ¹é…ï¼‰

![LSH Curve](https://cdn.sanity.io/images/vr8gru94/production/aace49fa240778e8ecf6e85ad08a2de7f5385566-1280x720.png)

**ç¤ºä¾‹è®¾ç½®** [Link](https://arxiv.org/pdf/2107.06499)
*   n = 9000, b = 20, r = 450
*   é˜ˆå€¼ï¼ˆå‘ç”Ÿç›¸å˜çš„åœ°æ–¹ï¼‰â‰ˆ (1/b)^(1/r)
*   æ¦‚ç‡çº¦ä¸º 1 - 1/e

---

## 5. æ€»ç»“ (Summary)

*   ç®—æ³•å·¥å…·ï¼šn-gram æ¨¡å‹ (KenLM)ï¼Œåˆ†ç±»å™¨ (fastText)ï¼Œé‡è¦æ€§é‡é‡‡æ · (DSIR)
*   åº”ç”¨ï¼šè¯­è¨€è¯†åˆ«ï¼Œè´¨é‡è¿‡æ»¤ï¼Œæ¯’æ€§è¿‡æ»¤
*   å»é‡ï¼šå“ˆå¸Œæ‰©å±•åˆ°ç”¨äºæ¨¡ç³ŠåŒ¹é…çš„å¤§å‹æ•°æ®é›†
*   ç°åœ¨ä½ æœ‰äº†å·¥å…·ï¼ˆæœºåˆ¶ï¼‰ï¼Œåªéœ€è¦èŠ±æ—¶é—´åœ¨æ•°æ®ä¸Šï¼ˆç›´è§‰ï¼‰
