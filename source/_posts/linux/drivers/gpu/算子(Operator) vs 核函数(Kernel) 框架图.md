---
title: '算子(Operator) vs 核函数(Kernel) 框架图'
date: '2025/11/21 18:06:27'
updated: '2025/11/21 18:06:27'
---

# 算子(Operator) vs 核函数(Kernel) 框架图

## 1. 整体架构框架图

```
┌─────────────────────────────────────────────────────────────────────────┐
│                       深度学习框架（如PyTorch）                           │
└─────────────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                           算子(Operator)层                              │
├─────────────────────────────────────────────────────────────────────────┤
│  torch.softmax()   torch.matmul()    torch.conv2d()    nn.Linear()     │
│   用户友好接口        矩阵乘法算子       卷积算子         全连接算子        │
└─────────────────────────────────────────────────────────────────────────┘
                                │
        ┌───────────────────────┼───────────────────────┐
        │                       │                       │
        ▼                       ▼                       ▼
┌───────────────┐       ┌───────────────┐       ┌───────────────┐
│   CPU后端      │       │   GPU后端      │       │  XPU后端       │
│  实现层       │       │  实现层        │       │  实现层        │
└───────────────┘       └───────────────┘       └───────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                         核函数(Kernel)调度层                            │
├─────────────────────────────────────────────────────────────────────────┤
│  kernel启动参数  │  内存管理   │  流管理   │  错误处理   │  性能优化   │
└─────────────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                        硬件执行层（GPU/CPU）                            │
├─────────────────────────────────────────────────────────────────────────┤
│  ████████████████████████████████████████████████████████████████████  │
│  █  SM0  █  SM1  █  SM2  █  SM3  █  ...  █  SMn  █  Warp调度器  █  │
│  ████████████████████████████████████████████████████████████████████  │
│  │       │       │       │       │       │       │               │    │
│  ▼       ▼       ▼       ▼       ▼       ▼       ▼               ▼    │
│┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐         ┌─────┐ │
││Thr  │ │Thr  │ │Thr  │ │Thr  │ │Thr  │ │Thr  │ │Thr  │ ...    │Thr  │ │
││0-31 │ │0-31 │ │0-31 │ │0-31 │ │0-31 │ │0-31 │ │0-31 │         │0-31 │ │
│└─────┘ └─────┘ └─────┘ └─────┘ └─────┘ └─────┘ └─────┘         └─────┘ │
└─────────────────────────────────────────────────────────────────────────┘
```

## 2. Softmax算子的详细执行流程图

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        用户调用：torch.softmax()                        │
└─────────────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                    Softmax算子（Operator层）                           │
├─────────────────────────────────────────────────────────────────────────┤
│ 输入验证 → 维度处理 → 后端选择 → 内存分配 → 调度执行 → 结果返回            │
└─────────────────────────────────────────────────────────────────────────┘
                                │
        ┌───────────────────────┼───────────────────────┐
        │                       │                       │
        ▼                       ▼                       ▼
┌───────────────┐       ┌───────────────┐       ┌───────────────┐
│   CPU路径     │       │   GPU路径     │       │  Triton路径   │
│               │       │               │       │               │
│ 循环实现      │       │  CUDA实现     │       │  Triton实现   │
│ SIMD优化      │       │               │       │               │
└───────────────┘       └───────────────┘       └───────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                  GPU后端：多Kernel协作执行                              │
├─────────────────────────────────────────────────────────────────────────┤
│ 步骤1：max_reduce_kernel   步骤2：sum_exp_kernel   步骤3：softmax_kernel │
│ 计算每行最大值             计算指数和               最终softmax计算      │
│                               │                       │               │
│ grid=(rows,1,1)             grid=(rows,1,1)         grid=(rows,1,1)   │
│ block=(256,1,1)             block=(256,1,1)         block=(cols,1,1)  │
└─────────────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                    GPU硬件执行细节（以步骤1为例）                       │
├─────────────────────────────────────────────────────────────────────────┤
│  Grid(rows个Block)                                                      │
│  ┌────────────┐ ┌────────────┐     ┌────────────┐                      │
│  │ Block 0    │ │ Block 1    │ ... │ Block n    │                      │
│  ├────────────┤ ├────────────┤     ├────────────┤                      │
│  │Warp0 Warp1 │ │Warp0 Warp1 │     │Warp0 Warp1 │                      │
│  │Warp2 Warp3 │ │Warp2 Warp3 │     │Warp2 Warp3 │                      │
│  │Warp4 Warp5 │ │Warp4 Warp5 │     │Warp4 Warp5 │                      │
│  │Warp6 Warp7 │ │Warp6 Warp7 │     │Warp6 Warp7 │                      │
│  └────────────┘ └────────────┘     └────────────┘                      │
│      │               │                     │                          │
│      ▼               ▼                     ▼                          │
│  ┌────────┐     ┌────────┐           ┌────────┐                        │
│  │Thread  │     │Thread  │           │Thread  │                        │
│  │0-31    │     │0-31    │    ...    │0-31    │                        │
│  └────────┘     └────────┘           └────────┘                        │
└─────────────────────────────────────────────────────────────────────────┘
```

## 3. 算子与核函数的层次关系图

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         应用层（用户代码）                              │
├─────────────────────────────────────────────────────────────────────────┤
│ output = torch.softmax(input, dim=1)                                   │
│ loss = nn.CrossEntropyLoss()(output, target)                           │
│ loss.backward()                                                        │
└─────────────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                       算子层（Operator Layer）                          │
├─────────────────────────────────────────────────────────────────────────┤
│  ┌────────────┐ 调用  ┌────────────┐ 调用  ┌────────────┐              │
│  │ Softmax    │──────▶│   Loss     │──────▶│ Backward    │              │
│  │ Operator   │       │ Operator   │       │ Operator    │              │
│  └────────────┘       └────────────┘       └────────────┘              │
│       │                     │                     │                    │
│       ▼                     ▼                     ▼                    │
└─────────────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                      核函数层（Kernel Layer）                           │
├─────────────────────────────────────────────────────────────────────────┤
│  ┌────────────┐  ┌────────────┐  ┌────────────┐  ┌────────────┐        │
│  │ MaxReduce  │  │ SumExp     │  │ Softmax    │  │ GradCalc   │        │
│  │ Kernel     │  │ Kernel     │  │ Kernel     │  │ Kernel     │        │
│  └────────────┘  └────────────┘  └────────────┘  └────────────┘        │
│       │               │               │               │                │
│       ▼               ▼               ▼               ▼                │
└─────────────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                       硬件层（Hardware Layer）                          │
├─────────────────────────────────────────────────────────────────────────┤
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐        │
│  │    GPU SM0      │  │    GPU SM1      │  │    GPU SM2      │        │
│  │  ┌───────────┐  │  │  ┌───────────┐  │  │  ┌───────────┐  │        │
│  │  │   Warp    │  │  │  │   Warp    │  │  │  │   Warp    │  │        │
│  │  │ Scheduler │  │  │  │ Scheduler │  │  │  │ Scheduler │  │        │
│  │  └───────────┘  │  │  └───────────┘  │  │  └───────────┘  │        │
│  │     │ │ │ │     │  │     │ │ │ │     │  │     │ │ │ │     │        │
│  │     ▼ ▼ ▼ ▼     │  │     ▼ ▼ ▼ ▼     │  │     ▼ ▼ ▼ ▼     │        │
│  │   Threads      │  │   Threads      │  │   Threads      │        │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘        │
└─────────────────────────────────────────────────────────────────────────┘
```

## 4. 计算图视角的关系图

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        计算图（Computational Graph）                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ┌───────────┐    ┌───────────┐    ┌───────────┐    ┌───────────┐      │
│  │  Input    │    │  Conv2D   │    │  ReLU     │    │ Softmax   │      │
│  │  Tensor   │───▶│ Operator  │───▶│ Operator  │───▶│ Operator  │───▶  │
│  └───────────┘    └───────────┘    └───────────┘    └───────────┘      │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
                                │
                                ▼  （每个算子展开为Kernel执行）
┌─────────────────────────────────────────────────────────────────────────┐
│                    Kernel执行时间线（Timeline）                         │
├─────────────────────────────────────────────────────────────────────────┤
│ 时间 │ Conv2D Kernel集群        │ ReLU Kernel        │ Softmax Kernel集群 │
│     │                          │                    │                   │
│  t0 │ ┌─────────────────────┐  │                    │                   │
│     │ │ im2col_kernel       │  │                    │                   │
│  t1 │ └─────────────────────┘  │                    │                   │
│     │ ┌─────────────────────┐  │                    │                   │
│  t2 │ │ gemm_kernel         │──────────────────────┐                    │
│  t3 │ └─────────────────────┘  │                    │                   │
│     │                          │ ┌───────────────┐  │                   │
│  t4 │                          │ │ relu_kernel  │  │                   │
│     │                          │ └───────────────┘  │                   │
│  t5 │                          │                    │ ┌───────────────┐ │
│     │                          │                    │ │ max_kernel    │ │
│  t6 │                          │                    │ └───────────────┘ │
│     │                          │                    │ ┌───────────────┐ │
│  t7 │                          │                    │ │ sum_kernel    │ │
│     │                          │                    │ └───────────────┘ │
│  t8 │                          │                    │ ┌───────────────┐ │
│     │                          │                    │ │ softmax_kernel│ │
│  t9 │                          │                    │ └───────────────┘ │
└─────────────────────────────────────────────────────────────────────────┘
```

## 5. 内存层次视角的框架图

```
┌─────────────────────────────────────────────────────────────────────────┐
│                           内存层次结构                                  │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                   算子层（Operator Level）                      │   │
│  │  ● 管理张量生命周期          ● 自动微分                          │   │
│  │  ● 计算图构建              ● 内存优化策略                        │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                │                                       │
│                                ▼ 内存分配/释放                          │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                核函数调度层（Kernel Dispatch）                   │   │
│  │  ● 全局内存管理            ● 流同步                             │   │
│  │  ● Kernel参数准备          ● 错误处理                           │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                │                                       │
│                                ▼ 数据传输                              │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                    GPU内存层次                                   │   │
│  │                                                                  │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐              │   │
│  │  │ 全局内存     │  │ 常量内存     │  │ 纹理内存     │              │   │
│  │  │ (Global)    │  │ (Constant)  │  │ (Texture)   │              │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘              │   │
│  │           │               │               │                     │   │
│  │           ▼               ▼               ▼                     │   │
│  │  ┌─────────────────────────────────────────────────────────┐   │   │
│  │  │                 共享内存（Shared Memory）                 │   │   │
│  │  │               Block内线程共享，速度快                    │   │   │
│  │  └─────────────────────────────────────────────────────────┘   │   │
│  │                          │                                     │   │
│  │                          ▼                                     │   │
│  │  ┌─────────────────────────────────────────────────────────┐   │   │
│  │  │                   寄存器（Registers）                    │   │   │
│  │  │               线程私有，速度最快，容量小                 │   │   │
│  │  └─────────────────────────────────────────────────────────┘   │   │
│  └─────────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────────┘
```

## 6. 性能优化视角的框架图

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        性能优化层次结构                                │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                   算法优化（算子层）                            │   │
│  │  ● 算法选择              ● 数值稳定性                           │   │
│  │  ● 计算复杂度优化        ● 内存访问模式优化                      │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                │                                       │
│                                ▼                                       │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                   实现优化（核函数层）                          │   │
│  │  ● 并行策略              ● 内存合并访问                         │   │
│  │  ● 共享内存使用          ● 指令级优化                           │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                │                                       │
│                                ▼                                       │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                   硬件优化（GPU架构）                           │   │
│  │  ● Warp调度优化          ● 缓存优化                            │   │
│  │  ● 计算吞吐量优化        ● 内存带宽优化                         │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                优化工具链（Tools for Optimization）                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐    │
│  │   Nsight    │  │    nvprof   │  │  Triton     │  │  自定义     │    │
│  │  Systems    │  │             │  │  编译器     │  │  性能分析   │    │
│  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘    │
│         │               │               │               │              │
│         ▼               ▼               ▼               ▼              │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐    │
│  │ 系统级分析  │  │ Kernel级    │  │ 自动优化    │  │ 热点分析    │    │
│  │             │  │ 分析        │  │             │  │             │    │
│  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘    │
└─────────────────────────────────────────────────────────────────────────┘
```

这些框架图从不同角度展示了算子与核函数的关系：

1. **整体架构**：显示从用户接口到硬件执行的完整流程
2. **执行流程**：展示Softmax算子的具体执行步骤
3. **层次关系**：显示算子与核函数的层级结构
4. **计算图视角**：在深度学习计算图中的位置
5. **内存层次**：数据在不同内存层级间的流动
6. **性能优化**：不同层次的优化策略

这些图表帮助理解算子作为高级功能单元，核函数作为底层执行单元的关系。
