---
date: 2025/03/28 19:06:58
updated: 2025/05/21 09:18:10
---


# network

- [<font color=Red>你管这破玩意叫网络</font>](https://mp.weixin.qq.com/s/MXHouvWLBsm7hqHUUjJaEw)
- [22张图详解浏览器请求数据包如何到达web服务器（搞懂网络可以出师了）](https://mp.weixin.qq.com/s/ylwRYbc2onEOVOxPsNfdwQ)
- [网卡与Linux网络结构（上）](https://mp.weixin.qq.com/s/2BXrbTJcaKV2OMgHfcuYNg)
- [网卡与Linux网络结构（中）](https://mp.weixin.qq.com/s/zlbdi6n8lVUARVEtC3lHcA)
- [网卡与Linux网络结构（下）](https://mp.weixin.qq.com/s/SiM6K2uhdjEist6ZL_klyw)
- [网卡与数据接收（上）](https://mp.weixin.qq.com/s/Ne9RFsJWPqzstZTlhIS1dQ)
- [网卡与数据接收（中）](https://mp.weixin.qq.com/s/_pXvYxImkcTomer4kBoiXQ)
- [源码溯源：网卡队列超时机制](https://mp.weixin.qq.com/s/FtgHB2yPb1-3p6KJ653B3w)
- [通俗理解数据中心CPU和GPU之后的第三颗关键芯片“DPU”](https://mp.weixin.qq.com/s/SVscsnpOAtsZ6IBk34AYug)
- [字节校招：TCP 和 UDP 可以同时绑定相同的端口吗？](https://mp.weixin.qq.com/s/4G4il3rZSMmEgvdvGSH_Gw)
- [59页PPT，全面了解"无线通信原理"](https://mp.weixin.qq.com/s/TEJYQJkT4FToeEoxI_9Tmg)
- [【Linux】如何查看网卡驱动](https://blog.csdn.net/imliuqun123/article/details/139596951)
- [Linux网络最难理解的五个核心概念](https://mp.weixin.qq.com/s/15oz0wMR-lGIjDmQcGo_rQ)

## 计算机网络

- [计算机网络常见面试题](https://www.cnblogs.com/wuwuyong/p/12198928.html)
- [计算机网络常见面试题](https://www.cnblogs.com/inception6-lxc/p/9152691.html)

- [字节一面：“为什么网络要分层？每一层的职责、包含哪些协议？”](https://www.toutiao.com/article/7194724931555607100)

- [高性能服务器开发](https://github.com/balloonwj/CppGuide)
  - [在线阅读站点1：https://balloonwj.github.io/cpp-guide-web/](https://balloonwj.github.io/cpp-guide-web/)
  - [在线阅读站点2：http://balloonwj.gitee.io/cpp-guide-web/](http://balloonwj.gitee.io/cpp-guide-web/)
  - [备份站点：http://101.37.25.166/blog/ ](http://101.37.25.166/blog/)

## socket

- [socket编程入门：1天玩转socket通信技术（非常详细）](http://m.biancheng.net/socket/)
- [socket缓冲区以及阻塞模式](http://m.biancheng.net/view/2349.html)
- [<font color=Red>Socket缓冲区</font>](https://blog.csdn.net/summer_fish/article/details/121740570)
- [Ubuntu 设置Socket缓冲区大小](https://zhuanlan.zhihu.com/p/486140420)
- [<font color=Red>Linux fd 系列 — socket fd 是什么？</font>](https://os.51cto.com/article/682138.html)
- [Linux内核 | socket底层的来龙去脉](https://mp.weixin.qq.com/s/6fxS9GDdKOIkKASiz0e07Q)
- [如何学习 Linux 内核网络协议栈](https://mp.weixin.qq.com/s/0eomyTaKWVeJbryIEdo1ug)
- [socket中write和send有什么区别？——拆解大厂面试题(校招)](https://mp.weixin.qq.com/s/Wq0Ns-5oWcMH7QKhmxwtHg)

### Netlink

- [Linux用户空间与内核空间通信(Netlink通信机制)](https://mp.weixin.qq.com/s/CMWxPcJzoN6TlDypaB4kXA)
- [深入了解Linux netlink机制：实现高性能网络通信](https://mp.weixin.qq.com/s/odS7qoVGrtOgSt99o5hw0g)

## select

- [Linux编程之select](https://www.cnblogs.com/skyfsm/p/7079458.html)

## epoll

- [Linux 高性能服务 epoll 的本质，真的不简单（含实例源码）](https://mp.weixin.qq.com/s/rpk0QmNfNiUoUH1fuDUpuA)

- [<font color=Red>如果这篇文章说不清epoll的本质，那就过来掐死我吧！ （1） - 知乎 (zhihu.com)</font>](https://zhuanlan.zhihu.com/p/63179839)
- [如果这篇文章说不清epoll的本质，那就过来掐死我吧！ （2） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/64138532)
- [如果这篇文章说不清epoll的本质，那就过来掐死我吧！ （3） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/64746509)

- [Epoll的本质（内部实现原理）](https://blog.csdn.net/songchuwang1868/article/details/89877739)
- [epoll LT 模式和 ET 模式详解（文末赠书） - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1636224)
- [Linux下的I/O复用与epoll详解](https://www.cnblogs.com/lojunren/p/3856290.html)

- [epoll源码剖析_wendy_keeping的博客-CSDN博客](https://blog.csdn.net/wendy_keeping/article/details/76577770)

- [盘点Linux Epoll那些致命弱点](https://mp.weixin.qq.com/s/tu7ovS2xO6ju-H5gB8TNCw)

- [大话 Select、Poll、Epoll - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1005481)

### 惊群效应

- [深入理解linux惊群效应（超详细）](https://mp.weixin.qq.com/s/CTj-HpnmX8d_hDWk5t-oqQ)

## libevent

- [Libevent深入浅出-《Libevent深入浅出》-书栈网·BookStack](https://www.bookstack.cn/read/libevent/450ef2232c710e15.md)
- [Libevent深入浅出· libevent深入浅出- Gitbooks](https://www.bookstack.cn/read/libevent/450ef2232c710e15.md)
- [libevent深入浅出_none123java321的博客-CSDN博客](https://blog.csdn.net/none123java321/article/details/113134499)
- [libevent源码深度剖析](https://www.cnblogs.com/lfsblack/p/5498556.html)

- [详解libevent网络库（一）-框架的搭建](https://blog.csdn.net/Lemon_tea666/article/details/92637297)
- [详解libevent网络库（二）-即时聊天通讯](https://blog.csdn.net/Lemon_tea666/article/details/92797520)

- [Linux网络编程 socketpair的使用](https://blog.csdn.net/y396397735/article/details/50684558)

- [一文讲懂什么是vlan、三层交换机、网关、DNS、子网掩码、MAC地址](https://mp.weixin.qq.com/s/73u2kO43gGp0BCHncQO0ng)
- [46张图带你了解网络传输、WIFI、以太网协议和网络寻址](https://mp.weixin.qq.com/s?__biz=MzUxMjEyNDgyNw==&mid=2247501617&idx=2&sn=460849b920d167dba10f7c4ad1fd927a&chksm=f96bb5c5ce1c3cd30bdfe24e10b07d227d45c1d1e9cb64eeabae977eb62f7203618740a88b5e&scene=178&cur_album_id=1598710257097179137#rd)
- [解析：单播、广播和组播的区别](https://bbs.huaweicloud.com/blogs/147408)

- [「linux」Socket缓存是如何影响TCP性能的？](https://mp.weixin.qq.com/s/2znyxjMUqpH9qjqCqTd69g)
- [Linux 网络性能的 15 个优化建议！](https://mp.weixin.qq.com/s/6uuL-oq5FCIMTER1M9Dnmg)

- [虾皮二面：既然有 HTTP 协议，为什么还要有 RPC?](https://mp.weixin.qq.com/s/cqmZX32TK50e7Ix_Uz_HoA)

- [虾皮二面：既然有 HTTP 协议，为什么还要有 RPC?](https://mp.weixin.qq.com/s/cqmZX32TK50e7Ix_Uz_HoA)

## IO模型

- [聊聊Linux 五种IO模型](https://www.jianshu.com/p/486b0965c296)
- [深入理解Linux I/O系统](https://cloud.tencent.com/developer/article/1901999)
- [linux文件io缓冲](http://www.daileinote.com/computer/linux_sys/10)
- [Linux 实现原理 — I/O 处理流程与优化手段](https://mp.weixin.qq.com/s/0aC4z2yl3n6kQ2PTt-aPNw)
- [【Linux】—— Linux下的文件缓冲区](https://blog.csdn.net/chenxiyuehh/article/details/90577631)
- [[基础]同步消息和异步消息传递的区别？](https://www.cnblogs.com/eason-liu/p/8053558.html)
- [终于明白了，一文彻底理解I/O多路复用](https://mp.weixin.qq.com/s?__biz=Mzg4OTYzODM4Mw==&mid=2247485708&idx=1&sn=d7c8bec26de6ddcaaff49eac0766dc87&source=41#wechat_redirect)
- [这次答应我，一举拿下I/O多路复用！](https://mp.weixin.qq.com/s/uJLw9tCUANNhWlFAIZfDIA)

## reactor

- [<font color=Red>一文图解高性能网络架构：Reactor和Proactor</font>](https://www.toutiao.com/article/7171814310321291810/)
- [<font color=Red>​网络 IO 演变发展过程和模型介绍</font>](https://www.toutiao.com/article/7196173859631219238)
- [3000字|程序员应如何理解Reactor模式？](https://mp.weixin.qq.com/s/kyEYNLtpsLZHK9n3j1pO0w)
- [Reactor模式详解＋源码实现](https://www.jianshu.com/p/188ef8462100)

- [高并发高性能服务器是如何实现的](https://mp.weixin.qq.com/s/Z07Hc9SRfGz6n8XhFHGVyA)
- [从小白到高手，你需要理解同步与异步(内含10张图)](https://mp.weixin.qq.com/s/xARtnqFQmi-Hzw01m__KQA)
- [读取文件时，程序经历了什么？](https://mp.weixin.qq.com/s?__biz=Mzg4OTYzODM4Mw==&mid=2247485706&idx=1&sn=bc5d6e4bf9ee5dccef520e5b2051d943&source=41#wechat_redirect)

- [Mac 地址会重复吗？Mac 地址也会耗尽吗？](https://mp.weixin.qq.com/s/AqTcVaxElxgwVjCPcxAFDA)

- [要理解网络，其实不就是理解这三张表吗](https://www.toutiao.com/article/7204305085835346467)

- [为什么要使用 TCP keepalive？C/C++代码实现TCP keepalive](https://www.toutiao.com/article/7166152475840561704/)

- [一文读懂物联网MQTT协议之基础特性篇](https://www.toutiao.com/article/7191463571258343948/)

- [为什么HTTPS是安全的？](https://mp.weixin.qq.com/s/Qafa3nE_vTzdl1jGTiUaMw)

- [刚插上网线，电脑怎么知道自己的IP是什么？](https://mp.weixin.qq.com/s/siG1XMxQnduBWdY-hvQO8g)

- [一张图了解八种流行的网络协议](https://mp.weixin.qq.com/s/f19jaSwKA0oTE87yAfyb2w)

## phy

- [Linux 实现原理 — 网卡驱动程序初始化流程](https://mp.weixin.qq.com/s/jcolfo4bvKPS-eRdeCkRCg)
- [PHY芯片快速深度理解（持续更新中……）](https://blog.csdn.net/qq_40715266/article/details/124095801)
- [【网络驱动】ifconfig up 后内核网络驱动做了什么？](https://mp.weixin.qq.com/s/VQWEkycARPQ2pCpDxx_Rnw)

## 扫描工具

- [【工具更新】内网大杀器Fscan](https://mp.weixin.qq.com/s/HnerPmV34KWKLADVsEx5gA)

## 内网穿透

- [内网穿透详解](https://www.toutiao.com/article/7317562665730097704/)

## 抓包

- [Linux性能优化-用tcpdump 和 Wireshark 分析网络流量](https://blog.csdn.net/hixiaoxiaoniao/article/details/87596703)
- [一文掌握 Linux 性能分析之网络篇](https://www.cnblogs.com/bakari/p/10515977.html)
- [wireshark过滤http包](https://blog.csdn.net/github_40044758/article/details/111290919)
- [<font color=Red>如何在 Linux 上使用 Wireshark 过滤器</font>](https://cn.linux-console.net/?p=8295)

  ```bash
  http.host=="www.baidu.com"
  ```

  ![http.host=="www.baidu.com"](https://cdn.jsdelivr.net/gh/realwujing/picture-bed/20240117135816.png)

## 收发包

- [Linux 网络数据包的接收和发送流程](https://mp.weixin.qq.com/s/YG_9N41AweQ3IqO_fjpidw)

网络数据包的接收过程通常分为**硬中断（Hard IRQ）**和**软中断（Soft IRQ）**两个阶段，这是 Linux 网络栈中的典型处理方式，能有效提高性能和响应速度。让我们一步步解释这个流程：  

---

### 网络数据包的接收流程（发包）

#### 🌟 **1. 硬中断阶段（Hard IRQ）**  
硬中断由**网卡（NIC, Network Interface Card）**直接触发，处理过程如下：  

1. **数据包到达网卡：**  
   - 网卡收到网络数据包后，将数据存入网卡的**接收环形缓冲区（RX Ring Buffer）**。  
   
2. **触发硬中断：**  
   - 网卡向 CPU 发送一个硬件中断信号（IRQ, Interrupt Request），通知系统“有新数据包需要处理了”。  
   
3. **硬中断处理程序（ISR, Interrupt Service Routine）：**  
   - CPU 响应中断，执行网卡驱动程序的硬中断处理逻辑。硬中断处理程序通常做以下工作：  
     - **禁用网卡中断**（避免大量中断拖垮系统）。  
     - **将数据包的描述符加入 NAPI 列表**（NAPI, New API，是 Linux 网络子系统的优化机制）。  
     - **触发软中断（Soft IRQ）或轮询机制**，让后续处理交给软中断来完成。  

硬中断要尽量短小精悍，只完成必要的工作，把大部分包处理逻辑交给软中断来做。  

---

#### ⚙️ **2. 软中断阶段（Soft IRQ/NAPI）**  
软中断是由内核在合适的时机触发的，用来处理大量网络包时的性能瓶颈。流程如下：  

1. **NAPI 轮询机制：**  
   - 在硬中断阶段，网卡驱动程序会将网络接口添加到 NAPI 列表，触发软中断处理逻辑。  
   
2. **处理数据包：**  
   - 软中断（`NET_RX_SOFTIRQ`）开始执行，从网卡的 RX 队列取出数据包：  
     - **协议栈处理**：解析以太网帧、IP 数据包、TCP/UDP 协议等。  
     - **分发到上层**：根据数据包类型，将数据递交给相应的内核模块（如 socket 层、iptables、防火墙等）。  
     - **交给用户态**：最终数据通过 `recv()` 系统调用传递给用户态程序。  

3. **恢复中断：**  
   - 数据包处理完成后，软中断会重新开启网卡中断，使得新的数据包到来时，能够再次触发硬中断。  

---

#### 📌 **为什么要分硬中断和软中断？**  
1. **减少中断风暴**：如果每个数据包都触发硬中断，CPU 会被中断淹没，导致性能下降。  
2. **批量处理**：软中断采用 NAPI 机制，可以在一次软中断处理过程中批量收包，提升效率。  
3. **更好的调度**：软中断可以被内核调度器管控，避免影响其他关键任务的执行。  

如果你需要优化网络性能，还可以研究：  
- **RPS/RFS（Receive Packet Steering/Flow Steering）**：多核场景下分散网络流量，减少单核瓶颈。  
- **XDP（eXpress Data Path）**：在网卡驱动层快速处理或丢弃数据包，减少内核协议栈负担。  

如果想了解具体代码逻辑，可以看看 `net/core/dev.c` 文件中的 `netif_receive_skb()`、`napi_poll()` 等函数。

### **📤 网络数据包的发送流程（发包）**  
网络数据包的发送也涉及**软中断**和**硬中断**，但与收包流程不同，主要由**上层协议栈**驱动发包，最终由**网卡（NIC）**负责数据的实际传输。整个流程如下：

---

#### **1️⃣ 用户态数据准备**
在用户态，应用程序（如 `ping`、`curl`、`iperf` 或自定义 socket 程序）调用 `send()`、`write()` 等系统调用，向 socket 发送数据：
```c
send(sockfd, buffer, len, 0);
```
这时，数据进入 **TCP/UDP 层** 进行处理。

---

#### **2️⃣ 内核协议栈处理**
当数据进入内核后，协议栈会依次处理它：
##### **（1）TCP/UDP 处理**
- **UDP**：直接封装 IP 头，进入下一步（适用于 DNS、VoIP、视频流等）。
- **TCP**：
  - 拆分数据（MSS，最大报文段）。
  - 计算 TCP 校验和。
  - 维护 TCP 发送队列（适用于重传、流控）。
  - 进入 **IP 层**。

##### **（2）IP 层处理**
- 填充源 IP、目的 IP、TTL 等。
- 计算 IP 校验和。
- 进行 **路由查找**（`fib_lookup()`），决定从哪个网卡发出。

##### **（3）以太网层处理**
- 根据 ARP 表查找目标 MAC 地址（如果是第一次发送，可能会先发送 ARP 请求）。
- 组装以太网帧（源 MAC、目的 MAC、协议类型）。
- 数据包进入网卡 **TX 队列**。

---

#### **3️⃣ 软中断处理（Soft IRQ / NET_TX_SOFTIRQ）**
1. **进入 `dev_queue_xmit()`**  
   - 这个函数检查数据包并调用驱动层的 `ndo_start_xmit()`，进入网卡队列。
   - 如果队列满了，可能会触发流控（TSO、GSO、Qdisc 等）。

2. **队列调度（Qdisc, Queueing Discipline）**
   - 如果开启了 `tc` 流控（如 `fq_codel`、`pfifo_fast`），数据可能会被重新排序。
   - 高速网卡可能会使用 **TSO（TCP Segmentation Offload）**，让网卡直接拆包，提高 CPU 效率。

---

#### **4️⃣ 硬中断触发（Hard IRQ）**
数据包最终进入网卡的 **发送环形缓冲区（TX Ring Buffer）**，这时：
1. **网卡触发硬中断**（或 DMA 直接传输）。
2. **网卡驱动调用 `xmit_done()` 释放已发送的 skb**。
3. **网卡将数据发送到物理链路（光纤、电缆、Wi-Fi）**，数据包真正进入网络。

---

#### **⚡ 为什么发包时硬中断较少？**
相较于收包，**发包通常不需要硬中断**，原因如下：
- **TX 采用 DMA 直接传输**，只需要在发送完成后通知 CPU。
- **网卡队列（TX Queue）优化**，减少 CPU 介入，提高吞吐量。
- **GSO/TSO（大包卸载）** 让网卡直接拆分 TCP 包，提高效率。

---

#### **📌 总结**
| 过程 | 触发方式 | 关键函数 |
|------|------|------|
| **用户调用 `send()`** | 用户态 | `sys_sendto()` |
| **协议栈处理**（TCP/UDP/IP/MAC） | 软中断（NET_TX_SOFTIRQ） | `dev_queue_xmit()` |
| **进入网卡队列（TX Queue）** | 软中断 | `ndo_start_xmit()` |
| **网卡 DMA 传输** | 硬件 | DMA |
| **发送完成通知（硬中断）** | 硬中断 | `xmit_done()` |

如果你关心具体的优化点，比如 **XDP、TSO、GSO、RPS/RFS** 等，可以深入研究 `net/core/dev.c`、`drivers/net/` 里的网卡驱动代码。

### 🌟 **收发包中的硬中断与软中断对比**  

| **对比项**         | **硬中断（Hard IRQ）** 🌩️ | **软中断（Soft IRQ）** ⚙️ |
|------------------|--------------------------|--------------------------|
| **触发时机**      | 由硬件事件触发（如网卡收到新数据包或发送完成） | 由内核调度或硬中断触发，批量处理网络事件 |
| **优先级**      | 高，立即打断 CPU 执行当前任务来响应 | 较低，由内核调度器在合适时机运行 |
| **主要作用**      | 快速响应硬件事件，通知系统有新包到达或传输完成 | 批量处理网络数据包，减少中断次数，优化性能 |
| **收包流程**      | 1. 网卡收到数据包，触发硬中断<br>2. 将包放入 RX 队列<br>3. 触发软中断来完成后续处理 | 1. 轮询 RX 队列取包<br>2. 解析协议头（IP、TCP/UDP）<br>3. 将数据交给上层协议或用户态 |
| **发包流程**      | 1. 发送完成后触发硬中断<br>2. 通知内核释放发送缓冲区 | 1. 将待发送数据放入 TX 队列<br>2. 由软中断驱动网卡取包发送 |
| **处理内容**      | - 禁用中断避免风暴<br>- 通知软中断处理数据包 | - 批量收发包处理<br>- 协议栈解析、流控、负载均衡 |
| **执行时长**      | 极短，避免长时间占用 CPU | 相对较长，批量处理多个包 |
| **性能优化**      | - 减少中断频率（NAPI）<br>- 合并多包触发 | - 使用 NAPI 机制<br>- 启用 RPS/RFS、XDP 提升性能 |
| **瓶颈与挑战**    | 过多硬中断会导致“中断风暴”，拖垮系统 | 处理量大时易拖慢系统，需配合硬件卸载优化 |
| **关键机制**      | - NAPI（New API）<br>- IRQ 禁用/使能机制 | - NAPI 轮询<br>- NET_RX_SOFTIRQ、NET_TX_SOFTIRQ |

总结：  
- **硬中断**：负责快速响应硬件事件（如新包到达、发送完成），避免 CPU 长时间被打断。  
- **软中断**：负责批量处理收发包逻辑，减少中断次数，提高系统吞吐量。  
- **NAPI 机制**：结合硬、软中断的优点，通过轮询和批量处理机制，进一步优化高流量场景下的性能。

## 丢包

- [Linux服务器时不时丢包，它凭啥能精准解决？](https://mp.weixin.qq.com/s/ecRMntFeL9smXtzO-htPVA)
- [Linux内核常见的网络丢包场景分析](https://mp.weixin.qq.com/s/vdW0L7nEdfrxSJ_9VGviaA)
- [Linux内核网络丢包探秘，这些办法轻松搞定](https://mp.weixin.qq.com/s/eLO9NxH4XfRca-PP5VCtPQ)
- [linux 丢包排查思路简述（tcp+rdma）](https://blog.csdn.net/zxpoiu/article/details/115748746)
- [使用dropwatch观测网络丢包](https://blog.csdn.net/xiayutian747/article/details/136016137)

在网络收发包过程中，判断一个包是否丢失，通常依赖于协议层面的机制。不同协议有各自的处理方式，下面以常见的 **TCP** 和 **UDP** 为例来解释：

### 🌐 **TCP 层的丢包判断机制：**
TCP 是可靠传输协议，内置了丢包检测机制，包括以下几种方法：

1. **确认机制（ACK）：**
   - TCP 通信是基于序列号（Sequence Number）和确认号（Acknowledgment Number）来跟踪数据包的传输情况。
   - 发送方发送一个数据包后，期望接收方返回一个 ACK 确认包。如果在设定的超时时间（RTO, Retransmission Timeout）内未收到 ACK，发送方会认为该包可能丢失，触发重传机制。

2. **超时重传（Timeout Retransmission）：**
   - 发送方为每个未确认的数据包启动一个定时器（RTO）。如果超时仍未收到 ACK，发送方认为数据包已丢失，重新发送。

3. **快速重传（Fast Retransmit）：**
   - 当接收方收到乱序的包时，会不断发送对最后正确接收的包的 ACK（称为“冗余 ACK”）。如果发送方连续收到 3 个相同的冗余 ACK，就会判断有数据包丢失，触发快速重传。

4. **选择性确认（SACK, Selective Acknowledgment）：**
   - 接收方可以通过 SACK 机制告知发送方哪些包已收到，哪些包未收到，从而让发送方仅重传丢失的部分。

---

### 📩 **UDP 层的丢包判断机制：**
UDP 是无连接、不可靠传输协议，本身不提供丢包检测机制。常见的做法包括：

1. **应用层序列号：**
   - 应用程序自行为每个 UDP 包附加序列号，接收方通过序列号判断是否有丢包。

2. **超时机制：**
   - 发送方在发送包后启动超时计时器，如果在一定时间内未收到预期的响应（如 ACK 或心跳包），认为数据包可能丢失。

3. **心跳机制（Heartbeat）：**
   - 定期发送探测包，检测链路状态。如果多次探测未响应，则认为有丢包或链路中断。

4. **FEC（Forward Error Correction，前向纠错）：**
   - 使用冗余信息或校验码（如 Reed-Solomon 编码）来检测并修复丢包，尤其在流媒体、VoIP 等场景常见。

---

### 📊 **底层机制（链路层/硬件）：**
1. **CRC 校验（循环冗余校验）：**
   - 每个数据包附带 CRC 校验码，接收方对收到的包进行校验，发现错误则丢弃该包，依赖上层协议重传。

2. **网卡统计：**
   - 网卡驱动程序维护收发包计数器（如 `ifconfig`、`ethtool` 工具可以查看），包括丢包统计信息，可用来辅助判断。

### dropwatch

以下是 dropwatch 工具的核心用法总结，涵盖输出解析、操作流程和实战技巧：

---

**一、核心使用流程**
**1. 启动监控**

```bash
sudo dropwatch -l kas  # 需root权限
dropwatch> start      # 开始捕获丢包事件
```

**2. 输出解析**
典型输出格式：

```plaintext
drop at: [函数名]+[偏移量] (addr: [内存地址])
```

关键函数与含义：

| 函数名                     | 丢包层级               | 常见原因                  |
|---------------------------|----------------------|-------------------------|
| `__netif_receive_skb_core` | 网卡驱动/数据链路层    | 网卡队列满，DMA失败       |
| `ip_rcv`                  | IP协议栈入口          | IP头校验失败，路由无匹配  |
| `tcp_v4_rcv`              | TCP协议处理层         | 端口未监听，序列号异常    |
| `kfree_skb`               | 内核通用释放点         | 内存不足，协议栈主动丢弃  |

**3. 高级过滤**

```bash
# 启动时过滤TCP相关丢包
dropwatch -l kas | grep -E 'tcp_|kfree_skb'
```

---

**二、基础操作指令速查**

| 命令               | 作用                          | 示例                  |
|--------------------|-----------------------------|-----------------------|
| `start`            | 开始实时监控丢包               | `dropwatch> start`    |
| `stop`             | 暂停监控（保持进程）            | `dropwatch> stop`     |
| `exit` / `Ctrl+C`  | 完全退出程序                   | `dropwatch> exit`     |
| `help`             | 查看所有可用命令                | `dropwatch> help`     |

---

**三、实战技巧**
**1. 获取完整调用栈**

```bash
dropwatch> stack  # 启用堆栈跟踪
dropwatch> start
```

输出示例：

```plaintext
drop at: tcp_v4_do_rcv+0x1a1
    [<ffffffff814a2b11>] tcp_rcv_established+0x8d
    [<ffffffff814a3b11>] tcp_v4_do_rcv+0x1a1
```

**2. 结合地址反查**

```bash
# 使用addr2line定位代码（需内核调试符号）
addr2line -e /usr/lib/debug/lib/modules/$(uname -r)/vmlinux ffffffff814a2b11
```

输出：`/net/ipv4/tcp_input.c:185`（具体代码行）

**3. 自动化监控脚本**

```bash
#!/bin/bash
# 监控10秒，统计TOP丢包点
timeout 10 dropwatch -l kas | awk '/drop at:/{print $3}' | sort | uniq -c | sort -nr
```

---

**四、生产环境注意事项**

1. 权限与依赖：
   • 需 `root` 权限运行

   • 内核需启用 `CONFIG_KPROBES` 和 `CONFIG_DEBUG_INFO`

2. 性能影响：
   • 高频丢包时可能增加CPU负载，建议短期诊断

3. 符号表问题：
   • 若输出纯地址（如 `ffffffff814a2b11`），安装调试符号包：

     ```bash
     apt install linux-image-$(uname -r)-dbgsym  # Debian/Ubuntu
     ```

---

**五、与其他工具对比**

| 工具          | 粒度       | 优势                  | 局限                |
|--------------|-----------|----------------------|---------------------|
| `dropwatch`  | 函数级     | 精准定位内核丢包位置   | 需符号表支持        |
| `perf`       | 指令级     | 支持动态探针           | 学习成本高          |
| `ethtool -S` | 网卡级     | 硬件丢包统计           | 无法定位协议栈问题  |

---

通过以上方法，可快速定位TCP/IP协议栈中丢包的具体位置，进而针对性优化内核参数或修复网络配置问题。

## ovs dpdk

### dpdk在用户态收发包是怎么避开内核态的？

DPDK（Data Plane Development Kit）通过以下机制在用户态直接收发包，绕开内核态：

#### 1. **用户态驱动**
DPDK使用用户态驱动（PMD）直接与网卡交互，避免了内核态和用户态之间的上下文切换。

#### 2. **轮询模式**
DPDK采用轮询模式（Poll Mode），持续检查网卡是否有新数据包到达，而不是依赖内核的中断机制，减少了中断处理的开销。

#### 3. **大页内存**
DPDK使用大页内存（HugePages）来减少TLB（Translation Lookaside Buffer）未命中，提升内存访问效率。

#### 4. **零拷贝**
DPDK通过零拷贝技术，数据包直接从网卡DMA到用户态内存，无需经过内核缓冲区，减少了数据复制开销。

#### 5. **内存池**
DPDK使用预分配的内存池管理数据包缓冲区，避免了频繁的内存分配和释放操作。

#### 6. **网卡队列绑定**
DPDK将网卡队列绑定到特定CPU核心，确保数据包处理在固定的CPU上进行，减少上下文切换和缓存失效。

#### 7. **绕过内核协议栈**
DPDK绕过内核网络协议栈，直接在用户态处理数据包，减少了内核协议栈的处理开销。

#### 8. **硬件加速**
DPDK利用网卡的硬件卸载功能（如校验和计算、TSO等），进一步减少CPU负担。

#### 总结
DPDK通过用户态驱动、轮询模式、大页内存、零拷贝、内存池、队列绑定、绕过内核协议栈和硬件加速等技术，实现了高效的用户态数据包处理，避免了内核态的开销。

### pmd一直不陷入内核态？网卡是本机的物理网卡？

#### 1. **用户态驱动（PMD）不陷入内核态**
DPDK的用户态驱动（PMD）确实**不陷入内核态**，它通过以下方式直接与网卡交互：

- **直接访问硬件寄存器**：PMD通过映射网卡的硬件寄存器到用户态空间，直接读写这些寄存器来控制网卡。
- **DMA（直接内存访问）**：PMD配置网卡的DMA引擎，使数据包直接从网卡传输到用户态预分配的内存池中，无需内核介入。
- **轮询模式**：PMD通过轮询网卡的接收和发送队列来检查数据包，而不是依赖内核的中断机制。

因此，PMD完全运行在用户态，避免了内核态和用户态之间的上下文切换。

---

#### 2. **网卡是本机的物理网卡**
DPDK主要支持**本机的物理网卡**，但也可以通过虚拟化技术（如SR-IOV）支持虚拟网卡。具体如下：

- **物理网卡**：DPDK支持多种物理网卡（如Intel的82599、X710等），并通过PMD直接管理这些网卡。
- **虚拟网卡**：在虚拟化环境中，DPDK可以通过SR-IOV（Single Root I/O Virtualization）技术将物理网卡虚拟化为多个虚拟功能（VF），并将这些VF直接分配给虚拟机使用。
- **虚拟设备**：DPDK还支持一些虚拟设备（如vhost-user），用于与虚拟机或容器通信。

---

#### 总结
- **PMD不陷入内核态**：它通过直接访问硬件寄存器和DMA，完全运行在用户态。
- **网卡类型**：DPDK主要支持本机物理网卡，但也支持虚拟化环境中的虚拟网卡。

这种设计使得DPDK能够实现高性能的数据包处理，避免了传统内核网络协议栈的开销。

### pmd访问寄存器、DMA不需要调用系统调用？

访问硬件寄存器和配置DMA通常需要**特权操作**（如访问I/O端口或映射物理内存），这些操作在传统情况下确实需要通过内核态的系统调用来完成。然而，DPDK通过以下机制避免了频繁的系统调用，实现了在用户态直接访问硬件资源：

---

#### 1. **内存映射（Memory Mapping）**
DPDK使用`mmap`系统调用将网卡的硬件寄存器和DMA区域映射到用户态地址空间。这是一个**一次性的操作**：
- 在初始化阶段，DPDK通过`mmap`将网卡的寄存器空间和DMA缓冲区映射到用户态。
- 映射完成后，用户态程序可以直接访问这些内存区域，无需每次访问都调用系统调用。

---

#### 2. **UIO（Userspace I/O）或VFIO（Virtual Function I/O）**
DPDK依赖于UIO或VFIO框架来实现用户态对硬件的访问：
- **UIO**：通过内核模块将硬件寄存器和中断处理暴露给用户态。
- **VFIO**：更现代的方式，提供更安全的硬件访问机制，支持DMA和中断处理。

这些框架在初始化阶段通过系统调用完成硬件资源的映射和配置，之后用户态程序可以直接访问硬件资源，无需频繁陷入内核态。

---

#### 3. **大页内存（HugePages）**
DPDK使用大页内存来存储数据包缓冲区。大页内存通过`mmap`映射到用户态后，用户态程序可以直接访问这些内存区域，避免了频繁的内存分配和内核态切换。

---

#### 4. **DMA配置**
DMA的配置通常需要特权操作，但DPDK在初始化阶段通过以下方式完成：
- 使用VFIO或UIO框架配置DMA引擎。
- 将DMA缓冲区映射到用户态内存。
- 之后，用户态程序可以直接操作DMA缓冲区，无需每次DMA传输都调用系统调用。

---

#### 5. **轮询模式（Poll Mode）**
DPDK使用轮询模式检查网卡的接收和发送队列，而不是依赖内核的中断机制。这种方式避免了频繁的中断处理和上下文切换。

---

#### 总结
- **初始化阶段**：DPDK通过`mmap`、UIO/VFIO等机制，将硬件寄存器和DMA区域映射到用户态地址空间。这些操作需要系统调用，但仅在初始化时完成。
- **运行时**：映射完成后，用户态程序可以直接访问硬件寄存器和DMA缓冲区，无需频繁调用系统调用。
- **性能优势**：通过减少系统调用和上下文切换，DPDK实现了高性能的数据包处理。

因此，DPDK的设计目标是**尽量减少运行时对内核态的依赖**，而不是完全避免系统调用。

### dpdk哪些代码会仅在用户态执行？哪些代码会导致陷入内核态？举例说明

在DPDK中，代码的执行分为**用户态**和**内核态**两部分。以下是具体的分类和示例：

---

#### 1. **仅在用户态执行的代码**
这些代码完全运行在用户态，不涉及内核态的切换。

##### 示例：
- **数据包处理逻辑**：
  ```c
  while (1) {
      struct rte_mbuf *pkts[BURST_SIZE];
      uint16_t nb_rx = rte_eth_rx_burst(port_id, queue_id, pkts, BURST_SIZE);
      if (nb_rx == 0) continue;

      for (int i = 0; i < nb_rx; i++) {
          process_packet(pkts[i]);  // 用户态处理数据包
          rte_pktmbuf_free(pkts[i]);  // 释放数据包缓冲区
      }
  }
  ```
  - `rte_eth_rx_burst`：从网卡队列中批量接收数据包。
  - `process_packet`：用户态的数据包处理逻辑。
  - `rte_pktmbuf_free`：释放数据包缓冲区。

- **内存池操作**：
  ```c
  struct rte_mempool *mp = rte_pktmbuf_pool_create("mbuf_pool", NUM_MBUFS, MBUF_CACHE_SIZE, 0, RTE_MBUF_DEFAULT_BUF_SIZE, rte_socket_id());
  ```
  - `rte_pktmbuf_pool_create`：创建内存池，用于分配数据包缓冲区。

- **队列操作**：
  ```c
  struct rte_ring *ring = rte_ring_create("msg_ring", RING_SIZE, rte_socket_id(), RING_F_SP_ENQ | RING_F_SC_DEQ);
  ```
  - `rte_ring_create`：创建无锁队列，用于线程间通信。

---

#### 2. **会导致陷入内核态的代码**
这些代码涉及与内核的交互，需要通过系统调用或内核模块的支持。

##### 示例：
- **初始化阶段的硬件资源映射**：
  ```c
  int ret = rte_eal_init(argc, argv);
  ```
  - `rte_eal_init`：初始化DPDK环境，包括映射硬件寄存器、配置大页内存等。这些操作需要通过`mmap`系统调用将硬件资源映射到用户态。

- **网卡配置**：
  ```c
  ret = rte_eth_dev_configure(port_id, nb_rx_queue, nb_tx_queue, &port_conf);
  ```
  - `rte_eth_dev_configure`：配置网卡的接收和发送队列。在底层可能通过UIO/VFIO框架与内核交互。

- **中断处理**：
  ```c
  rte_eth_dev_rx_intr_ctl(port_id, queue_id, RTE_EPOLL_PER_THREAD, 1);
  ```
  - `rte_eth_dev_rx_intr_ctl`：配置网卡的中断处理。虽然DPDK主要使用轮询模式，但在某些场景下可能涉及中断处理，需要内核支持。

- **大页内存分配**：
  ```c
  struct rte_memseg *ms = rte_malloc_virt2memseg(ptr, NULL);
  ```
  - `rte_malloc_virt2memseg`：将虚拟地址转换为大页内存段。底层依赖于`mmap`系统调用。

---

#### 3. **总结**
- **仅在用户态执行的代码**：
  - 数据包接收、处理和释放。
  - 内存池和队列的操作。
  - 用户态的业务逻辑。

- **会导致陷入内核态的代码**：
  - 硬件资源映射和初始化。
  - 网卡配置和中断处理。
  - 大页内存分配和管理。

DPDK的设计目标是尽量减少运行时的内核态切换，大部分高性能数据路径代码都运行在用户态，只有在初始化和配置阶段才需要与内核交互。

### dpdk在内核有对应的驱动吗？

是的，DPDK 在内核中确实有对应的驱动支持，但这些驱动的作用与传统内核网络协议栈的驱动不同。DPDK 依赖的驱动主要用于**将硬件资源暴露给用户态**，而不是直接处理数据包。以下是 DPDK 在内核中的驱动及其作用：

---

#### 1. **UIO（Userspace I/O）驱动**
UIO 是 DPDK 最早使用的驱动框架，它的作用是将硬件资源（如网卡的寄存器、内存映射区域等）暴露给用户态程序。

##### 主要功能：
- **硬件资源映射**：将网卡的寄存器空间和 DMA 区域映射到用户态地址空间。
- **中断处理**：UIO 驱动可以处理硬件中断，并将中断信息传递给用户态程序。

##### 示例：
- `igb_uio` 或 `uio_pci_generic` 是常用的 UIO 驱动模块。

##### 使用方式：
```bash
modprobe uio_pci_generic
```

---

#### 2. **VFIO（Virtual Function I/O）驱动**
VFIO 是一个更现代的驱动框架，支持更安全和灵活的硬件访问机制。DPDK 通常优先使用 VFIO 而不是 UIO。

##### 主要功能：
- **安全的硬件访问**：VFIO 使用 IOMMU（Input-Output Memory Management Unit）来隔离用户态程序对硬件的访问，防止非法内存访问。
- **支持 SR-IOV**：VFIO 可以管理虚拟功能（VF），适用于虚拟化环境。
- **中断和 DMA 支持**：VFIO 支持将中断和 DMA 配置暴露给用户态程序。

##### 示例：
- `vfio-pci` 是常用的 VFIO 驱动模块。

##### 使用方式：
```bash
modprobe vfio-pci
```

---

#### 3. **DPDK 与内核驱动的关系**
- **内核驱动的作用**：
  - 初始化硬件（如网卡）。
  - 将硬件资源（寄存器、内存区域、中断等）暴露给用户态。
  - 不参与数据包的处理（数据包处理完全由用户态程序完成）。

- **用户态程序的作用**：
  - 通过映射的硬件资源直接操作网卡。
  - 使用轮询模式接收和发送数据包。
  - 完全绕过内核网络协议栈。

---

#### 4. **与传统内核驱动的区别**
传统内核网络驱动（如 `igb`、`ixgbe`）会直接处理数据包，并将数据包传递给内核网络协议栈。而 DPDK 使用的驱动（如 `igb_uio` 或 `vfio-pci`）只是将硬件资源暴露给用户态，数据包的处理完全由用户态程序完成。

---

#### 5. **总结**
- DPDK 在内核中有对应的驱动（如 `igb_uio` 或 `vfio-pci`），但这些驱动的作用与传统内核驱动不同。
- 内核驱动的主要任务是将硬件资源暴露给用户态，而不是处理数据包。
- 数据包的处理完全由用户态程序完成，绕过了内核网络协议栈。

通过这种设计，DPDK 实现了高性能的用户态网络数据包处理。

### ovs也是运行在用户态？

是的，**Open vSwitch (OVS)** 的主要数据路径（fast path）运行在用户态，而控制路径（control path）则运行在内核态或用户态，具体取决于 OVS 的部署方式。以下是对 OVS 架构的详细说明：

---

#### 1. **OVS 的架构**
OVS 分为两个主要部分：
- **用户态组件**：负责高性能的数据包处理。
- **内核态组件**：负责与内核交互和管理。

---

#### 2. **用户态组件**
##### **OVS 用户态数据路径（fast path）**
- **DPDK 加速模式**：
  - 当 OVS 使用 DPDK 时，数据包处理完全运行在用户态。
  - OVS 通过 DPDK 的 PMD（Poll Mode Driver）直接与网卡交互，绕过内核网络协议栈。
  - 数据包的处理（如匹配流表、执行动作）完全在用户态完成。

- **内核加速模式**：
  - 当 OVS 不使用 DPDK 时，数据包处理仍然主要运行在用户态，但部分操作可能依赖内核模块（如 `openvswitch.ko`）。

##### **用户态组件的主要功能**：
- **流表匹配**：根据数据包的特征匹配流表项。
- **执行动作**：根据流表项的指令执行动作（如转发、修改数据包等）。
- **与控制器通信**：通过 OpenFlow 协议与 SDN 控制器通信。

---

#### 3. **内核态组件**
##### **内核模块（`openvswitch.ko`）**
- 当 OVS 不使用 DPDK 时，内核模块负责以下任务：
  - 将数据包从内核网络栈传递到用户态。
  - 处理部分控制路径逻辑（如端口管理、VLAN 配置等）。
  - 提供与内核网络栈的兼容性。

- 当 OVS 使用 DPDK 时，内核模块的作用被最小化，甚至可以不加载。

##### **内核态组件的主要功能**：
- **与硬件交互**：在不使用 DPDK 时，内核模块负责与网卡驱动交互。
- **管理端口和 VLAN**：配置和管理虚拟交换机端口。
- **提供兼容性**：支持传统的内核网络栈功能。

---

#### 4. **OVS 的两种模式**
##### **DPDK 加速模式**
- 数据路径完全运行在用户态。
- 高性能，适合需要低延迟和高吞吐量的场景。
- 依赖 DPDK 用户态驱动（如 `vfio-pci`）。

##### **内核加速模式**
- 数据路径主要运行在用户态，但部分操作依赖内核模块。
- 性能较低，但兼容性更好。
- 依赖内核模块（如 `openvswitch.ko`）。

---

#### 5. **总结**
- **OVS 的主要数据路径运行在用户态**，无论是使用 DPDK 还是内核加速模式。
- 当使用 DPDK 时，OVS 完全绕过内核网络栈，数据包处理完全在用户态完成。
- 当不使用 DPDK 时，OVS 仍然主要运行在用户态，但部分操作依赖内核模块。
- 内核态组件主要用于管理和兼容性，而不是数据包处理。

因此，OVS 的设计目标是**将高性能的数据路径放在用户态**，同时通过内核模块提供兼容性和管理功能。

## dns

- [扫盲DNS缓存及客户端配置](https://mp.weixin.qq.com/s/r4yvPXu-DTYlmCflEE7o7A)
- [Github Pages 绑定域名遇到的坑](https://blog.csdn.net/i_do_not_know_you/article/details/105594269)

## proxy

macOS Monterey 12.7.5在系统偏好设置-网络-WiFi-代理中修改配置无法保存，clashx中设置为系统代理也无法取消，详情参见下图：

![代理](https://cdn.jsdelivr.net/gh/realwujing/picture-bed/20240603234753.png)

查看内核版本：

  ```bash
  uname -a
  Darwin MacBook-Pro.local 21.6.0 Darwin Kernel Version 21.6.0: Wed Apr 24 06:02:02 PDT 2024; root:xnu-8020.240.18.708.4~1/RELEASE_X86_64 x86_64
  ```

查看系统系统发型版本：

```bash
sw_vers
ProductName:    macOS
ProductVersion: 12.7.5
BuildVersion:   21H1222
```

在Finder中查看/Library/Preferences/SystemConfiguration/preferences.plist文件，发现图标左下角有把锁，打开简介，显示已锁定。

移除文件锁定：取消对 preferences.plist 文件的用户不可更改（uchg）标志，使其可被修改：

  ```bash
  sudo chflags nouchg /Library/Preferences/SystemConfiguration/preferences.plist
  ```

## wifi

- [Linux通过命令行连接wifi的方式](https://www.toutiao.com/article/7407405280704659994)
