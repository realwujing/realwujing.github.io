

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicon.jpg">
  <link rel="icon" href="/images/favicon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Wu Jing">
  <meta name="keywords" content="HTML, JavaScript, Hexo, Linux, qemu, C++, namespace, git, bcc, bpf, initramfs, k8s, architect, strings, assembly, linux">
  
    <meta name="description" content="大模型从0到1｜第一讲：概述和Tokenization  课程链接：Stanford CS336 Spring 2025 - Lecture 1   第一部分：课程概述与动机 1. 为什么需要这门课？ 当前AI教育的空白： - 大多数课程教你如何使用大模型（调用API、prompt engineering） - 一些课程教你如何微调现有模型（fine-tuning） - 但很少有课程教你如何从零构">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型从0到1｜第一讲：概述和Tokenization">
<meta property="og:url" content="https://realwujing.github.io/linux/drivers/gpu/stanford-cs336/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BB%8E0%E5%88%B01%EF%BD%9C%E7%AC%AC%E4%B8%80%E8%AE%B2%EF%BC%9A%E6%A6%82%E8%BF%B0%E5%92%8Ctokenization/index.html">
<meta property="og:site_name" content="WuJing&#39;s Blog">
<meta property="og:description" content="大模型从0到1｜第一讲：概述和Tokenization  课程链接：Stanford CS336 Spring 2025 - Lecture 1   第一部分：课程概述与动机 1. 为什么需要这门课？ 当前AI教育的空白： - 大多数课程教你如何使用大模型（调用API、prompt engineering） - 一些课程教你如何微调现有模型（fine-tuning） - 但很少有课程教你如何从零构">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-11-22T23:22:50.000Z">
<meta property="article:modified_time" content="2025-11-22T23:22:50.000Z">
<meta property="article:author" content="Wu Jing">
<meta property="article:tag" content="git">
<meta property="article:tag" content="architect">
<meta name="twitter:card" content="summary_large_image">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>大模型从0到1｜第一讲：概述和Tokenization - WuJing&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"realwujing.github.io","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"6b5123e146041483d13bdfaeb6e42a76","google":"UA-265632133-1","gtag":"G-E7BV6T4RCW","tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?6b5123e146041483d13bdfaeb6e42a76";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  
    <!-- Google Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript('https://www.google-analytics.com/analytics.js', function() {
          window.ga = window.ga || function() { (ga.q = ga.q || []).push(arguments) };
          ga.l = +new Date;
          ga('create', 'UA-265632133-1', 'auto');
          ga('send', 'pageview');
        });
      }
    </script>
  

  
    <!-- Google gtag.js -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript('https://www.googletagmanager.com/gtag/js?id=G-E7BV6T4RCW', function() {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-E7BV6T4RCW');
        });
      }
    </script>
  

  

  

  

  



  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>WuJing&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="大模型从0到1｜第一讲：概述和Tokenization"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-11-22 23:22" pubdate>
          2025年11月22日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          55k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          460 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">大模型从0到1｜第一讲：概述和Tokenization</h1>
            
            
              <div class="markdown-body">
                
                <h1
id="大模型从0到1第一讲概述和tokenization">大模型从0到1｜第一讲：概述和Tokenization</h1>
<blockquote>
<p>课程链接：<a
target="_blank" rel="noopener" href="https://stanford-cs336.github.io/spring2025-lectures/?trace=var/traces/lecture_01.json">Stanford
CS336 Spring 2025 - Lecture 1</a></p>
</blockquote>
<hr />
<h2 id="第一部分课程概述与动机">第一部分：课程概述与动机</h2>
<h3 id="为什么需要这门课">1. 为什么需要这门课？</h3>
<p><strong>当前AI教育的空白</strong>： -
大多数课程教你如何<strong>使用</strong>大模型（调用API、prompt
engineering） -
一些课程教你如何<strong>微调</strong>现有模型（fine-tuning） -
但很少有课程教你如何<strong>从零构建</strong>一个大模型</p>
<p><strong>CS336的独特定位</strong>： <figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><span class="hljs-section">其他课程: 使用者视角 → 如何调用模型</span><br><span class="hljs-section">CS336:    构建者视角 → 如何创造模型</span><br></code></pre></td></tr></table></figure></p>
<p><strong>课程目标</strong>： 1. 理解大模型的每一个组件 2.
能够从零实现一个完整的语言模型 3. 掌握训练、评估、部署的完整流程 4.
理解设计决策背后的原理</p>
<h3 id="课程结构从0到1的完整路径">2. 课程结构：从0到1的完整路径</h3>
<p><strong>第一阶段：基础组件（Weeks 1-3）</strong> <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Week</span> <span class="hljs-number">1</span>: Tokenization          ← 本讲<br><span class="hljs-attribute">Week</span> <span class="hljs-number">2</span>: Model Architecture    <br><span class="hljs-attribute">Week</span> <span class="hljs-number">3</span>: Training Basics<br></code></pre></td></tr></table></figure></p>
<p><strong>第二阶段：预训练（Weeks 4-6）</strong> <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Week</span> <span class="hljs-number">4</span>: Data Processing<br><span class="hljs-attribute">Week</span> <span class="hljs-number">5</span>: Optimization<br><span class="hljs-attribute">Week</span> <span class="hljs-number">6</span>: Distributed Training<br></code></pre></td></tr></table></figure></p>
<p><strong>第三阶段：对齐与应用（Weeks 7-9）</strong> <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Week</span> <span class="hljs-number">7</span>: Instruction Tuning<br><span class="hljs-attribute">Week</span> <span class="hljs-number">8</span>: RLHF<br><span class="hljs-attribute">Week</span> <span class="hljs-number">9</span>: Evaluation &amp; Deployment<br></code></pre></td></tr></table></figure></p>
<p><strong>第四阶段：前沿话题（Weeks 10-12）</strong> <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Week</span> <span class="hljs-number">10</span>: Scaling Laws<br><span class="hljs-attribute">Week</span> <span class="hljs-number">11</span>: Efficient Training<br><span class="hljs-attribute">Week</span> <span class="hljs-number">12</span>: Future Directions<br></code></pre></td></tr></table></figure></p>
<h3 id="学习成果">3. 学习成果</h3>
<p>完成本课程后，你将能够：</p>
<p><strong>理论层面</strong>： - 解释每个组件的设计原理 -
理解不同架构选择的权衡 - 掌握训练大模型的关键技术</p>
<p><strong>实践层面</strong>： - 从零实现一个Transformer模型 -
训练一个小规模但完整的语言模型 - 评估和优化模型性能</p>
<p><strong>工程层面</strong>： - 处理大规模数据 - 实现分布式训练 -
优化推理性能</p>
<hr />
<h2 id="第二部分什么是tokenization">第二部分：什么是Tokenization？</h2>
<h3 id="核心概念">1. 核心概念</h3>
<p><strong>定义</strong>：Tokenization是将原始文本转换为模型可处理的数字序列的过程。</p>
<p><strong>完整流程</strong>： <figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs scss">原始文本 (Raw Text)<br>    ↓<br>分词 (Tokenization)<br>    ↓<br>Token序列 (Token Sequence)<br>    ↓<br>Token ID序列 (Token ID Sequence)<br>    ↓<br>嵌入向量 (Embedding Vectors)<br>    ↓<br>模型输入 (Model Input)<br></code></pre></td></tr></table></figure></p>
<p><strong>示例</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 原始文本</span><br>text = <span class="hljs-string">&quot;Hello, world!&quot;</span><br><br><span class="hljs-comment"># 分词</span><br>tokens = [<span class="hljs-string">&quot;Hello&quot;</span>, <span class="hljs-string">&quot;,&quot;</span>, <span class="hljs-string">&quot; world&quot;</span>, <span class="hljs-string">&quot;!&quot;</span>]<br><br><span class="hljs-comment"># Token IDs</span><br>token_ids = [<span class="hljs-number">15496</span>, <span class="hljs-number">11</span>, <span class="hljs-number">995</span>, <span class="hljs-number">0</span>]<br><br><span class="hljs-comment"># 嵌入向量（假设d_model=768）</span><br>embeddings = [<br>    [<span class="hljs-number">0.123</span>, -<span class="hljs-number">0.456</span>, ..., <span class="hljs-number">0.789</span>],  <span class="hljs-comment"># &quot;Hello&quot;的768维向量</span><br>    [<span class="hljs-number">0.234</span>, -<span class="hljs-number">0.567</span>, ..., <span class="hljs-number">0.890</span>],  <span class="hljs-comment"># &quot;,&quot;的768维向量</span><br>    [<span class="hljs-number">0.345</span>, -<span class="hljs-number">0.678</span>, ..., <span class="hljs-number">0.901</span>],  <span class="hljs-comment"># &quot; world&quot;的768维向量</span><br>    [<span class="hljs-number">0.456</span>, -<span class="hljs-number">0.789</span>, ..., <span class="hljs-number">0.012</span>],  <span class="hljs-comment"># &quot;!&quot;的768维向量</span><br>]<br></code></pre></td></tr></table></figure></p>
<h3 id="为什么需要tokenization">2. 为什么需要Tokenization？</h3>
<p><strong>问题1：神经网络只能处理数字</strong> <figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">文本</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;cat&quot;</span><br><span class="hljs-attribute">❌ 不能直接输入神经网络</span><br><span class="hljs-attribute">✓ 需要转换为数字</span><span class="hljs-punctuation">:</span> <span class="hljs-string">[3415]</span><br><span class="hljs-attribute">✓ 再转换为向量</span><span class="hljs-punctuation">:</span> <span class="hljs-string">[0.1, -0.2, 0.3, ...]</span><br></code></pre></td></tr></table></figure></p>
<p><strong>问题2：直接使用字符效率低</strong> <figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs prolog">文本: <span class="hljs-string">&quot;The cat sat on the mat&quot;</span><br>字符级: [<span class="hljs-string">&#x27;T&#x27;</span>,<span class="hljs-string">&#x27;h&#x27;</span>,<span class="hljs-string">&#x27;e&#x27;</span>,<span class="hljs-string">&#x27; &#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;t&#x27;</span>,<span class="hljs-string">&#x27; &#x27;</span>,<span class="hljs-string">&#x27;s&#x27;</span>,<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;t&#x27;</span>,<span class="hljs-string">&#x27; &#x27;</span>,<span class="hljs-string">&#x27;o&#x27;</span>,<span class="hljs-string">&#x27;n&#x27;</span>,<span class="hljs-string">&#x27; &#x27;</span>,<span class="hljs-string">&#x27;t&#x27;</span>,<span class="hljs-string">&#x27;h&#x27;</span>,<span class="hljs-string">&#x27;e&#x27;</span>,<span class="hljs-string">&#x27; &#x27;</span>,<span class="hljs-string">&#x27;m&#x27;</span>,<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;t&#x27;</span>]<br>长度: <span class="hljs-number">22</span>个字符<br><br>子词级: [<span class="hljs-string">&#x27;The&#x27;</span>, <span class="hljs-string">&#x27; cat&#x27;</span>, <span class="hljs-string">&#x27; sat&#x27;</span>, <span class="hljs-string">&#x27; on&#x27;</span>, <span class="hljs-string">&#x27; the&#x27;</span>, <span class="hljs-string">&#x27; mat&#x27;</span>]<br>长度: <span class="hljs-number">6</span>个token<br></code></pre></td></tr></table></figure></p>
<p><strong>问题3：直接使用单词词汇表爆炸</strong> <figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><span class="hljs-section">英语单词数: ~170,000</span><br><span class="hljs-section">加上变形: ~500,000+</span><br><span class="hljs-section">加上专有名词: 无限</span><br><br><span class="hljs-section">子词方案: 30,000-50,000个token即可覆盖</span><br></code></pre></td></tr></table></figure></p>
<h3 id="token粒度的选择">3. Token粒度的选择</h3>
<table style="width:100%;">

<thead>
<tr class="header">
<th style="text-align: left;">粒度</th>
<th style="text-align: left;">词汇表大小</th>
<th style="text-align: left;">序列长度</th>
<th style="text-align: left;">优点</th>
<th style="text-align: left;">缺点</th>
<th style="text-align: left;">使用场景</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>字符级</strong></td>
<td style="text-align: left;">很小（~100）</td>
<td style="text-align: left;">很长</td>
<td style="text-align: left;">无OOV，简单</td>
<td style="text-align: left;">序列太长，难学习</td>
<td style="text-align: left;">拼写检查</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>子词级</strong></td>
<td style="text-align: left;">中等（30k-50k）</td>
<td style="text-align: left;">中等</td>
<td style="text-align: left;">平衡好</td>
<td style="text-align: left;">需要训练</td>
<td style="text-align: left;"><strong>现代LLM</strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>单词级</strong></td>
<td style="text-align: left;">很大（&gt;100k）</td>
<td style="text-align: left;">短</td>
<td style="text-align: left;">语义清晰</td>
<td style="text-align: left;">OOV严重</td>
<td style="text-align: left;">传统NLP</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>字节级</strong></td>
<td style="text-align: left;">256</td>
<td style="text-align: left;">最长</td>
<td style="text-align: left;">通用性强</td>
<td style="text-align: left;">序列极长</td>
<td style="text-align: left;">多语言</td>
</tr>
</tbody>
</table>
<p><strong>实际对比</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">text = <span class="hljs-string">&quot;The tokenization process is important.&quot;</span><br><br><span class="hljs-comment"># 字符级（包括空格）</span><br>char_tokens = [<span class="hljs-string">&#x27;T&#x27;</span>,<span class="hljs-string">&#x27;h&#x27;</span>,<span class="hljs-string">&#x27;e&#x27;</span>,<span class="hljs-string">&#x27; &#x27;</span>,<span class="hljs-string">&#x27;t&#x27;</span>,<span class="hljs-string">&#x27;o&#x27;</span>,<span class="hljs-string">&#x27;k&#x27;</span>,<span class="hljs-string">&#x27;e&#x27;</span>,<span class="hljs-string">&#x27;n&#x27;</span>,<span class="hljs-string">&#x27;i&#x27;</span>,<span class="hljs-string">&#x27;z&#x27;</span>,<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;t&#x27;</span>,<span class="hljs-string">&#x27;i&#x27;</span>,<span class="hljs-string">&#x27;o&#x27;</span>,<span class="hljs-string">&#x27;n&#x27;</span>,<span class="hljs-string">&#x27; &#x27;</span>,<span class="hljs-string">&#x27;p&#x27;</span>,<span class="hljs-string">&#x27;r&#x27;</span>,<span class="hljs-string">&#x27;o&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;e&#x27;</span>,<span class="hljs-string">&#x27;s&#x27;</span>,<span class="hljs-string">&#x27;s&#x27;</span>,<span class="hljs-string">&#x27; &#x27;</span>,<span class="hljs-string">&#x27;i&#x27;</span>,<span class="hljs-string">&#x27;s&#x27;</span>,<span class="hljs-string">&#x27; &#x27;</span>,<span class="hljs-string">&#x27;i&#x27;</span>,<span class="hljs-string">&#x27;m&#x27;</span>,<span class="hljs-string">&#x27;p&#x27;</span>,<span class="hljs-string">&#x27;o&#x27;</span>,<span class="hljs-string">&#x27;r&#x27;</span>,<span class="hljs-string">&#x27;t&#x27;</span>,<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;n&#x27;</span>,<span class="hljs-string">&#x27;t&#x27;</span>,<span class="hljs-string">&#x27;.&#x27;</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;字符级长度: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(char_tokens)&#125;</span>&quot;</span>)  <span class="hljs-comment"># 38</span><br><br><span class="hljs-comment"># 子词级（GPT-2 tokenizer）</span><br>subword_tokens = [<span class="hljs-string">&#x27;The&#x27;</span>, <span class="hljs-string">&#x27; token&#x27;</span>, <span class="hljs-string">&#x27;ization&#x27;</span>, <span class="hljs-string">&#x27; process&#x27;</span>, <span class="hljs-string">&#x27; is&#x27;</span>, <span class="hljs-string">&#x27; important&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;子词级长度: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(subword_tokens)&#125;</span>&quot;</span>)  <span class="hljs-comment"># 7</span><br><br><span class="hljs-comment"># 单词级</span><br>word_tokens = [<span class="hljs-string">&#x27;The&#x27;</span>, <span class="hljs-string">&#x27;tokenization&#x27;</span>, <span class="hljs-string">&#x27;process&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;important&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;单词级长度: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(word_tokens)&#125;</span>&quot;</span>)  <span class="hljs-comment"># 6</span><br></code></pre></td></tr></table></figure></p>
<p><strong>为什么子词级是最佳选择？</strong></p>
<ol type="1">
<li><p><strong>处理未知词</strong>： <figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs prolog">单词级: <span class="hljs-string">&quot;ChatGPT&quot;</span> → [<span class="hljs-symbol">UNK</span>]（未知）<br>子词级: <span class="hljs-string">&quot;ChatGPT&quot;</span> → [<span class="hljs-string">&quot;Chat&quot;</span>, <span class="hljs-string">&quot;G&quot;</span>, <span class="hljs-string">&quot;PT&quot;</span>]（可以表示）<br></code></pre></td></tr></table></figure></p></li>
<li><p><strong>学习词根和词缀</strong>： <figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-string">&quot;play&quot;</span>, <span class="hljs-string">&quot;player&quot;</span>, <span class="hljs-string">&quot;playing&quot;</span>, <span class="hljs-string">&quot;played&quot;</span><br>子词级可以学到 <span class="hljs-string">&quot;play&quot;</span> 是共同的词根<br></code></pre></td></tr></table></figure></p></li>
<li><p><strong>多语言支持</strong>： <figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs prolog">英语: <span class="hljs-string">&quot;hello&quot;</span> → [<span class="hljs-string">&quot;hello&quot;</span>]<br>中文: <span class="hljs-string">&quot;你好&quot;</span> → [<span class="hljs-string">&quot;你&quot;</span>, <span class="hljs-string">&quot;好&quot;</span>]<br>都可以用同一个tokenizer处理<br></code></pre></td></tr></table></figure></p></li>
</ol>
<hr />
<h2
id="第三部分主流tokenization算法">第三部分：主流Tokenization算法</h2>
<h3 id="bpe-byte-pair-encoding">1. BPE (Byte-Pair Encoding)</h3>
<h4 id="算法原理">1.1 算法原理</h4>
<p><strong>历史</strong>：BPE最初是1994年提出的数据压缩算法，2016年被Sennrich等人引入NLP。</p>
<p><strong>核心思想</strong>：从字符开始，迭代地合并最频繁出现的相邻字符对。</p>
<p><strong>算法流程</strong>： <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">1.</span> 初始化：将文本分割为字符<br><span class="hljs-bullet">2.</span> 统计：计算所有相邻字符对的频率<br><span class="hljs-bullet">3.</span> 合并：选择频率最高的字符对，合并为新token<br><span class="hljs-bullet">4.</span> 更新：更新词汇表和文本<br><span class="hljs-bullet">5.</span> 重复：重复步骤2-4，直到达到目标词汇表大小<br></code></pre></td></tr></table></figure></p>
<h4 id="详细示例">1.2 详细示例</h4>
<p><strong>训练语料</strong>： <figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk"><span class="hljs-comment">&quot;low low low low low&quot;</span><br><span class="hljs-comment">&quot;lower lower&quot;</span><br><span class="hljs-comment">&quot;newest newest newest newest newest newest&quot;</span><br></code></pre></td></tr></table></figure></p>
<p><strong>训练过程</strong>：</p>
<p><strong>步骤0：初始化</strong> <figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs gcode">词汇表: [<span class="hljs-string">&#x27;l&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>, <span class="hljs-string">&#x27;&lt;/w&gt;&#x27;</span>]<br>文本表示:<br><span class="hljs-string">&quot;l o w &lt;/w&gt;&quot;</span> <span class="hljs-comment">(出现5次)</span><br><span class="hljs-string">&quot;l o w e r &lt;/w&gt;&quot;</span> <span class="hljs-comment">(出现2次)</span><br><span class="hljs-string">&quot;n e w e s t &lt;/w&gt;&quot;</span> <span class="hljs-comment">(出现6次)</span><br></code></pre></td></tr></table></figure></p>
<p><strong>步骤1：统计字符对频率</strong> <figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs arcade"><span class="hljs-string">&#x27;l o&#x27;</span>: <span class="hljs-number">7</span>次 (<span class="hljs-number">5</span>次在low, <span class="hljs-number">2</span>次在<span class="hljs-built_in">lower</span>)<br><span class="hljs-string">&#x27;o w&#x27;</span>: <span class="hljs-number">7</span>次<br><span class="hljs-string">&#x27;w &lt;/w&gt;&#x27;</span>: <span class="hljs-number">5</span>次<br><span class="hljs-string">&#x27;w e&#x27;</span>: <span class="hljs-number">8</span>次 (<span class="hljs-number">2</span>次在<span class="hljs-built_in">lower</span>, <span class="hljs-number">6</span>次在newest)<br><span class="hljs-string">&#x27;e w&#x27;</span>: <span class="hljs-number">6</span>次<br><span class="hljs-string">&#x27;e s&#x27;</span>: <span class="hljs-number">6</span>次<br><span class="hljs-string">&#x27;s t&#x27;</span>: <span class="hljs-number">6</span>次<br><span class="hljs-string">&#x27;t &lt;/w&gt;&#x27;</span>: <span class="hljs-number">6</span>次<br>...<br></code></pre></td></tr></table></figure></p>
<p><strong>步骤2：合并最频繁的对 'w e' (8次)</strong> <figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs gcode">词汇表: [<span class="hljs-string">&#x27;l&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>, <span class="hljs-string">&#x27;&lt;/w&gt;&#x27;</span>, <span class="hljs-string">&#x27;we&#x27;</span>]<br>文本表示:<br><span class="hljs-string">&quot;l o w &lt;/w&gt;&quot;</span> <span class="hljs-comment">(5次)</span><br><span class="hljs-string">&quot;l o we r &lt;/w&gt;&quot;</span> <span class="hljs-comment">(2次)</span><br><span class="hljs-string">&quot;n e we s t &lt;/w&gt;&quot;</span> <span class="hljs-comment">(6次)</span><br></code></pre></td></tr></table></figure></p>
<p><strong>步骤3：继续合并 'e we' (6次)</strong> <figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs gcode">词汇表: [..., <span class="hljs-string">&#x27;we&#x27;</span>, <span class="hljs-string">&#x27;ewe&#x27;</span>]<br>文本表示:<br><span class="hljs-string">&quot;l o w &lt;/w&gt;&quot;</span> <span class="hljs-comment">(5次)</span><br><span class="hljs-string">&quot;l o we r &lt;/w&gt;&quot;</span> <span class="hljs-comment">(2次)</span><br><span class="hljs-string">&quot;n ewe s t &lt;/w&gt;&quot;</span> <span class="hljs-comment">(6次)</span><br></code></pre></td></tr></table></figure></p>
<p><strong>步骤4：合并 'l o' (7次)</strong> <figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs gcode">词汇表: [..., <span class="hljs-string">&#x27;lo&#x27;</span>]<br>文本表示:<br><span class="hljs-string">&quot;lo w &lt;/w&gt;&quot;</span> <span class="hljs-comment">(5次)</span><br><span class="hljs-string">&quot;lo we r &lt;/w&gt;&quot;</span> <span class="hljs-comment">(2次)</span><br><span class="hljs-string">&quot;n ewe s t &lt;/w&gt;&quot;</span> <span class="hljs-comment">(6次)</span><br></code></pre></td></tr></table></figure></p>
<p><strong>继续迭代...</strong></p>
<p><strong>最终词汇表可能包含</strong>： <figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs less">基础字符: <span class="hljs-selector-attr">[<span class="hljs-string">&#x27;l&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>, <span class="hljs-string">&#x27;&lt;/w&gt;&#x27;</span>]</span><br>合并的<span class="hljs-selector-tag">token</span>: <span class="hljs-selector-attr">[<span class="hljs-string">&#x27;lo&#x27;</span>, <span class="hljs-string">&#x27;low&#x27;</span>, <span class="hljs-string">&#x27;we&#x27;</span>, <span class="hljs-string">&#x27;ewe&#x27;</span>, <span class="hljs-string">&#x27;est&#x27;</span>, <span class="hljs-string">&#x27;west&#x27;</span>, <span class="hljs-string">&#x27;newest&#x27;</span>, ...]</span><br></code></pre></td></tr></table></figure></p>
<h4 id="python实现">1.3 Python实现</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict, Counter<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BPETokenizer</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size=<span class="hljs-number">1000</span></span>):<br>        self.vocab_size = vocab_size<br>        self.vocab = &#123;&#125;<br>        self.merges = []<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_stats</span>(<span class="hljs-params">self, vocab</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;统计相邻token对的频率&quot;&quot;&quot;</span><br>        pairs = defaultdict(<span class="hljs-built_in">int</span>)<br>        <span class="hljs-keyword">for</span> word, freq <span class="hljs-keyword">in</span> vocab.items():<br>            symbols = word.split()<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(symbols) - <span class="hljs-number">1</span>):<br>                pairs[symbols[i], symbols[i + <span class="hljs-number">1</span>]] += freq<br>        <span class="hljs-keyword">return</span> pairs<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">merge_vocab</span>(<span class="hljs-params">self, pair, vocab</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;合并指定的token对&quot;&quot;&quot;</span><br>        new_vocab = &#123;&#125;<br>        bigram = <span class="hljs-string">&#x27; &#x27;</span>.join(pair)<br>        replacement = <span class="hljs-string">&#x27;&#x27;</span>.join(pair)<br>        <br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> vocab:<br>            new_word = word.replace(bigram, replacement)<br>            new_vocab[new_word] = vocab[word]<br>        <br>        <span class="hljs-keyword">return</span> new_vocab<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, texts</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;训练BPE tokenizer&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 1. 初始化词汇表（字符级）</span><br>        vocab = defaultdict(<span class="hljs-built_in">int</span>)<br>        <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>            words = text.split()<br>            <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>                <span class="hljs-comment"># 添加词尾标记</span><br>                vocab[<span class="hljs-string">&#x27; &#x27;</span>.join(<span class="hljs-built_in">list</span>(word)) + <span class="hljs-string">&#x27; &lt;/w&gt;&#x27;</span>] += <span class="hljs-number">1</span><br>        <br>        <span class="hljs-comment"># 2. 迭代合并</span><br>        num_merges = self.vocab_size - <span class="hljs-built_in">len</span>(<span class="hljs-built_in">set</span>(<span class="hljs-string">&#x27;&#x27;</span>.join(texts)))<br>        <br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_merges):<br>            pairs = self.get_stats(vocab)<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> pairs:<br>                <span class="hljs-keyword">break</span><br>            <br>            <span class="hljs-comment"># 选择频率最高的pair</span><br>            best_pair = <span class="hljs-built_in">max</span>(pairs, key=pairs.get)<br>            vocab = self.merge_vocab(best_pair, vocab)<br>            <br>            <span class="hljs-comment"># 记录合并操作</span><br>            self.merges.append(best_pair)<br>            <br>            <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Merge <span class="hljs-subst">&#123;i+<span class="hljs-number">1</span>&#125;</span>: <span class="hljs-subst">&#123;best_pair&#125;</span> (freq: <span class="hljs-subst">&#123;pairs[best_pair]&#125;</span>)&quot;</span>)<br>        <br>        <span class="hljs-comment"># 3. 构建最终词汇表</span><br>        self.vocab = self._build_vocab(vocab)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_vocab</span>(<span class="hljs-params">self, vocab</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;从训练结果构建词汇表&quot;&quot;&quot;</span><br>        tokens = <span class="hljs-built_in">set</span>()<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> vocab.keys():<br>            tokens.update(word.split())<br>        <span class="hljs-keyword">return</span> &#123;token: idx <span class="hljs-keyword">for</span> idx, token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">sorted</span>(tokens))&#125;<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">self, text</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;使用训练好的BPE进行分词&quot;&quot;&quot;</span><br>        words = text.split()<br>        tokens = []<br>        <br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>            <span class="hljs-comment"># 初始化为字符级</span><br>            word_tokens = <span class="hljs-built_in">list</span>(word) + [<span class="hljs-string">&#x27;&lt;/w&gt;&#x27;</span>]<br>            <br>            <span class="hljs-comment"># 应用学到的合并规则</span><br>            <span class="hljs-keyword">for</span> pair <span class="hljs-keyword">in</span> self.merges:<br>                i = <span class="hljs-number">0</span><br>                <span class="hljs-keyword">while</span> i &lt; <span class="hljs-built_in">len</span>(word_tokens) - <span class="hljs-number">1</span>:<br>                    <span class="hljs-keyword">if</span> (word_tokens[i], word_tokens[i + <span class="hljs-number">1</span>]) == pair:<br>                        word_tokens = (word_tokens[:i] + <br>                                     [<span class="hljs-string">&#x27;&#x27;</span>.join(pair)] + <br>                                     word_tokens[i + <span class="hljs-number">2</span>:])<br>                    <span class="hljs-keyword">else</span>:<br>                        i += <span class="hljs-number">1</span><br>            <br>            tokens.extend(word_tokens)<br>        <br>        <span class="hljs-keyword">return</span> tokens<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, text</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;将文本编码为token IDs&quot;&quot;&quot;</span><br>        tokens = self.tokenize(text)<br>        <span class="hljs-keyword">return</span> [self.vocab.get(token, self.vocab.get(<span class="hljs-string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="hljs-number">0</span>)) <br>                <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> tokens]<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params">self, token_ids</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;将token IDs解码为文本&quot;&quot;&quot;</span><br>        inv_vocab = &#123;v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> self.vocab.items()&#125;<br>        tokens = [inv_vocab.get(tid, <span class="hljs-string">&#x27;&lt;unk&gt;&#x27;</span>) <span class="hljs-keyword">for</span> tid <span class="hljs-keyword">in</span> token_ids]<br>        text = <span class="hljs-string">&#x27;&#x27;</span>.join(tokens).replace(<span class="hljs-string">&#x27;&lt;/w&gt;&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>).strip()<br>        <span class="hljs-keyword">return</span> text<br><br><span class="hljs-comment"># 使用示例</span><br>texts = [<br>    <span class="hljs-string">&quot;low low low low low&quot;</span>,<br>    <span class="hljs-string">&quot;lower lower&quot;</span>,<br>    <span class="hljs-string">&quot;newest newest newest newest newest newest&quot;</span>,<br>    <span class="hljs-string">&quot;widest widest widest&quot;</span><br>]<br><br>tokenizer = BPETokenizer(vocab_size=<span class="hljs-number">50</span>)<br>tokenizer.train(texts)<br><br><span class="hljs-comment"># 测试</span><br>test_text = <span class="hljs-string">&quot;lowest newer&quot;</span><br>tokens = tokenizer.tokenize(test_text)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Tokens: <span class="hljs-subst">&#123;tokens&#125;</span>&quot;</span>)<br><br>ids = tokenizer.encode(test_text)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Token IDs: <span class="hljs-subst">&#123;ids&#125;</span>&quot;</span>)<br><br>decoded = tokenizer.decode(ids)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Decoded: <span class="hljs-subst">&#123;decoded&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<h4 id="byte-level-bpe">1.4 Byte-Level BPE</h4>
<p><strong>GPT-2引入的改进</strong>：在字节级别而不是字符级别操作。</p>
<p><strong>优势</strong>： <figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs x86asm">字符级BPE: 需要处理所有Unicode字符（&gt;100k）<br>字节级BPE: 只需要处理<span class="hljs-number">256</span>个字节<br><br>示例：<br><span class="hljs-string">&quot;你好&quot;</span> (Unicode字符)<br>→ UTF-<span class="hljs-number">8</span>编码: [<span class="hljs-number">0xE4</span>, <span class="hljs-number">0xBD</span>, <span class="hljs-number">0xA0</span>, <span class="hljs-number">0xE5</span>, <span class="hljs-number">0xA5</span>, <span class="hljs-number">0xBD</span>]<br>→ <span class="hljs-number">6</span>个字节<br>→ 可以用<span class="hljs-number">256</span>个基础token表示任何文本<br></code></pre></td></tr></table></figure></p>
<p><strong>实现</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">bytes_to_unicode</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    创建字节到Unicode字符的映射</span><br><span class="hljs-string">    避免使用控制字符和空白字符</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    bs = (<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">ord</span>(<span class="hljs-string">&quot;!&quot;</span>), <span class="hljs-built_in">ord</span>(<span class="hljs-string">&quot;~&quot;</span>) + <span class="hljs-number">1</span>)) + <br>          <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">ord</span>(<span class="hljs-string">&quot;¡&quot;</span>), <span class="hljs-built_in">ord</span>(<span class="hljs-string">&quot;¬&quot;</span>) + <span class="hljs-number">1</span>)) + <br>          <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">ord</span>(<span class="hljs-string">&quot;®&quot;</span>), <span class="hljs-built_in">ord</span>(<span class="hljs-string">&quot;ÿ&quot;</span>) + <span class="hljs-number">1</span>)))<br>    <br>    cs = bs[:]<br>    n = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>**<span class="hljs-number">8</span>):<br>        <span class="hljs-keyword">if</span> b <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> bs:<br>            bs.append(b)<br>            cs.append(<span class="hljs-number">2</span>**<span class="hljs-number">8</span> + n)<br>            n += <span class="hljs-number">1</span><br>    <br>    cs = [<span class="hljs-built_in">chr</span>(n) <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> cs]<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(bs, cs))<br><br><span class="hljs-comment"># GPT-2使用的映射</span><br>byte_encoder = bytes_to_unicode()<br>byte_decoder = &#123;v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> byte_encoder.items()&#125;<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode_text</span>(<span class="hljs-params">text</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;将文本编码为字节序列&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> [byte_encoder[b] <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> text.encode(<span class="hljs-string">&#x27;utf-8&#x27;</span>)]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">decode_bytes</span>(<span class="hljs-params">byte_tokens</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;将字节序列解码为文本&quot;&quot;&quot;</span><br>    byte_array = <span class="hljs-built_in">bytes</span>([byte_decoder[c] <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> byte_tokens])<br>    <span class="hljs-keyword">return</span> byte_array.decode(<span class="hljs-string">&#x27;utf-8&#x27;</span>, errors=<span class="hljs-string">&#x27;replace&#x27;</span>)<br></code></pre></td></tr></table></figure></p>
<h3 id="wordpiece">2. WordPiece</h3>
<h4 id="算法原理-1">2.1 算法原理</h4>
<p><strong>历史</strong>：由Google开发，用于BERT等模型。</p>
<p><strong>与BPE的区别</strong>： <figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">BPE:</span>       选择频率最高的字符对<br><span class="hljs-symbol">WordPiece:</span> 选择能最大化语言模型似然度的字符对<br></code></pre></td></tr></table></figure></p>
<p><strong>合并准则</strong>： <figure class="highlight erlang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs erlang"><span class="hljs-function"><span class="hljs-title">score</span><span class="hljs-params">(x, y)</span> = <span class="hljs-title">log</span> P<span class="hljs-params">(xy)</span> - <span class="hljs-title">log</span> P<span class="hljs-params">(x)</span> - <span class="hljs-title">log</span> P<span class="hljs-params">(y)</span></span><br><span class="hljs-function">            = <span class="hljs-title">log</span> [P<span class="hljs-params">(xy)</span> / <span class="hljs-params">(P(x)</span> × P<span class="hljs-params">(y)</span>)]</span><br><span class="hljs-function"></span><br><span class="hljs-function">选择<span class="hljs-title">score</span>最大的<span class="hljs-title">pair</span>进行合并</span><br></code></pre></td></tr></table></figure></p>
<p><strong>直觉理解</strong>： - 如果x和y经常一起出现，P(xy)会很大 -
如果它们独立出现也很频繁，P(x)和P(y)也大 -
score衡量的是"一起出现"相对于"独立出现"的增益</p>
<h4 id="实现示例">2.2 实现示例</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WordPieceTokenizer</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size=<span class="hljs-number">1000</span></span>):<br>        self.vocab_size = vocab_size<br>        self.vocab = &#123;&#125;<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_pair_scores</span>(<span class="hljs-params">self, vocab</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;计算每个pair的score&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 统计单个token和pair的频率</span><br>        token_freq = defaultdict(<span class="hljs-built_in">int</span>)<br>        pair_freq = defaultdict(<span class="hljs-built_in">int</span>)<br>        total = <span class="hljs-number">0</span><br>        <br>        <span class="hljs-keyword">for</span> word, freq <span class="hljs-keyword">in</span> vocab.items():<br>            symbols = word.split()<br>            <span class="hljs-keyword">for</span> symbol <span class="hljs-keyword">in</span> symbols:<br>                token_freq[symbol] += freq<br>                total += freq<br>            <br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(symbols) - <span class="hljs-number">1</span>):<br>                pair = (symbols[i], symbols[i + <span class="hljs-number">1</span>])<br>                pair_freq[pair] += freq<br>        <br>        <span class="hljs-comment"># 计算score</span><br>        scores = &#123;&#125;<br>        <span class="hljs-keyword">for</span> pair, freq <span class="hljs-keyword">in</span> pair_freq.items():<br>            x, y = pair<br>            <span class="hljs-comment"># score = log(P(xy) / (P(x) * P(y)))</span><br>            p_xy = freq / total<br>            p_x = token_freq[x] / total<br>            p_y = token_freq[y] / total<br>            <br>            <span class="hljs-keyword">if</span> p_x &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> p_y &gt; <span class="hljs-number">0</span>:<br>                scores[pair] = math.log(p_xy / (p_x * p_y))<br>        <br>        <span class="hljs-keyword">return</span> scores<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, texts</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;训练WordPiece tokenizer&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 初始化</span><br>        vocab = defaultdict(<span class="hljs-built_in">int</span>)<br>        <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>            words = text.split()<br>            <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>                vocab[<span class="hljs-string">&#x27; &#x27;</span>.join(<span class="hljs-built_in">list</span>(word)) + <span class="hljs-string">&#x27; &lt;/w&gt;&#x27;</span>] += <span class="hljs-number">1</span><br>        <br>        <span class="hljs-comment"># 迭代合并</span><br>        num_merges = self.vocab_size - <span class="hljs-built_in">len</span>(<span class="hljs-built_in">set</span>(<span class="hljs-string">&#x27;&#x27;</span>.join(texts)))<br>        <br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_merges):<br>            scores = self.get_pair_scores(vocab)<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> scores:<br>                <span class="hljs-keyword">break</span><br>            <br>            <span class="hljs-comment"># 选择score最高的pair</span><br>            best_pair = <span class="hljs-built_in">max</span>(scores, key=scores.get)<br>            vocab = self.merge_vocab(best_pair, vocab)<br>            <br>            <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Merge <span class="hljs-subst">&#123;i+<span class="hljs-number">1</span>&#125;</span>: <span class="hljs-subst">&#123;best_pair&#125;</span> (score: <span class="hljs-subst">&#123;scores[best_pair]:<span class="hljs-number">.4</span>f&#125;</span>)&quot;</span>)<br>        <br>        self.vocab = self._build_vocab(vocab)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">merge_vocab</span>(<span class="hljs-params">self, pair, vocab</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;合并指定的pair&quot;&quot;&quot;</span><br>        new_vocab = &#123;&#125;<br>        bigram = <span class="hljs-string">&#x27; &#x27;</span>.join(pair)<br>        replacement = <span class="hljs-string">&#x27;&#x27;</span>.join(pair)<br>        <br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> vocab:<br>            new_word = word.replace(bigram, replacement)<br>            new_vocab[new_word] = vocab[word]<br>        <br>        <span class="hljs-keyword">return</span> new_vocab<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_vocab</span>(<span class="hljs-params">self, vocab</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;构建词汇表&quot;&quot;&quot;</span><br>        tokens = <span class="hljs-built_in">set</span>()<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> vocab.keys():<br>            tokens.update(word.split())<br>        <span class="hljs-keyword">return</span> &#123;token: idx <span class="hljs-keyword">for</span> idx, token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">sorted</span>(tokens))&#125;<br></code></pre></td></tr></table></figure>
<h4 id="wordpiece的特殊标记">2.3 WordPiece的特殊标记</h4>
<p><strong>BERT使用的标记</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 子词前缀</span><br><span class="hljs-string">&quot;playing&quot;</span> → [<span class="hljs-string">&quot;play&quot;</span>, <span class="hljs-string">&quot;##ing&quot;</span>]<br><span class="hljs-string">&quot;unhappiness&quot;</span> → [<span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-string">&quot;##happiness&quot;</span>]<br><br><span class="hljs-comment"># 特殊token</span><br>[CLS]: 句子开始<br>[SEP]: 句子分隔<br>[MASK]: 掩码（用于MLM预训练）<br>[PAD]: 填充<br>[UNK]: 未知词<br></code></pre></td></tr></table></figure></p>
<p><strong>示例</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer<br><br>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>)<br><br>text = <span class="hljs-string">&quot;playing football&quot;</span><br>tokens = tokenizer.tokenize(text)<br><span class="hljs-built_in">print</span>(tokens)  <span class="hljs-comment"># [&#x27;play&#x27;, &#x27;##ing&#x27;, &#x27;football&#x27;]</span><br><br><span class="hljs-comment"># 完整编码（包含特殊token）</span><br>encoded = tokenizer.encode(text, add_special_tokens=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(encoded)  <span class="hljs-comment"># [101, 2652, 2075, 2374, 102]</span><br><span class="hljs-comment"># 101: [CLS], 2652: play, 2075: ##ing, 2374: football, 102: [SEP]</span><br></code></pre></td></tr></table></figure></p>
<h3 id="unigram-language-model">3. Unigram Language Model</h3>
<h4 id="算法原理-2">3.1 算法原理</h4>
<p><strong>核心思想</strong>：与BPE/WordPiece相反，从大词汇表开始，逐步删除。</p>
<p><strong>算法流程</strong>： <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">1.</span> 初始化：创建一个大的候选词汇表（所有子串）<br><span class="hljs-bullet">2.</span> 训练：使用EM算法估计每个token的概率<br><span class="hljs-bullet">3.</span> 剪枝：删除对似然度贡献最小的token<br><span class="hljs-bullet">4.</span> 重复：重复步骤2-3，直到达到目标大小<br></code></pre></td></tr></table></figure></p>
<p><strong>数学基础</strong>：</p>
<p>给定词汇表V和文本X，目标是最大化： <figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-constructor">L(X)</span> = Σ log <span class="hljs-constructor">P(<span class="hljs-params">x</span>)</span> <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X<br><br>其中 <span class="hljs-constructor">P(<span class="hljs-params">x</span>)</span> = Σ <span class="hljs-constructor">P(<span class="hljs-params">segmentation</span>)</span> × <span class="hljs-constructor">P(<span class="hljs-params">tokens</span> <span class="hljs-params">in</span> <span class="hljs-params">segmentation</span>)</span><br></code></pre></td></tr></table></figure></p>
<p>对于每个词x，可能有多种分词方式： <figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs prolog"><span class="hljs-string">&quot;unhappiness&quot;</span>可能的分词：<br>- [<span class="hljs-string">&quot;unhappiness&quot;</span>]<br>- [<span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-string">&quot;happiness&quot;</span>]<br>- [<span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-string">&quot;happy&quot;</span>, <span class="hljs-string">&quot;ness&quot;</span>]<br>- [<span class="hljs-string">&quot;u&quot;</span>, <span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-string">&quot;h&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;p&quot;</span>, <span class="hljs-string">&quot;p&quot;</span>, <span class="hljs-string">&quot;i&quot;</span>, <span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-string">&quot;e&quot;</span>, <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-string">&quot;s&quot;</span>]<br></code></pre></td></tr></table></figure></p>
<h4 id="实现示例-1">3.2 实现示例</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">UnigramTokenizer</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size=<span class="hljs-number">1000</span></span>):<br>        self.vocab_size = vocab_size<br>        self.vocab = &#123;&#125;<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_vocab</span>(<span class="hljs-params">self, texts</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;初始化候选词汇表&quot;&quot;&quot;</span><br>        vocab = <span class="hljs-built_in">set</span>()<br>        <br>        <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>            words = text.split()<br>            <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>                <span class="hljs-comment"># 添加所有可能的子串</span><br>                <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(word)):<br>                    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i + <span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(word) + <span class="hljs-number">1</span>):<br>                        vocab.add(word[i:j])<br>        <br>        <span class="hljs-comment"># 初始化概率（均匀分布）</span><br>        prob = <span class="hljs-number">1.0</span> / <span class="hljs-built_in">len</span>(vocab)<br>        <span class="hljs-keyword">return</span> &#123;token: prob <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> vocab&#125;<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_best_segmentation</span>(<span class="hljs-params">self, word, vocab</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;使用动态规划找到最佳分词&quot;&quot;&quot;</span><br>        n = <span class="hljs-built_in">len</span>(word)<br>        <span class="hljs-comment"># dp[i] = (最大log概率, 分词方案)</span><br>        dp = [(-<span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>), [])] * (n + <span class="hljs-number">1</span>)<br>        dp[<span class="hljs-number">0</span>] = (<span class="hljs-number">0.0</span>, [])<br>        <br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n + <span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i):<br>                token = word[j:i]<br>                <span class="hljs-keyword">if</span> token <span class="hljs-keyword">in</span> vocab:<br>                    score = dp[j][<span class="hljs-number">0</span>] + math.log(vocab[token])<br>                    <span class="hljs-keyword">if</span> score &gt; dp[i][<span class="hljs-number">0</span>]:<br>                        dp[i] = (score, dp[j][<span class="hljs-number">1</span>] + [token])<br>        <br>        <span class="hljs-keyword">return</span> dp[n][<span class="hljs-number">1</span>]<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">em_step</span>(<span class="hljs-params">self, texts, vocab</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;EM算法的一步&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># E-step: 计算期望计数</span><br>        token_count = defaultdict(<span class="hljs-built_in">float</span>)<br>        total_count = <span class="hljs-number">0</span><br>        <br>        <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>            words = text.split()<br>            <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>                segmentation = self.get_best_segmentation(word, vocab)<br>                <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> segmentation:<br>                    token_count[token] += <span class="hljs-number">1</span><br>                    total_count += <span class="hljs-number">1</span><br>        <br>        <span class="hljs-comment"># M-step: 更新概率</span><br>        new_vocab = &#123;&#125;<br>        <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> vocab:<br>            new_vocab[token] = token_count[token] / total_count <span class="hljs-keyword">if</span> total_count &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>        <br>        <span class="hljs-keyword">return</span> new_vocab<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_loss</span>(<span class="hljs-params">self, texts, vocab</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;计算当前词汇表的损失&quot;&quot;&quot;</span><br>        total_loss = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>            words = text.split()<br>            <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>                segmentation = self.get_best_segmentation(word, vocab)<br>                word_loss = <span class="hljs-built_in">sum</span>(math.log(vocab[token]) <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> segmentation)<br>                total_loss += word_loss<br>        <span class="hljs-keyword">return</span> total_loss<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, texts, num_iterations=<span class="hljs-number">10</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;训练Unigram tokenizer&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 1. 初始化大词汇表</span><br>        vocab = self.initialize_vocab(texts)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Initial vocab size: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(vocab)&#125;</span>&quot;</span>)<br>        <br>        <span class="hljs-comment"># 2. EM迭代</span><br>        <span class="hljs-keyword">for</span> iteration <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iterations):<br>            vocab = self.em_step(texts, vocab)<br>            loss = self.compute_loss(texts, vocab)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Iteration <span class="hljs-subst">&#123;iteration + <span class="hljs-number">1</span>&#125;</span>, Loss: <span class="hljs-subst">&#123;loss:<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br>        <br>        <span class="hljs-comment"># 3. 剪枝到目标大小</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(vocab) &gt; self.vocab_size:<br>            <span class="hljs-comment"># 计算删除每个token的损失增加</span><br>            loss_increase = &#123;&#125;<br>            current_loss = self.compute_loss(texts, vocab)<br>            <br>            <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span>(vocab.keys()):<br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(token) == <span class="hljs-number">1</span>:  <span class="hljs-comment"># 保留单字符</span><br>                    <span class="hljs-keyword">continue</span><br>                <br>                <span class="hljs-comment"># 临时删除token</span><br>                temp_vocab = vocab.copy()<br>                <span class="hljs-keyword">del</span> temp_vocab[token]<br>                <br>                <span class="hljs-comment"># 重新归一化</span><br>                total_prob = <span class="hljs-built_in">sum</span>(temp_vocab.values())<br>                temp_vocab = &#123;k: v / total_prob <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> temp_vocab.items()&#125;<br>                <br>                <span class="hljs-comment"># 计算损失增加</span><br>                new_loss = self.compute_loss(texts, temp_vocab)<br>                loss_increase[token] = new_loss - current_loss<br>            <br>            <span class="hljs-comment"># 删除损失增加最小的token</span><br>            <span class="hljs-keyword">if</span> loss_increase:<br>                token_to_remove = <span class="hljs-built_in">min</span>(loss_increase, key=loss_increase.get)<br>                <span class="hljs-keyword">del</span> vocab[token_to_remove]<br>                <br>                <span class="hljs-comment"># 重新归一化</span><br>                total_prob = <span class="hljs-built_in">sum</span>(vocab.values())<br>                vocab = &#123;k: v / total_prob <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> vocab.items()&#125;<br>                <br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(vocab) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Vocab size: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(vocab)&#125;</span>, Removed: <span class="hljs-subst">&#123;token_to_remove&#125;</span>&quot;</span>)<br>        <br>        self.vocab = vocab<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">self, text</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;分词&quot;&quot;&quot;</span><br>        words = text.split()<br>        tokens = []<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>            segmentation = self.get_best_segmentation(word, self.vocab)<br>            tokens.extend(segmentation)<br>        <span class="hljs-keyword">return</span> tokens<br></code></pre></td></tr></table></figure>
<h4 id="unigram的优势">3.3 Unigram的优势</h4>
<p><strong>1. 多种分词方式</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Unigram可以为同一个词提供多种分词</span><br><span class="hljs-comment"># 在训练时可以采样不同的分词方式，增加鲁棒性</span><br><br>word = <span class="hljs-string">&quot;unhappiness&quot;</span><br>possible_segmentations = [<br>    ([<span class="hljs-string">&quot;unhappiness&quot;</span>], <span class="hljs-number">0.3</span>),<br>    ([<span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-string">&quot;happiness&quot;</span>], <span class="hljs-number">0.5</span>),<br>    ([<span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-string">&quot;happy&quot;</span>, <span class="hljs-string">&quot;ness&quot;</span>], <span class="hljs-number">0.2</span>)<br>]<br><br><span class="hljs-comment"># 可以根据概率采样</span><br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sample_segmentation</span>(<span class="hljs-params">word, vocab</span>):<br>    <span class="hljs-comment"># 获取所有可能的分词及其概率</span><br>    <span class="hljs-comment"># 根据概率采样</span><br>    <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure></p>
<p><strong>2. 对噪声更鲁棒</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 即使有拼写错误，也能找到合理的分词</span><br><span class="hljs-string">&quot;hapiness&quot;</span> → [<span class="hljs-string">&quot;hap&quot;</span>, <span class="hljs-string">&quot;i&quot;</span>, <span class="hljs-string">&quot;ness&quot;</span>]  <span class="hljs-comment"># 缺少一个p</span><br><span class="hljs-string">&quot;happinesss&quot;</span> → [<span class="hljs-string">&quot;happiness&quot;</span>, <span class="hljs-string">&quot;s&quot;</span>]  <span class="hljs-comment"># 多了一个s</span><br></code></pre></td></tr></table></figure></p>
<p><strong>3. 更灵活的词汇表</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 可以轻松调整词汇表大小</span><br><span class="hljs-comment"># 不需要重新训练，只需要调整剪枝阈值</span><br></code></pre></td></tr></table></figure></p>
<h3 id="三种算法对比总结">4. 三种算法对比总结</h3>
<table>

<thead>
<tr class="header">
<th style="text-align: left;">特性</th>
<th style="text-align: left;">BPE</th>
<th style="text-align: left;">WordPiece</th>
<th style="text-align: left;">Unigram</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>训练方向</strong></td>
<td style="text-align: left;">自底向上（合并）</td>
<td style="text-align: left;">自底向上（合并）</td>
<td style="text-align: left;">自顶向下（删除）</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>合并/删除准则</strong></td>
<td style="text-align: left;">频率最高</td>
<td style="text-align: left;">似然度增益最大</td>
<td style="text-align: left;">似然度损失最小</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>分词确定性</strong></td>
<td style="text-align: left;">确定性</td>
<td style="text-align: left;">确定性</td>
<td style="text-align: left;">可以概率采样</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>训练复杂度</strong></td>
<td style="text-align: left;">O(n²)</td>
<td style="text-align: left;">O(n²)</td>
<td style="text-align: left;">O(n³)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>实现难度</strong></td>
<td style="text-align: left;">简单</td>
<td style="text-align: left;">中等</td>
<td style="text-align: left;">复杂</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>多语言支持</strong></td>
<td style="text-align: left;">好</td>
<td style="text-align: left;">好</td>
<td style="text-align: left;">最好</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>对噪声鲁棒性</strong></td>
<td style="text-align: left;">中等</td>
<td style="text-align: left;">中等</td>
<td style="text-align: left;">好</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>代表模型</strong></td>
<td style="text-align: left;">GPT系列, Llama</td>
<td style="text-align: left;">BERT, DistilBERT</td>
<td style="text-align: left;">T5, mBART, XLNet</td>
</tr>
</tbody>
</table>
<p><strong>性能对比实验</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 在相同语料上训练三种tokenizer</span><br>corpus = load_corpus(<span class="hljs-string">&quot;wikitext-103&quot;</span>)<br><br><span class="hljs-comment"># BPE</span><br>bpe = BPETokenizer(vocab_size=<span class="hljs-number">32000</span>)<br>bpe.train(corpus)<br>bpe_compression = evaluate_compression(bpe, test_corpus)<br>bpe_coverage = evaluate_coverage(bpe, test_corpus)<br><br><span class="hljs-comment"># WordPiece</span><br>wp = WordPieceTokenizer(vocab_size=<span class="hljs-number">32000</span>)<br>wp.train(corpus)<br>wp_compression = evaluate_compression(wp, test_corpus)<br>wp_coverage = evaluate_coverage(wp, test_corpus)<br><br><span class="hljs-comment"># Unigram</span><br>uni = UnigramTokenizer(vocab_size=<span class="hljs-number">32000</span>)<br>uni.train(corpus)<br>uni_compression = evaluate_compression(uni, test_corpus)<br>uni_coverage = evaluate_coverage(uni, test_corpus)<br><br><span class="hljs-comment"># 结果（示例）</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">算法        压缩率    覆盖率    训练时间</span><br><span class="hljs-string">BPE         4.2      99.5%     10min</span><br><span class="hljs-string">WordPiece   4.3      99.6%     15min</span><br><span class="hljs-string">Unigram     4.4      99.7%     25min</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure></p>
<hr />
<h2 id="第四部分sentencepiece---生产级实现">第四部分：SentencePiece -
生产级实现</h2>
<h3 id="为什么需要sentencepiece">1. 为什么需要SentencePiece？</h3>
<p><strong>传统tokenizer的问题</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 传统流程</span><br>text = <span class="hljs-string">&quot;Hello world&quot;</span><br><span class="hljs-comment"># 1. 预分词（依赖语言）</span><br>words = text.split()  <span class="hljs-comment"># [&quot;Hello&quot;, &quot;world&quot;]</span><br><span class="hljs-comment"># 2. 子词分词</span><br>tokens = subword_tokenize(words)  <span class="hljs-comment"># [&quot;Hello&quot;, &quot;world&quot;]</span><br><br><span class="hljs-comment"># 问题：</span><br><span class="hljs-comment"># - 依赖空格分词（中文、日文怎么办？）</span><br><span class="hljs-comment"># - 预处理步骤不可逆（丢失了原始格式）</span><br><span class="hljs-comment"># - 难以处理多语言</span><br></code></pre></td></tr></table></figure></p>
<p><strong>SentencePiece的解决方案</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># SentencePiece流程</span><br>text = <span class="hljs-string">&quot;Hello world&quot;</span><br><span class="hljs-comment"># 直接处理原始文本，将空格视为特殊字符</span><br>tokens = sp.encode(text)  <span class="hljs-comment"># [&quot;▁Hello&quot;, &quot;▁world&quot;]</span><br><br><span class="hljs-comment"># 优势：</span><br><span class="hljs-comment"># - 语言无关（不需要预分词）</span><br><span class="hljs-comment"># - 完全可逆（可以完美还原原文）</span><br><span class="hljs-comment"># - 统一处理所有语言</span><br></code></pre></td></tr></table></figure></p>
<h3 id="sentencepiece的核心特性">2. SentencePiece的核心特性</h3>
<h4 id="语言无关性">2.1 语言无关性</h4>
<p><strong>空格处理</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> sentencepiece <span class="hljs-keyword">as</span> spm<br><br><span class="hljs-comment"># 训练</span><br>spm.SentencePieceTrainer.train(<br>    <span class="hljs-built_in">input</span>=<span class="hljs-string">&#x27;corpus.txt&#x27;</span>,<br>    model_prefix=<span class="hljs-string">&#x27;sp_model&#x27;</span>,<br>    vocab_size=<span class="hljs-number">32000</span>,<br>    character_coverage=<span class="hljs-number">0.9995</span>,  <span class="hljs-comment"># 字符覆盖率</span><br>    model_type=<span class="hljs-string">&#x27;bpe&#x27;</span>  <span class="hljs-comment"># 或 &#x27;unigram&#x27;</span><br>)<br><br><span class="hljs-comment"># 使用</span><br>sp = spm.SentencePieceProcessor()<br>sp.load(<span class="hljs-string">&#x27;sp_model.model&#x27;</span>)<br><br><span class="hljs-comment"># 英文（有空格）</span><br>text_en = <span class="hljs-string">&quot;Hello world&quot;</span><br>tokens_en = sp.encode_as_pieces(text_en)<br><span class="hljs-built_in">print</span>(tokens_en)  <span class="hljs-comment"># [&#x27;▁Hello&#x27;, &#x27;▁world&#x27;]</span><br><br><span class="hljs-comment"># 中文（无空格）</span><br>text_zh = <span class="hljs-string">&quot;你好世界&quot;</span><br>tokens_zh = sp.encode_as_pieces(text_zh)<br><span class="hljs-built_in">print</span>(tokens_zh)  <span class="hljs-comment"># [&#x27;▁你好&#x27;, &#x27;世界&#x27;]</span><br><br><span class="hljs-comment"># 混合</span><br>text_mix = <span class="hljs-string">&quot;Hello 世界&quot;</span><br>tokens_mix = sp.encode_as_pieces(text_mix)<br><span class="hljs-built_in">print</span>(tokens_mix)  <span class="hljs-comment"># [&#x27;▁Hello&#x27;, &#x27;▁世界&#x27;]</span><br></code></pre></td></tr></table></figure></p>
<p><strong>▁符号的含义</strong>： <figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs prolog">▁ (<span class="hljs-symbol">U</span>+<span class="hljs-number">2581</span>) 表示原始文本中的空格<br>这样可以区分：<br><span class="hljs-string">&quot;hello world&quot;</span> → [<span class="hljs-string">&quot;▁hello&quot;</span>, <span class="hljs-string">&quot;▁world&quot;</span>]<br><span class="hljs-string">&quot;helloworld&quot;</span>  → [<span class="hljs-string">&quot;▁helloworld&quot;</span>]<br></code></pre></td></tr></table></figure></p>
<h4 id="完全可逆性">2.2 完全可逆性</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 编码</span><br>text = <span class="hljs-string">&quot;Hello, world! How are you?&quot;</span><br>ids = sp.encode_as_ids(text)<br>pieces = sp.encode_as_pieces(text)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Original: <span class="hljs-subst">&#123;text&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;IDs: <span class="hljs-subst">&#123;ids&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Pieces: <span class="hljs-subst">&#123;pieces&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 解码</span><br>decoded_from_ids = sp.decode_ids(ids)<br>decoded_from_pieces = sp.decode_pieces(pieces)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Decoded from IDs: <span class="hljs-subst">&#123;decoded_from_ids&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Decoded from pieces: <span class="hljs-subst">&#123;decoded_from_pieces&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 验证可逆性</span><br><span class="hljs-keyword">assert</span> text == decoded_from_ids<br><span class="hljs-keyword">assert</span> text == decoded_from_pieces<br></code></pre></td></tr></table></figure>
<h4 id="子词正则化subword-regularization">2.3 子词正则化（Subword
Regularization）</h4>
<p><strong>核心思想</strong>：在训练时，对同一个词使用不同的分词方式，增加模型鲁棒性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 启用子词正则化</span><br>sp = spm.SentencePieceProcessor()<br>sp.load(<span class="hljs-string">&#x27;sp_model.model&#x27;</span>)<br><br>text = <span class="hljs-string">&quot;unhappiness&quot;</span><br><br><span class="hljs-comment"># 确定性分词</span><br>tokens_deterministic = sp.encode_as_pieces(text)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Deterministic: <span class="hljs-subst">&#123;tokens_deterministic&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 随机采样不同的分词（用于训练）</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    tokens_sampled = sp.sample_encode_as_pieces(text, nbest_size=-<span class="hljs-number">1</span>, alpha=<span class="hljs-number">0.1</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Sample <span class="hljs-subst">&#123;i+<span class="hljs-number">1</span>&#125;</span>: <span class="hljs-subst">&#123;tokens_sampled&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 输出示例：</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Deterministic: [&#x27;▁un&#x27;, &#x27;happiness&#x27;]</span><br><span class="hljs-string">Sample 1: [&#x27;▁un&#x27;, &#x27;happiness&#x27;]</span><br><span class="hljs-string">Sample 2: [&#x27;▁un&#x27;, &#x27;happy&#x27;, &#x27;ness&#x27;]</span><br><span class="hljs-string">Sample 3: [&#x27;▁unhappiness&#x27;]</span><br><span class="hljs-string">Sample 4: [&#x27;▁un&#x27;, &#x27;hap&#x27;, &#x27;pi&#x27;, &#x27;ness&#x27;]</span><br><span class="hljs-string">Sample 5: [&#x27;▁un&#x27;, &#x27;happiness&#x27;]</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p><strong>参数说明</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># nbest_size: 考虑的top-k分词方案</span><br><span class="hljs-comment"># nbest_size=-1: 考虑所有可能的分词</span><br><span class="hljs-comment"># nbest_size=10: 只考虑概率最高的10种</span><br><br><span class="hljs-comment"># alpha: 平滑参数</span><br><span class="hljs-comment"># alpha=0.0: 总是选择最优分词（确定性）</span><br><span class="hljs-comment"># alpha=1.0: 完全随机</span><br><span class="hljs-comment"># alpha=0.1: 轻微随机化（推荐）</span><br></code></pre></td></tr></table></figure></p>
<h3 id="sentencepiece实战">3. SentencePiece实战</h3>
<h4 id="训练自定义tokenizer">3.1 训练自定义tokenizer</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> sentencepiece <span class="hljs-keyword">as</span> spm<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 准备训练数据</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_corpus</span>(<span class="hljs-params">input_files, output_file</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;合并多个文件为训练语料&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(output_file, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> outf:<br>        <span class="hljs-keyword">for</span> input_file <span class="hljs-keyword">in</span> input_files:<br>            <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(input_file, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> inf:<br>                outf.write(inf.read())<br><br><span class="hljs-comment"># 训练参数</span><br>train_params = &#123;<br>    <span class="hljs-string">&#x27;input&#x27;</span>: <span class="hljs-string">&#x27;corpus.txt&#x27;</span>,<br>    <span class="hljs-string">&#x27;model_prefix&#x27;</span>: <span class="hljs-string">&#x27;my_tokenizer&#x27;</span>,<br>    <span class="hljs-string">&#x27;vocab_size&#x27;</span>: <span class="hljs-number">32000</span>,<br>    <span class="hljs-string">&#x27;character_coverage&#x27;</span>: <span class="hljs-number">0.9995</span>,<br>    <span class="hljs-string">&#x27;model_type&#x27;</span>: <span class="hljs-string">&#x27;bpe&#x27;</span>,  <span class="hljs-comment"># 或 &#x27;unigram&#x27;, &#x27;char&#x27;, &#x27;word&#x27;</span><br>    <br>    <span class="hljs-comment"># 特殊token</span><br>    <span class="hljs-string">&#x27;pad_id&#x27;</span>: <span class="hljs-number">0</span>,<br>    <span class="hljs-string">&#x27;unk_id&#x27;</span>: <span class="hljs-number">1</span>,<br>    <span class="hljs-string">&#x27;bos_id&#x27;</span>: <span class="hljs-number">2</span>,<br>    <span class="hljs-string">&#x27;eos_id&#x27;</span>: <span class="hljs-number">3</span>,<br>    <span class="hljs-string">&#x27;pad_piece&#x27;</span>: <span class="hljs-string">&#x27;[PAD]&#x27;</span>,<br>    <span class="hljs-string">&#x27;unk_piece&#x27;</span>: <span class="hljs-string">&#x27;[UNK]&#x27;</span>,<br>    <span class="hljs-string">&#x27;bos_piece&#x27;</span>: <span class="hljs-string">&#x27;[BOS]&#x27;</span>,<br>    <span class="hljs-string">&#x27;eos_piece&#x27;</span>: <span class="hljs-string">&#x27;[EOS]&#x27;</span>,<br>    <br>    <span class="hljs-comment"># 额外的特殊token</span><br>    <span class="hljs-string">&#x27;user_defined_symbols&#x27;</span>: [<span class="hljs-string">&#x27;[MASK]&#x27;</span>, <span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>],<br>    <br>    <span class="hljs-comment"># 训练参数</span><br>    <span class="hljs-string">&#x27;num_threads&#x27;</span>: <span class="hljs-number">16</span>,<br>    <span class="hljs-string">&#x27;max_sentence_length&#x27;</span>: <span class="hljs-number">16384</span>,<br>    <span class="hljs-string">&#x27;shuffle_input_sentence&#x27;</span>: <span class="hljs-literal">True</span>,<br>    <span class="hljs-string">&#x27;train_extremely_large_corpus&#x27;</span>: <span class="hljs-literal">False</span>,<br>    <br>    <span class="hljs-comment"># 字符覆盖率（重要！）</span><br>    <span class="hljs-comment"># 0.9995: 适合大多数语言</span><br>    <span class="hljs-comment"># 1.0: 包含所有字符（可能包含噪声）</span><br>    <span class="hljs-string">&#x27;character_coverage&#x27;</span>: <span class="hljs-number">0.9995</span>,<br>    <br>    <span class="hljs-comment"># 分词参数</span><br>    <span class="hljs-string">&#x27;split_by_whitespace&#x27;</span>: <span class="hljs-literal">True</span>,<br>    <span class="hljs-string">&#x27;split_by_number&#x27;</span>: <span class="hljs-literal">True</span>,<br>    <span class="hljs-string">&#x27;split_by_unicode_script&#x27;</span>: <span class="hljs-literal">True</span>,<br>    <br>    <span class="hljs-comment"># 控制词汇表</span><br>    <span class="hljs-string">&#x27;max_sentencepiece_length&#x27;</span>: <span class="hljs-number">16</span>,<br>    <span class="hljs-string">&#x27;byte_fallback&#x27;</span>: <span class="hljs-literal">True</span>,  <span class="hljs-comment"># 使用字节回退处理未知字符</span><br>&#125;<br><br><span class="hljs-comment"># 训练</span><br>spm.SentencePieceTrainer.train(**train_params)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training completed!&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Model saved: my_tokenizer.model&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Vocab saved: my_tokenizer.vocab&quot;</span>)<br></code></pre></td></tr></table></figure>
<h4 id="加载和使用">3.2 加载和使用</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载模型</span><br>sp = spm.SentencePieceProcessor()<br>sp.load(<span class="hljs-string">&#x27;my_tokenizer.model&#x27;</span>)<br><br><span class="hljs-comment"># 基本信息</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Vocab size: <span class="hljs-subst">&#123;sp.vocab_size()&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;BOS ID: <span class="hljs-subst">&#123;sp.bos_id()&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;EOS ID: <span class="hljs-subst">&#123;sp.eos_id()&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;PAD ID: <span class="hljs-subst">&#123;sp.pad_id()&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;UNK ID: <span class="hljs-subst">&#123;sp.unk_id()&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 编码</span><br>text = <span class="hljs-string">&quot;Hello, world! This is a test.&quot;</span><br><br><span class="hljs-comment"># 方法1: 编码为pieces</span><br>pieces = sp.encode_as_pieces(text)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Pieces: <span class="hljs-subst">&#123;pieces&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 方法2: 编码为IDs</span><br>ids = sp.encode_as_ids(text)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;IDs: <span class="hljs-subst">&#123;ids&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 方法3: 同时获取pieces和IDs</span><br>pieces_and_ids = [(sp.id_to_piece(<span class="hljs-built_in">id</span>), <span class="hljs-built_in">id</span>) <span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> ids]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Pieces and IDs: <span class="hljs-subst">&#123;pieces_and_ids&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 解码</span><br>decoded = sp.decode_ids(ids)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Decoded: <span class="hljs-subst">&#123;decoded&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 验证</span><br><span class="hljs-keyword">assert</span> text == decoded<br></code></pre></td></tr></table></figure>
<h4 id="批处理">3.3 批处理</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 批量编码</span><br>texts = [<br>    <span class="hljs-string">&quot;Hello, world!&quot;</span>,<br>    <span class="hljs-string">&quot;How are you?&quot;</span>,<br>    <span class="hljs-string">&quot;I&#x27;m fine, thank you.&quot;</span><br>]<br><br><span class="hljs-comment"># 编码为IDs</span><br>ids_batch = [sp.encode_as_ids(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts]<br><br><span class="hljs-comment"># 填充到相同长度</span><br>max_len = <span class="hljs-built_in">max</span>(<span class="hljs-built_in">len</span>(ids) <span class="hljs-keyword">for</span> ids <span class="hljs-keyword">in</span> ids_batch)<br>padded_ids = [<br>    ids + [sp.pad_id()] * (max_len - <span class="hljs-built_in">len</span>(ids))<br>    <span class="hljs-keyword">for</span> ids <span class="hljs-keyword">in</span> ids_batch<br>]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Padded batch shape: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(padded_ids)&#125;</span> x <span class="hljs-subst">&#123;max_len&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 解码</span><br>decoded_texts = [sp.decode_ids(ids) <span class="hljs-keyword">for</span> ids <span class="hljs-keyword">in</span> ids_batch]<br><span class="hljs-keyword">for</span> original, decoded <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(texts, decoded_texts):<br>    <span class="hljs-keyword">assert</span> original == decoded<br></code></pre></td></tr></table></figure>
<h4 id="与hugging-face集成">3.4 与Hugging Face集成</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast<br><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> SentencePieceBPETokenizer<br><br><span class="hljs-comment"># 方法1: 直接使用SentencePiece</span><br><span class="hljs-keyword">import</span> sentencepiece <span class="hljs-keyword">as</span> spm<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SPTokenizer</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_path</span>):<br>        self.sp = spm.SentencePieceProcessor()<br>        self.sp.load(model_path)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, text, **kwargs</span>):<br>        <span class="hljs-keyword">return</span> &#123;<br>            <span class="hljs-string">&#x27;input_ids&#x27;</span>: self.sp.encode_as_ids(text),<br>            <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>] * <span class="hljs-built_in">len</span>(self.sp.encode_as_ids(text))<br>        &#125;<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params">self, ids, **kwargs</span>):<br>        <span class="hljs-keyword">return</span> self.sp.decode_ids(ids)<br><br><span class="hljs-comment"># 方法2: 转换为Hugging Face格式</span><br><span class="hljs-comment"># 需要先导出为JSON格式</span><br>tokenizer = PreTrainedTokenizerFast(<br>    tokenizer_file=<span class="hljs-string">&quot;my_tokenizer.json&quot;</span>,<br>    bos_token=<span class="hljs-string">&quot;[BOS]&quot;</span>,<br>    eos_token=<span class="hljs-string">&quot;[EOS]&quot;</span>,<br>    unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>,<br>    pad_token=<span class="hljs-string">&quot;[PAD]&quot;</span>,<br>)<br></code></pre></td></tr></table></figure>
<hr />
<h2
id="第五部分tokenization的关键设计决策">第五部分：Tokenization的关键设计决策</h2>
<h3 id="词汇表大小vocabulary-size">1. 词汇表大小（Vocabulary Size）</h3>
<h4 id="权衡分析">1.1 权衡分析</h4>
<p><strong>小词汇表（如8k-16k）</strong>： <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">优点：<br><span class="hljs-bullet">- </span>嵌入矩阵小（vocab_size × d_model）<br><span class="hljs-bullet">- </span>训练更快<br><span class="hljs-bullet">- </span>显存占用少<br><br>缺点：<br><span class="hljs-bullet">- </span>序列更长（每个词被分成更多token）<br><span class="hljs-bullet">- </span>注意力计算成本高（O(n²)）<br><span class="hljs-bullet">- </span>信息密度低<br></code></pre></td></tr></table></figure></p>
<p><strong>大词汇表（如100k-256k）</strong>： <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">优点：<br><span class="hljs-bullet">- </span>序列更短<br><span class="hljs-bullet">- </span>信息密度高<br><span class="hljs-bullet">- </span>更好的多语言支持<br><br>缺点：<br><span class="hljs-bullet">- </span>嵌入矩阵巨大<br><span class="hljs-bullet">- </span>训练慢<br><span class="hljs-bullet">- </span>容易过拟合<br></code></pre></td></tr></table></figure></p>
<p><strong>实验对比</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 在相同文本上测试不同词汇表大小</span><br>text = <span class="hljs-string">&quot;The quick brown fox jumps over the lazy dog&quot;</span><br><br><span class="hljs-comment"># 词汇表大小: 1000</span><br>tokenizer_1k = train_tokenizer(corpus, vocab_size=<span class="hljs-number">1000</span>)<br>tokens_1k = tokenizer_1k.encode(text)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Vocab 1k: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(tokens_1k)&#125;</span> tokens&quot;</span>)  <span class="hljs-comment"># 例如: 15 tokens</span><br><br><span class="hljs-comment"># 词汇表大小: 10000</span><br>tokenizer_10k = train_tokenizer(corpus, vocab_size=<span class="hljs-number">10000</span>)<br>tokens_10k = tokenizer_10k.encode(text)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Vocab 10k: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(tokens_10k)&#125;</span> tokens&quot;</span>)  <span class="hljs-comment"># 例如: 10 tokens</span><br><br><span class="hljs-comment"># 词汇表大小: 50000</span><br>tokenizer_50k = train_tokenizer(corpus, vocab_size=<span class="hljs-number">50000</span>)<br>tokens_50k = tokenizer_50k.encode(text)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Vocab 50k: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(tokens_50k)&#125;</span> tokens&quot;</span>)  <span class="hljs-comment"># 例如: 9 tokens</span><br><br><span class="hljs-comment"># 计算成本对比</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_cost</span>(<span class="hljs-params">vocab_size, seq_len, d_model=<span class="hljs-number">768</span>, num_layers=<span class="hljs-number">12</span></span>):<br>    <span class="hljs-comment"># 嵌入层参数</span><br>    embedding_params = vocab_size * d_model<br>    <br>    <span class="hljs-comment"># 注意力计算（简化）</span><br>    attention_flops = num_layers * seq_len * seq_len * d_model<br>    <br>    <span class="hljs-keyword">return</span> &#123;<br>        <span class="hljs-string">&#x27;embedding_params&#x27;</span>: embedding_params,<br>        <span class="hljs-string">&#x27;attention_flops&#x27;</span>: attention_flops,<br>        <span class="hljs-string">&#x27;total_cost&#x27;</span>: embedding_params + attention_flops<br>    &#125;<br><br><span class="hljs-keyword">for</span> vocab_size, seq_len <span class="hljs-keyword">in</span> [(<span class="hljs-number">1000</span>, <span class="hljs-number">15</span>), (<span class="hljs-number">10000</span>, <span class="hljs-number">10</span>), (<span class="hljs-number">50000</span>, <span class="hljs-number">9</span>)]:<br>    cost = compute_cost(vocab_size, seq_len)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Vocab <span class="hljs-subst">&#123;vocab_size&#125;</span>, Seq <span class="hljs-subst">&#123;seq_len&#125;</span>: <span class="hljs-subst">&#123;cost&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure></p>
<h4 id="主流模型的选择">1.2 主流模型的选择</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">模型</th>
<th style="text-align: left;">词汇表大小</th>
<th style="text-align: left;">原因</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>GPT-2</strong></td>
<td style="text-align: left;">50,257</td>
<td style="text-align: left;">字节级BPE，覆盖所有Unicode</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>GPT-3</strong></td>
<td style="text-align: left;">50,257</td>
<td style="text-align: left;">与GPT-2相同</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>BERT</strong></td>
<td style="text-align: left;">30,522</td>
<td style="text-align: left;">WordPiece，英文为主</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>RoBERTa</strong></td>
<td style="text-align: left;">50,265</td>
<td style="text-align: left;">字节级BPE</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>T5</strong></td>
<td style="text-align: left;">32,000</td>
<td style="text-align: left;">SentencePiece Unigram</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Llama</strong></td>
<td style="text-align: left;">32,000</td>
<td style="text-align: left;">SentencePiece BPE</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Llama 2</strong></td>
<td style="text-align: left;">32,000</td>
<td style="text-align: left;">与Llama相同</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>PaLM</strong></td>
<td style="text-align: left;">256,000</td>
<td style="text-align: left;">大词汇表，多语言</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>GPT-4</strong></td>
<td style="text-align: left;">~100,000</td>
<td style="text-align: left;">推测，支持更多语言</td>
</tr>
</tbody>
</table>
<p><strong>趋势观察</strong>： <figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs subunit">早期模型（2018<span class="hljs-string">-2019</span>）: 30k<span class="hljs-string">-50</span>k<br>现代模型（2020<span class="hljs-string">-2022</span>）: 32k<span class="hljs-string">-50</span>k（标准化）<br>多语言模型（2022+）: 100k<span class="hljs-string">-256</span>k<br></code></pre></td></tr></table></figure></p>
<h4 id="如何选择词汇表大小">1.3 如何选择词汇表大小？</h4>
<p><strong>决策树</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_vocab_size</span>(<span class="hljs-params"></span><br><span class="hljs-params">    languages,           <span class="hljs-comment"># 支持的语言</span></span><br><span class="hljs-params">    domain,             <span class="hljs-comment"># 领域（通用/专业）</span></span><br><span class="hljs-params">    model_size,         <span class="hljs-comment"># 模型参数量</span></span><br><span class="hljs-params">    compute_budget      <span class="hljs-comment"># 计算预算</span></span><br><span class="hljs-params"></span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(languages) == <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> languages[<span class="hljs-number">0</span>] == <span class="hljs-string">&#x27;english&#x27;</span>:<br>        <span class="hljs-comment"># 单语言英文</span><br>        <span class="hljs-keyword">if</span> model_size &lt; <span class="hljs-number">1e9</span>:  <span class="hljs-comment"># &lt;1B参数</span><br>            <span class="hljs-keyword">return</span> <span class="hljs-number">16000</span><br>        <span class="hljs-keyword">elif</span> model_size &lt; <span class="hljs-number">10e9</span>:  <span class="hljs-comment"># 1B-10B</span><br>            <span class="hljs-keyword">return</span> <span class="hljs-number">32000</span><br>        <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># &gt;10B</span><br>            <span class="hljs-keyword">return</span> <span class="hljs-number">50000</span><br>    <br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">len</span>(languages) &lt;= <span class="hljs-number">5</span>:<br>        <span class="hljs-comment"># 少数语言</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-number">32000</span><br>    <br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># 多语言</span><br>        <span class="hljs-keyword">if</span> compute_budget == <span class="hljs-string">&#x27;low&#x27;</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">64000</span><br>        <span class="hljs-keyword">elif</span> compute_budget == <span class="hljs-string">&#x27;medium&#x27;</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">128000</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">256000</span><br>    <br>    <span class="hljs-comment"># 领域调整</span><br>    <span class="hljs-keyword">if</span> domain == <span class="hljs-string">&#x27;code&#x27;</span>:<br>        <span class="hljs-comment"># 代码需要更大的词汇表</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">int</span>(base_size * <span class="hljs-number">1.5</span>)<br>    <span class="hljs-keyword">elif</span> domain == <span class="hljs-string">&#x27;medical&#x27;</span>:<br>        <span class="hljs-comment"># 医学术语多</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">int</span>(base_size * <span class="hljs-number">1.3</span>)<br>    <br>    <span class="hljs-keyword">return</span> base_size<br></code></pre></td></tr></table></figure></p>
<h3 id="特殊token的设计">2. 特殊Token的设计</h3>
<h4 id="必需的特殊token">2.1 必需的特殊Token</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SpecialTokens</span>:<br>    <span class="hljs-comment"># 核心特殊token</span><br>    PAD = <span class="hljs-string">&#x27;[PAD]&#x27;</span>    <span class="hljs-comment"># ID: 0, 填充token</span><br>    UNK = <span class="hljs-string">&#x27;[UNK]&#x27;</span>    <span class="hljs-comment"># ID: 1, 未知token</span><br>    BOS = <span class="hljs-string">&#x27;[BOS]&#x27;</span>    <span class="hljs-comment"># ID: 2, 序列开始</span><br>    EOS = <span class="hljs-string">&#x27;[EOS]&#x27;</span>    <span class="hljs-comment"># ID: 3, 序列结束</span><br>    <br>    <span class="hljs-comment"># 可选特殊token</span><br>    CLS = <span class="hljs-string">&#x27;[CLS]&#x27;</span>    <span class="hljs-comment"># 分类token（BERT）</span><br>    SEP = <span class="hljs-string">&#x27;[SEP]&#x27;</span>    <span class="hljs-comment"># 分隔token（BERT）</span><br>    MASK = <span class="hljs-string">&#x27;[MASK]&#x27;</span>  <span class="hljs-comment"># 掩码token（MLM）</span><br></code></pre></td></tr></table></figure>
<p><strong>使用场景</strong>：</p>
<p><strong>1. PAD - 填充</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 批处理时对齐序列长度</span><br>texts = [<span class="hljs-string">&quot;Hello&quot;</span>, <span class="hljs-string">&quot;Hello world&quot;</span>, <span class="hljs-string">&quot;Hi&quot;</span>]<br>tokenized = [tokenizer.encode(t) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> texts]<br><br><span class="hljs-comment"># 不同长度</span><br><span class="hljs-built_in">print</span>(tokenized)<br><span class="hljs-comment"># [[15496], [15496, 995], [17250]]</span><br><br><span class="hljs-comment"># 填充到相同长度</span><br>max_len = <span class="hljs-built_in">max</span>(<span class="hljs-built_in">len</span>(t) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> tokenized)<br>padded = [<br>    t + [tokenizer.pad_id] * (max_len - <span class="hljs-built_in">len</span>(t))<br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> tokenized<br>]<br><br><span class="hljs-built_in">print</span>(padded)<br><span class="hljs-comment"># [[15496, 0, 0], [15496, 995, 0], [17250, 0, 0]]</span><br></code></pre></td></tr></table></figure></p>
<p><strong>2. BOS/EOS - 序列边界</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 生成任务</span><br>prompt = <span class="hljs-string">&quot;Once upon a time&quot;</span><br>input_ids = [tokenizer.bos_id] + tokenizer.encode(prompt)<br><br><span class="hljs-comment"># 生成</span><br>generated_ids = model.generate(input_ids)<br><br><span class="hljs-comment"># 检测结束</span><br><span class="hljs-keyword">if</span> generated_ids[-<span class="hljs-number">1</span>] == tokenizer.eos_id:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Generation completed&quot;</span>)<br></code></pre></td></tr></table></figure></p>
<p><strong>3. MASK - 掩码语言模型</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># BERT预训练</span><br>text = <span class="hljs-string">&quot;The cat sat on the mat&quot;</span><br>tokens = tokenizer.encode(text)<br><br><span class="hljs-comment"># 随机掩码15%的token</span><br><span class="hljs-keyword">import</span> random<br>masked_tokens = tokens.copy()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(masked_tokens)):<br>    <span class="hljs-keyword">if</span> random.random() &lt; <span class="hljs-number">0.15</span>:<br>        masked_tokens[i] = tokenizer.mask_id<br><br><span class="hljs-comment"># 训练目标：预测被掩码的token</span><br></code></pre></td></tr></table></figure></p>
<h4 id="自定义特殊token">2.2 自定义特殊Token</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 为特定任务添加特殊token</span><br>custom_tokens = [<br>    <span class="hljs-string">&#x27;[INST]&#x27;</span>,      <span class="hljs-comment"># 指令开始</span><br>    <span class="hljs-string">&#x27;[/INST]&#x27;</span>,     <span class="hljs-comment"># 指令结束</span><br>    <span class="hljs-string">&#x27;[SYS]&#x27;</span>,       <span class="hljs-comment"># 系统消息</span><br>    <span class="hljs-string">&#x27;[/SYS]&#x27;</span>,      <span class="hljs-comment"># 系统消息结束</span><br>    <span class="hljs-string">&#x27;[USER]&#x27;</span>,      <span class="hljs-comment"># 用户消息</span><br>    <span class="hljs-string">&#x27;[ASSISTANT]&#x27;</span>, <span class="hljs-comment"># 助手消息</span><br>]<br><br><span class="hljs-comment"># 添加到tokenizer</span><br>tokenizer.add_special_tokens(&#123;<span class="hljs-string">&#x27;additional_special_tokens&#x27;</span>: custom_tokens&#125;)<br><br><span class="hljs-comment"># 使用</span><br>prompt = <span class="hljs-string">&quot;[INST] What is the capital of France? [/INST]&quot;</span><br>tokens = tokenizer.encode(prompt)<br></code></pre></td></tr></table></figure>
<p><strong>Llama 2的Chat格式</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Llama 2使用特殊格式</span><br>template = <span class="hljs-string">&quot;&quot;&quot;&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;</span><br><span class="hljs-string">&#123;system_prompt&#125;</span><br><span class="hljs-string">&lt;&lt;/SYS&gt;&gt;</span><br><span class="hljs-string"></span><br><span class="hljs-string">&#123;user_message&#125; [/INST]&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># 示例</span><br>conversation = template.<span class="hljs-built_in">format</span>(<br>    system_prompt=<span class="hljs-string">&quot;You are a helpful assistant.&quot;</span>,<br>    user_message=<span class="hljs-string">&quot;What is the capital of France?&quot;</span><br>)<br><br><span class="hljs-comment"># Tokenize</span><br>tokens = tokenizer.encode(conversation)<br></code></pre></td></tr></table></figure></p>
<h3 id="预处理策略">3. 预处理策略</h3>
<h4 id="unicode规范化">3.1 Unicode规范化</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> unicodedata<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">normalize_text</span>(<span class="hljs-params">text, form=<span class="hljs-string">&#x27;NFKC&#x27;</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Unicode规范化</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Forms:</span><br><span class="hljs-string">    - NFC: Canonical Decomposition, followed by Canonical Composition</span><br><span class="hljs-string">    - NFD: Canonical Decomposition</span><br><span class="hljs-string">    - NFKC: Compatibility Decomposition, followed by Canonical Composition</span><br><span class="hljs-string">    - NFKD: Compatibility Decomposition</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> unicodedata.normalize(form, text)<br><br><span class="hljs-comment"># 示例</span><br>text1 = <span class="hljs-string">&quot;café&quot;</span>  <span class="hljs-comment"># 使用组合字符 é</span><br>text2 = <span class="hljs-string">&quot;café&quot;</span>  <span class="hljs-comment"># 使用 e + 重音符</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Text1 length: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(text1)&#125;</span>&quot;</span>)  <span class="hljs-comment"># 4</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Text2 length: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(text2)&#125;</span>&quot;</span>)  <span class="hljs-comment"># 5</span><br><br><span class="hljs-comment"># 规范化后相同</span><br>norm1 = normalize_text(text1, <span class="hljs-string">&#x27;NFC&#x27;</span>)<br>norm2 = normalize_text(text2, <span class="hljs-string">&#x27;NFC&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Normalized equal: <span class="hljs-subst">&#123;norm1 == norm2&#125;</span>&quot;</span>)  <span class="hljs-comment"># True</span><br></code></pre></td></tr></table></figure>
<h4 id="大小写处理">3.2 大小写处理</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 策略1: 保留原始大小写（GPT系列）</span><br>text = <span class="hljs-string">&quot;Hello World&quot;</span><br>tokens = tokenizer.encode(text)  <span class="hljs-comment"># [&quot;Hello&quot;, &quot; World&quot;]</span><br><br><span class="hljs-comment"># 策略2: 转换为小写（BERT uncased）</span><br>text = <span class="hljs-string">&quot;Hello World&quot;</span><br>text_lower = text.lower()<br>tokens = tokenizer.encode(text_lower)  <span class="hljs-comment"># [&quot;hello&quot;, &quot; world&quot;]</span><br><br><span class="hljs-comment"># 策略3: 混合策略</span><br><span class="hljs-comment"># - 保留专有名词的大小写</span><br><span class="hljs-comment"># - 其他词转小写</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">smart_lowercase</span>(<span class="hljs-params">text</span>):<br>    words = text.split()<br>    result = []<br>    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>        <span class="hljs-keyword">if</span> is_proper_noun(word):<br>            result.append(word)<br>        <span class="hljs-keyword">else</span>:<br>            result.append(word.lower())<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27; &#x27;</span>.join(result)<br></code></pre></td></tr></table></figure>
<h4 id="空白字符处理">3.3 空白字符处理</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">normalize_whitespace</span>(<span class="hljs-params">text</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;规范化空白字符&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 1. 替换各种空白字符为标准空格</span><br>    text = re.sub(<span class="hljs-string">r&#x27;[\t\n\r\f\v]&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, text)<br>    <br>    <span class="hljs-comment"># 2. 合并多个连续空格</span><br>    text = re.sub(<span class="hljs-string">r&#x27; +&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, text)<br>    <br>    <span class="hljs-comment"># 3. 去除首尾空格</span><br>    text = text.strip()<br>    <br>    <span class="hljs-keyword">return</span> text<br><br><span class="hljs-comment"># 示例</span><br>text = <span class="hljs-string">&quot;Hello    world\n\nHow  are\tyou?&quot;</span><br>normalized = normalize_whitespace(text)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Normalized: &#x27;<span class="hljs-subst">&#123;normalized&#125;</span>&#x27;&quot;</span>)<br><span class="hljs-comment"># &quot;Hello world How are you?&quot;</span><br></code></pre></td></tr></table></figure>
<h4 id="数字处理">3.4 数字处理</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 策略1: 保留原始数字（GPT）</span><br>text = <span class="hljs-string">&quot;The price is $123.45&quot;</span><br>tokens = tokenizer.encode(text)<br><span class="hljs-comment"># [&quot;The&quot;, &quot; price&quot;, &quot; is&quot;, &quot; $&quot;, &quot;123&quot;, &quot;.&quot;, &quot;45&quot;]</span><br><br><span class="hljs-comment"># 策略2: 数字分离（某些tokenizer）</span><br><span class="hljs-comment"># &quot;123&quot; → [&quot;1&quot;, &quot;2&quot;, &quot;3&quot;]</span><br><br><span class="hljs-comment"># 策略3: 数字规范化</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">normalize_numbers</span>(<span class="hljs-params">text</span>):<br>    <span class="hljs-comment"># 将所有数字替换为特殊token</span><br>    text = re.sub(<span class="hljs-string">r&#x27;\d+&#x27;</span>, <span class="hljs-string">&#x27;[NUM]&#x27;</span>, text)<br>    <span class="hljs-keyword">return</span> text<br><br>text = <span class="hljs-string">&quot;I have 3 apples and 5 oranges&quot;</span><br>normalized = normalize_numbers(text)<br><span class="hljs-comment"># &quot;I have [NUM] apples and [NUM] oranges&quot;</span><br><br><span class="hljs-comment"># 策略4: 数字分组</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">group_digits</span>(<span class="hljs-params">text</span>):<br>    <span class="hljs-comment"># 按位数分组</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">replace_number</span>(<span class="hljs-params"><span class="hljs-keyword">match</span></span>):<br>        num = <span class="hljs-keyword">match</span>.group()<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(num) &lt;= <span class="hljs-number">2</span>:<br>            <span class="hljs-keyword">return</span> num<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 长数字分组</span><br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27; &#x27;</span>.join(num[i:i+<span class="hljs-number">2</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(num), <span class="hljs-number">2</span>))<br>    <br>    <span class="hljs-keyword">return</span> re.sub(<span class="hljs-string">r&#x27;\d+&#x27;</span>, replace_number, text)<br><br>text = <span class="hljs-string">&quot;The year is 2024&quot;</span><br>grouped = group_digits(text)<br><span class="hljs-comment"># &quot;The year is 20 24&quot;</span><br></code></pre></td></tr></table></figure>
<hr />
<h2 id="第六部分多语言tokenization">第六部分：多语言Tokenization</h2>
<h3 id="多语言的挑战">1. 多语言的挑战</h3>
<h4 id="不同语言的特点">1.1 不同语言的特点</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 英语：空格分隔，字母表小</span><br>text_en = <span class="hljs-string">&quot;Hello world&quot;</span><br><span class="hljs-comment"># 特点：26个字母，明确的词边界</span><br><br><span class="hljs-comment"># 中文：无空格，字符多</span><br>text_zh = <span class="hljs-string">&quot;你好世界&quot;</span><br><span class="hljs-comment"># 特点：数千个常用字，无明确词边界</span><br><br><span class="hljs-comment"># 日语：混合文字系统</span><br>text_ja = <span class="hljs-string">&quot;こんにちは世界&quot;</span><br><span class="hljs-comment"># 特点：平假名、片假名、汉字混用</span><br><br><span class="hljs-comment"># 阿拉伯语：从右到左</span><br>text_ar = <span class="hljs-string">&quot;مرحبا بالعالم&quot;</span><br><span class="hljs-comment"># 特点：RTL书写，连写</span><br><br><span class="hljs-comment"># 韩语：音节文字</span><br>text_ko = <span class="hljs-string">&quot;안녕하세요 세계&quot;</span><br><span class="hljs-comment"># 特点：音节组合，有空格</span><br></code></pre></td></tr></table></figure>
<h4 id="词汇表分配不均">1.2 词汇表分配不均</h4>
<p><strong>问题</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 在多语言语料上训练tokenizer</span><br>corpus = &#123;<br>    <span class="hljs-string">&#x27;english&#x27;</span>: <span class="hljs-number">1000000</span>,  <span class="hljs-comment"># 100万句</span><br>    <span class="hljs-string">&#x27;chinese&#x27;</span>: <span class="hljs-number">100000</span>,   <span class="hljs-comment"># 10万句</span><br>    <span class="hljs-string">&#x27;swahili&#x27;</span>: <span class="hljs-number">10000</span>,    <span class="hljs-comment"># 1万句</span><br>&#125;<br><br><span class="hljs-comment"># 训练后的词汇表分布</span><br>vocab_distribution = &#123;<br>    <span class="hljs-string">&#x27;english&#x27;</span>: <span class="hljs-number">25000</span>,  <span class="hljs-comment"># 78%的词汇表</span><br>    <span class="hljs-string">&#x27;chinese&#x27;</span>: <span class="hljs-number">6000</span>,   <span class="hljs-comment"># 19%</span><br>    <span class="hljs-string">&#x27;swahili&#x27;</span>: <span class="hljs-number">1000</span>,   <span class="hljs-comment"># 3%</span><br>&#125;<br><br><span class="hljs-comment"># 结果：</span><br><span class="hljs-comment"># - 英语：高效编码（平均4个字符/token）</span><br><span class="hljs-comment"># - 中文：中等效率（平均2个字符/token）</span><br><span class="hljs-comment"># - 斯瓦希里语：低效（接近字符级）</span><br></code></pre></td></tr></table></figure></p>
<p><strong>解决方案1：语言采样</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sample_corpus</span>(<span class="hljs-params">corpus, target_distribution</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    按目标分布采样语料</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    sampled = &#123;&#125;<br>    <span class="hljs-keyword">for</span> lang, target_ratio <span class="hljs-keyword">in</span> target_distribution.items():<br>        <span class="hljs-comment"># 过采样低资源语言</span><br>        <span class="hljs-keyword">if</span> lang <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;swahili&#x27;</span>, <span class="hljs-string">&#x27;zulu&#x27;</span>, ...]:<br>            sample_ratio = target_ratio * <span class="hljs-number">2</span><br>        <span class="hljs-keyword">else</span>:<br>            sample_ratio = target_ratio<br>        <br>        sampled[lang] = corpus[lang][:<span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(corpus[lang]) * sample_ratio)]<br>    <br>    <span class="hljs-keyword">return</span> sampled<br><br><span class="hljs-comment"># 使用</span><br>target_dist = &#123;<br>    <span class="hljs-string">&#x27;english&#x27;</span>: <span class="hljs-number">0.4</span>,   <span class="hljs-comment"># 40%</span><br>    <span class="hljs-string">&#x27;chinese&#x27;</span>: <span class="hljs-number">0.3</span>,   <span class="hljs-comment"># 30%</span><br>    <span class="hljs-string">&#x27;swahili&#x27;</span>: <span class="hljs-number">0.3</span>,   <span class="hljs-comment"># 30%（过采样）</span><br>&#125;<br><br>balanced_corpus = sample_corpus(corpus, target_dist)<br></code></pre></td></tr></table></figure></p>
<p><strong>解决方案2：字符覆盖率</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># SentencePiece的character_coverage参数</span><br>spm.SentencePieceTrainer.train(<br>    <span class="hljs-built_in">input</span>=<span class="hljs-string">&#x27;multilingual_corpus.txt&#x27;</span>,<br>    model_prefix=<span class="hljs-string">&#x27;multilingual_tokenizer&#x27;</span>,<br>    vocab_size=<span class="hljs-number">250000</span>,  <span class="hljs-comment"># 更大的词汇表</span><br>    character_coverage=<span class="hljs-number">0.9995</span>,  <span class="hljs-comment"># 覆盖99.95%的字符</span><br>    <span class="hljs-comment"># 这会确保低频字符也被包含</span><br>)<br></code></pre></td></tr></table></figure></p>
<h3 id="多语言tokenizer的最佳实践">2. 多语言Tokenizer的最佳实践</h3>
<h4 id="使用sentencepiece">2.1 使用SentencePiece</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> sentencepiece <span class="hljs-keyword">as</span> spm<br><br><span class="hljs-comment"># 训练多语言tokenizer</span><br>spm.SentencePieceTrainer.train(<br>    <span class="hljs-built_in">input</span>=<span class="hljs-string">&#x27;multilingual_corpus.txt&#x27;</span>,<br>    model_prefix=<span class="hljs-string">&#x27;multilingual_sp&#x27;</span>,<br>    vocab_size=<span class="hljs-number">250000</span>,<br>    character_coverage=<span class="hljs-number">0.9995</span>,<br>    model_type=<span class="hljs-string">&#x27;unigram&#x27;</span>,  <span class="hljs-comment"># Unigram对多语言更友好</span><br>    <br>    <span class="hljs-comment"># 多语言设置</span><br>    split_by_unicode_script=<span class="hljs-literal">True</span>,  <span class="hljs-comment"># 按Unicode脚本分割</span><br>    split_by_whitespace=<span class="hljs-literal">True</span>,<br>    split_by_number=<span class="hljs-literal">True</span>,<br>    <br>    <span class="hljs-comment"># 字节回退</span><br>    byte_fallback=<span class="hljs-literal">True</span>,  <span class="hljs-comment"># 处理未知字符</span><br>    <br>    <span class="hljs-comment"># 采样</span><br>    input_sentence_size=<span class="hljs-number">10000000</span>,  <span class="hljs-comment"># 使用1000万句训练</span><br>    shuffle_input_sentence=<span class="hljs-literal">True</span>,<br>)<br></code></pre></td></tr></table></figure>
<h4 id="评估多语言性能">2.2 评估多语言性能</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_multilingual_tokenizer</span>(<span class="hljs-params">tokenizer, test_corpora</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    评估tokenizer在不同语言上的性能</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    results = &#123;&#125;<br>    <br>    <span class="hljs-keyword">for</span> lang, corpus <span class="hljs-keyword">in</span> test_corpora.items():<br>        <span class="hljs-comment"># 1. 压缩率</span><br>        total_chars = <span class="hljs-built_in">sum</span>(<span class="hljs-built_in">len</span>(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> corpus)<br>        total_tokens = <span class="hljs-built_in">sum</span>(<span class="hljs-built_in">len</span>(tokenizer.encode(text)) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> corpus)<br>        compression_rate = total_chars / total_tokens<br>        <br>        <span class="hljs-comment"># 2. 未知token比例</span><br>        unk_count = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> corpus:<br>            tokens = tokenizer.encode(text)<br>            unk_count += tokens.count(tokenizer.unk_id)<br>        unk_ratio = unk_count / total_tokens<br>        <br>        <span class="hljs-comment"># 3. 平均token长度</span><br>        avg_token_len = total_chars / total_tokens<br>        <br>        results[lang] = &#123;<br>            <span class="hljs-string">&#x27;compression_rate&#x27;</span>: compression_rate,<br>            <span class="hljs-string">&#x27;unk_ratio&#x27;</span>: unk_ratio,<br>            <span class="hljs-string">&#x27;avg_token_length&#x27;</span>: avg_token_len,<br>        &#125;<br>    <br>    <span class="hljs-keyword">return</span> results<br><br><span class="hljs-comment"># 使用</span><br>test_corpora = &#123;<br>    <span class="hljs-string">&#x27;english&#x27;</span>: load_corpus(<span class="hljs-string">&#x27;en&#x27;</span>),<br>    <span class="hljs-string">&#x27;chinese&#x27;</span>: load_corpus(<span class="hljs-string">&#x27;zh&#x27;</span>),<br>    <span class="hljs-string">&#x27;arabic&#x27;</span>: load_corpus(<span class="hljs-string">&#x27;ar&#x27;</span>),<br>    <span class="hljs-string">&#x27;swahili&#x27;</span>: load_corpus(<span class="hljs-string">&#x27;sw&#x27;</span>),<br>&#125;<br><br>results = evaluate_multilingual_tokenizer(tokenizer, test_corpora)<br><br><span class="hljs-comment"># 打印结果</span><br><span class="hljs-keyword">for</span> lang, metrics <span class="hljs-keyword">in</span> results.items():<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\n<span class="hljs-subst">&#123;lang&#125;</span>:&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;  Compression rate: <span class="hljs-subst">&#123;metrics[<span class="hljs-string">&#x27;compression_rate&#x27;</span>]:<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;  UNK ratio: <span class="hljs-subst">&#123;metrics[<span class="hljs-string">&#x27;unk_ratio&#x27;</span>]:<span class="hljs-number">.2</span>%&#125;</span>&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;  Avg token length: <span class="hljs-subst">&#123;metrics[<span class="hljs-string">&#x27;avg_token_length&#x27;</span>]:<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 期望结果（示例）：</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">english:</span><br><span class="hljs-string">  Compression rate: 4.2</span><br><span class="hljs-string">  UNK ratio: 0.01%</span><br><span class="hljs-string">  Avg token length: 4.2</span><br><span class="hljs-string"></span><br><span class="hljs-string">chinese:</span><br><span class="hljs-string">  Compression rate: 2.1</span><br><span class="hljs-string">  UNK ratio: 0.05%</span><br><span class="hljs-string">  Avg token length: 2.1</span><br><span class="hljs-string"></span><br><span class="hljs-string">arabic:</span><br><span class="hljs-string">  Compression rate: 3.5</span><br><span class="hljs-string">  UNK ratio: 0.02%</span><br><span class="hljs-string">  Avg token length: 3.5</span><br><span class="hljs-string"></span><br><span class="hljs-string">swahili:</span><br><span class="hljs-string">  Compression rate: 3.8</span><br><span class="hljs-string">  UNK ratio: 0.10%</span><br><span class="hljs-string">  Avg token length: 3.8</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h3 id="代码和特殊领域">3. 代码和特殊领域</h3>
<h4 id="代码tokenization">3.1 代码Tokenization</h4>
<p><strong>挑战</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 代码有特殊的结构</span><br>code = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">def fibonacci(n):</span><br><span class="hljs-string">    if n &lt;= 1:</span><br><span class="hljs-string">        return n</span><br><span class="hljs-string">    return fibonacci(n-1) + fibonacci(n-2)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># 需要保留：</span><br><span class="hljs-comment"># - 缩进（空格/tab）</span><br><span class="hljs-comment"># - 标识符（变量名、函数名）</span><br><span class="hljs-comment"># - 关键字（def, if, return）</span><br><span class="hljs-comment"># - 运算符（+, -, &lt;=）</span><br><span class="hljs-comment"># - 括号和标点</span><br></code></pre></td></tr></table></figure></p>
<p><strong>解决方案</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. 训练代码专用tokenizer</span><br>spm.SentencePieceTrainer.train(<br>    <span class="hljs-built_in">input</span>=<span class="hljs-string">&#x27;code_corpus.txt&#x27;</span>,  <span class="hljs-comment"># 包含多种编程语言</span><br>    model_prefix=<span class="hljs-string">&#x27;code_tokenizer&#x27;</span>,<br>    vocab_size=<span class="hljs-number">50000</span>,<br>    <br>    <span class="hljs-comment"># 代码特定设置</span><br>    split_by_whitespace=<span class="hljs-literal">False</span>,  <span class="hljs-comment"># 保留缩进</span><br>    treat_whitespace_as_suffix=<span class="hljs-literal">False</span>,<br>    <br>    <span class="hljs-comment"># 保留特殊字符</span><br>    user_defined_symbols=[<br>        <span class="hljs-string">&#x27;==&#x27;</span>, <span class="hljs-string">&#x27;!=&#x27;</span>, <span class="hljs-string">&#x27;&lt;=&#x27;</span>, <span class="hljs-string">&#x27;&gt;=&#x27;</span>,  <span class="hljs-comment"># 运算符</span><br>        <span class="hljs-string">&#x27;-&gt;&#x27;</span>, <span class="hljs-string">&#x27;=&gt;&#x27;</span>,              <span class="hljs-comment"># 箭头</span><br>        <span class="hljs-string">&#x27;...&#x27;</span>, <span class="hljs-string">&#x27;...&#x27;</span>,            <span class="hljs-comment"># 省略号</span><br>    ],<br>    <br>    <span class="hljs-comment"># 不分割数字</span><br>    split_by_number=<span class="hljs-literal">False</span>,<br>)<br><br><span class="hljs-comment"># 2. 使用专门的代码tokenizer（如CodeBERT）</span><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer<br><br>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&#x27;microsoft/codebert-base&#x27;</span>)<br>tokens = tokenizer.tokenize(code)<br><span class="hljs-built_in">print</span>(tokens)<br></code></pre></td></tr></table></figure></p>
<h4 id="数学公式">3.2 数学公式</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># LaTeX公式</span><br>formula = <span class="hljs-string">r&quot;\frac&#123;-b \pm \sqrt&#123;b^2 - 4ac&#125;&#125;&#123;2a&#125;&quot;</span><br><br><span class="hljs-comment"># 挑战：</span><br><span class="hljs-comment"># - 特殊符号：\, &#123;, &#125;, ^, _</span><br><span class="hljs-comment"># - 命令：\frac, \sqrt, \pm</span><br><span class="hljs-comment"># - 需要保持结构</span><br><br><span class="hljs-comment"># 解决方案：添加数学符号到词汇表</span><br>math_symbols = [<br>    <span class="hljs-string">r&#x27;\frac&#x27;</span>, <span class="hljs-string">r&#x27;\sqrt&#x27;</span>, <span class="hljs-string">r&#x27;\sum&#x27;</span>, <span class="hljs-string">r&#x27;\int&#x27;</span>,<br>    <span class="hljs-string">r&#x27;\alpha&#x27;</span>, <span class="hljs-string">r&#x27;\beta&#x27;</span>, <span class="hljs-string">r&#x27;\gamma&#x27;</span>,<br>    <span class="hljs-string">r&#x27;\pm&#x27;</span>, <span class="hljs-string">r&#x27;\times&#x27;</span>, <span class="hljs-string">r&#x27;\div&#x27;</span>,<br>]<br><br>tokenizer.add_tokens(math_symbols)<br></code></pre></td></tr></table></figure>
<hr />
<h2
id="第七部分tokenization的陷阱和最佳实践">第七部分：Tokenization的陷阱和最佳实践</h2>
<h3 id="常见陷阱">1. 常见陷阱</h3>
<h4 id="训练-推理不一致">1.1 训练-推理不一致</h4>
<p><strong>问题</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 训练时</span><br>train_tokenizer = load_tokenizer(<span class="hljs-string">&#x27;v1.0&#x27;</span>)<br>train_data = tokenizer.encode(texts)<br><br><span class="hljs-comment"># 几个月后，推理时</span><br>inference_tokenizer = load_tokenizer(<span class="hljs-string">&#x27;v2.0&#x27;</span>)  <span class="hljs-comment"># ❌ 版本不同！</span><br>test_data = tokenizer.encode(texts)<br><br><span class="hljs-comment"># 结果：token IDs不匹配，模型性能下降</span><br></code></pre></td></tr></table></figure></p>
<p><strong>解决方案</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. 版本控制</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TokenizerConfig</span>:<br>    version = <span class="hljs-string">&quot;1.0.0&quot;</span><br>    vocab_size = <span class="hljs-number">32000</span><br>    model_type = <span class="hljs-string">&quot;bpe&quot;</span><br>    <span class="hljs-comment"># ... 其他配置</span><br><br><span class="hljs-comment"># 2. 保存tokenizer和模型一起</span><br>model_dir = <span class="hljs-string">&quot;my_model_v1&quot;</span><br>tokenizer.save(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;model_dir&#125;</span>/tokenizer&quot;</span>)<br>model.save(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;model_dir&#125;</span>/model&quot;</span>)<br><br><span class="hljs-comment"># 3. 在模型配置中记录tokenizer信息</span><br>config = &#123;<br>    <span class="hljs-string">&#x27;model_name&#x27;</span>: <span class="hljs-string">&#x27;my_gpt&#x27;</span>,<br>    <span class="hljs-string">&#x27;tokenizer_version&#x27;</span>: <span class="hljs-string">&#x27;1.0.0&#x27;</span>,<br>    <span class="hljs-string">&#x27;tokenizer_hash&#x27;</span>: compute_hash(tokenizer),<br>&#125;<br></code></pre></td></tr></table></figure></p>
<h4 id="特殊字符处理不当">1.2 特殊字符处理不当</h4>
<p><strong>问题</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 表情符号</span><br>text = <span class="hljs-string">&quot;I love Python! 😍🐍&quot;</span><br><br><span class="hljs-comment"># 不当处理</span><br>tokens_bad = tokenizer.encode(text)<br><span class="hljs-comment"># 可能结果：[&quot;I&quot;, &quot; love&quot;, &quot; Python&quot;, &quot;!&quot;, &quot; &quot;, &quot;�&quot;, &quot;�&quot;, &quot;�&quot;, &quot;�&quot;]</span><br><br><span class="hljs-comment"># 正确处理（字节级BPE）</span><br>tokens_good = byte_level_tokenizer.encode(text)<br><span class="hljs-comment"># [&quot;I&quot;, &quot; love&quot;, &quot; Python&quot;, &quot;!&quot;, &quot; 😍&quot;, &quot;🐍&quot;]</span><br></code></pre></td></tr></table></figure></p>
<p><strong>解决方案</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用字节级BPE或设置byte_fallback</span><br>spm.SentencePieceTrainer.train(<br>    <span class="hljs-built_in">input</span>=<span class="hljs-string">&#x27;corpus.txt&#x27;</span>,<br>    model_prefix=<span class="hljs-string">&#x27;tokenizer&#x27;</span>,<br>    vocab_size=<span class="hljs-number">32000</span>,<br>    byte_fallback=<span class="hljs-literal">True</span>,  <span class="hljs-comment"># 关键！</span><br>    character_coverage=<span class="hljs-number">0.9995</span>,<br>)<br></code></pre></td></tr></table></figure></p>
<h4 id="空格处理不一致">1.3 空格处理不一致</h4>
<p><strong>问题</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 不同的空格处理</span><br>text1 = <span class="hljs-string">&quot;Hello world&quot;</span><br>text2 = <span class="hljs-string">&quot;Hello  world&quot;</span>  <span class="hljs-comment"># 两个空格</span><br><br><span class="hljs-comment"># 某些tokenizer</span><br>tokens1 = tokenizer.encode(text1)  <span class="hljs-comment"># [&quot;Hello&quot;, &quot; world&quot;]</span><br>tokens2 = tokenizer.encode(text2)  <span class="hljs-comment"># [&quot;Hello&quot;, &quot;  world&quot;]  # 不同！</span><br><br><span class="hljs-comment"># 解码后</span><br>decoded1 = tokenizer.decode(tokens1)  <span class="hljs-comment"># &quot;Hello world&quot;</span><br>decoded2 = tokenizer.decode(tokens2)  <span class="hljs-comment"># &quot;Hello  world&quot;</span><br></code></pre></td></tr></table></figure></p>
<p><strong>解决方案</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 预处理：规范化空格</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess</span>(<span class="hljs-params">text</span>):<br>    <span class="hljs-comment"># 合并多个空格</span><br>    text = re.sub(<span class="hljs-string">r&#x27; +&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, text)<br>    <span class="hljs-keyword">return</span> text<br><br>text = preprocess(text)<br>tokens = tokenizer.encode(text)<br></code></pre></td></tr></table></figure></p>
<h3 id="最佳实践">2. 最佳实践</h3>
<h4 id="tokenizer训练流程">2.1 Tokenizer训练流程</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TokenizerTrainingPipeline</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>        self.config = config<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_corpus</span>(<span class="hljs-params">self, raw_files</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;1. 准备训练语料&quot;&quot;&quot;</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Step 1: Preparing corpus...&quot;</span>)<br>        <br>        <span class="hljs-comment"># 合并文件</span><br>        corpus_file = <span class="hljs-string">&#x27;training_corpus.txt&#x27;</span><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(corpus_file, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> outf:<br>            <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> raw_files:<br>                <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> inf:<br>                    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> inf:<br>                        <span class="hljs-comment"># 清洗</span><br>                        line = self.clean_text(line)<br>                        <span class="hljs-keyword">if</span> line.strip():<br>                            outf.write(line + <span class="hljs-string">&#x27;\n&#x27;</span>)<br>        <br>        <span class="hljs-keyword">return</span> corpus_file<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">clean_text</span>(<span class="hljs-params">self, text</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;2. 文本清洗&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># Unicode规范化</span><br>        text = unicodedata.normalize(<span class="hljs-string">&#x27;NFKC&#x27;</span>, text)<br>        <br>        <span class="hljs-comment"># 去除控制字符</span><br>        text = <span class="hljs-string">&#x27;&#x27;</span>.join(ch <span class="hljs-keyword">for</span> ch <span class="hljs-keyword">in</span> text <span class="hljs-keyword">if</span> unicodedata.category(ch)[<span class="hljs-number">0</span>] != <span class="hljs-string">&#x27;C&#x27;</span> <span class="hljs-keyword">or</span> ch <span class="hljs-keyword">in</span> <span class="hljs-string">&#x27;\n\t&#x27;</span>)<br>        <br>        <span class="hljs-comment"># 规范化空白</span><br>        text = re.sub(<span class="hljs-string">r&#x27;[ \t]+&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, text)<br>        <br>        <span class="hljs-keyword">return</span> text<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, corpus_file</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;3. 训练tokenizer&quot;&quot;&quot;</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Step 2: Training tokenizer...&quot;</span>)<br>        <br>        spm.SentencePieceTrainer.train(<br>            <span class="hljs-built_in">input</span>=corpus_file,<br>            model_prefix=self.config[<span class="hljs-string">&#x27;model_prefix&#x27;</span>],<br>            vocab_size=self.config[<span class="hljs-string">&#x27;vocab_size&#x27;</span>],<br>            character_coverage=self.config[<span class="hljs-string">&#x27;character_coverage&#x27;</span>],<br>            model_type=self.config[<span class="hljs-string">&#x27;model_type&#x27;</span>],<br>            <br>            <span class="hljs-comment"># 特殊token</span><br>            pad_id=<span class="hljs-number">0</span>,<br>            unk_id=<span class="hljs-number">1</span>,<br>            bos_id=<span class="hljs-number">2</span>,<br>            eos_id=<span class="hljs-number">3</span>,<br>            <br>            <span class="hljs-comment"># 训练参数</span><br>            num_threads=<span class="hljs-number">16</span>,<br>            train_extremely_large_corpus=<span class="hljs-literal">True</span>,<br>            shuffle_input_sentence=<span class="hljs-literal">True</span>,<br>        )<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params">self, test_corpus</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;4. 评估tokenizer&quot;&quot;&quot;</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Step 3: Evaluating tokenizer...&quot;</span>)<br>        <br>        sp = spm.SentencePieceProcessor()<br>        sp.load(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;self.config[<span class="hljs-string">&#x27;model_prefix&#x27;</span>]&#125;</span>.model&quot;</span>)<br>        <br>        metrics = &#123;<br>            <span class="hljs-string">&#x27;compression_rate&#x27;</span>: [],<br>            <span class="hljs-string">&#x27;unk_ratio&#x27;</span>: [],<br>        &#125;<br>        <br>        <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> test_corpus:<br>            tokens = sp.encode_as_ids(text)<br>            metrics[<span class="hljs-string">&#x27;compression_rate&#x27;</span>].append(<span class="hljs-built_in">len</span>(text) / <span class="hljs-built_in">len</span>(tokens))<br>            metrics[<span class="hljs-string">&#x27;unk_ratio&#x27;</span>].append(tokens.count(sp.unk_id()) / <span class="hljs-built_in">len</span>(tokens))<br>        <br>        <span class="hljs-keyword">return</span> &#123;<br>            <span class="hljs-string">&#x27;avg_compression_rate&#x27;</span>: np.mean(metrics[<span class="hljs-string">&#x27;compression_rate&#x27;</span>]),<br>            <span class="hljs-string">&#x27;avg_unk_ratio&#x27;</span>: np.mean(metrics[<span class="hljs-string">&#x27;unk_ratio&#x27;</span>]),<br>        &#125;<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_metadata</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;5. 保存元数据&quot;&quot;&quot;</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Step 4: Saving metadata...&quot;</span>)<br>        <br>        metadata = &#123;<br>            <span class="hljs-string">&#x27;version&#x27;</span>: <span class="hljs-string">&#x27;1.0.0&#x27;</span>,<br>            <span class="hljs-string">&#x27;config&#x27;</span>: self.config,<br>            <span class="hljs-string">&#x27;training_date&#x27;</span>: datetime.now().isoformat(),<br>            <span class="hljs-string">&#x27;corpus_stats&#x27;</span>: self.get_corpus_stats(),<br>        &#125;<br>        <br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;self.config[<span class="hljs-string">&#x27;model_prefix&#x27;</span>]&#125;</span>_metadata.json&quot;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            json.dump(metadata, f, indent=<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># 使用</span><br>config = &#123;<br>    <span class="hljs-string">&#x27;model_prefix&#x27;</span>: <span class="hljs-string">&#x27;my_tokenizer&#x27;</span>,<br>    <span class="hljs-string">&#x27;vocab_size&#x27;</span>: <span class="hljs-number">32000</span>,<br>    <span class="hljs-string">&#x27;character_coverage&#x27;</span>: <span class="hljs-number">0.9995</span>,<br>    <span class="hljs-string">&#x27;model_type&#x27;</span>: <span class="hljs-string">&#x27;bpe&#x27;</span>,<br>&#125;<br><br>pipeline = TokenizerTrainingPipeline(config)<br>corpus_file = pipeline.prepare_corpus(raw_files)<br>pipeline.train(corpus_file)<br>metrics = pipeline.evaluate(test_corpus)<br>pipeline.save_metadata()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Training completed!&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Metrics: <span class="hljs-subst">&#123;metrics&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<h4 id="tokenizer测试">2.2 Tokenizer测试</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> unittest<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TokenizerTests</span>(unittest.TestCase):<br><span class="hljs-meta">    @classmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">setUpClass</span>(<span class="hljs-params">cls</span>):<br>        cls.tokenizer = load_tokenizer(<span class="hljs-string">&#x27;my_tokenizer&#x27;</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">test_reversibility</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;测试可逆性&quot;&quot;&quot;</span><br>        texts = [<br>            <span class="hljs-string">&quot;Hello, world!&quot;</span>,<br>            <span class="hljs-string">&quot;你好，世界！&quot;</span>,<br>            <span class="hljs-string">&quot;مرحبا بالعالم&quot;</span>,<br>            <span class="hljs-string">&quot;Hello 世界 مرحبا&quot;</span>,<br>        ]<br>        <br>        <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>            encoded = self.tokenizer.encode(text)<br>            decoded = self.tokenizer.decode(encoded)<br>            self.assertEqual(text, decoded, <span class="hljs-string">f&quot;Failed for: <span class="hljs-subst">&#123;text&#125;</span>&quot;</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">test_special_tokens</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;测试特殊token&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># BOS/EOS</span><br>        text = <span class="hljs-string">&quot;Hello&quot;</span><br>        encoded = self.tokenizer.encode(text, add_special_tokens=<span class="hljs-literal">True</span>)<br>        self.assertEqual(encoded[<span class="hljs-number">0</span>], self.tokenizer.bos_id)<br>        self.assertEqual(encoded[-<span class="hljs-number">1</span>], self.tokenizer.eos_id)<br>        <br>        <span class="hljs-comment"># PAD</span><br>        self.assertIsNotNone(self.tokenizer.pad_id)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">test_empty_string</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;测试空字符串&quot;&quot;&quot;</span><br>        encoded = self.tokenizer.encode(<span class="hljs-string">&quot;&quot;</span>)<br>        self.assertEqual(<span class="hljs-built_in">len</span>(encoded), <span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">test_long_text</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;测试长文本&quot;&quot;&quot;</span><br>        long_text = <span class="hljs-string">&quot;Hello &quot;</span> * <span class="hljs-number">10000</span><br>        encoded = self.tokenizer.encode(long_text)<br>        decoded = self.tokenizer.decode(encoded)<br>        self.assertEqual(long_text, decoded)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">test_special_characters</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;测试特殊字符&quot;&quot;&quot;</span><br>        special_texts = [<br>            <span class="hljs-string">&quot;Hello\nWorld&quot;</span>,  <span class="hljs-comment"># 换行</span><br>            <span class="hljs-string">&quot;Hello\tWorld&quot;</span>,  <span class="hljs-comment"># Tab</span><br>            <span class="hljs-string">&quot;Hello😀World&quot;</span>,  <span class="hljs-comment"># Emoji</span><br>            <span class="hljs-string">&quot;Hello\u200bWorld&quot;</span>,  <span class="hljs-comment"># 零宽空格</span><br>        ]<br>        <br>        <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> special_texts:<br>            encoded = self.tokenizer.encode(text)<br>            decoded = self.tokenizer.decode(encoded)<br>            self.assertEqual(text, decoded)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">test_consistency</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;测试一致性&quot;&quot;&quot;</span><br>        text = <span class="hljs-string">&quot;Hello, world!&quot;</span><br>        <br>        <span class="hljs-comment"># 多次编码应该得到相同结果</span><br>        encoded1 = self.tokenizer.encode(text)<br>        encoded2 = self.tokenizer.encode(text)<br>        self.assertEqual(encoded1, encoded2)<br><br><span class="hljs-comment"># 运行测试</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    unittest.main()<br></code></pre></td></tr></table></figure>
<hr />
<h2 id="第八部分实战案例分析">第八部分：实战案例分析</h2>
<h3 id="gpt-2gpt-3-tokenizer深度解析">1. GPT-2/GPT-3
Tokenizer深度解析</h3>
<h4 id="设计特点">1.1 设计特点</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer<br><br>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&#x27;gpt2&#x27;</span>)<br><br><span class="hljs-comment"># 关键特性</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Vocab size: <span class="hljs-subst">&#123;tokenizer.vocab_size&#125;</span>&quot;</span>)  <span class="hljs-comment"># 50257</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Model max length: <span class="hljs-subst">&#123;tokenizer.model_max_length&#125;</span>&quot;</span>)  <span class="hljs-comment"># 1024</span><br><br><span class="hljs-comment"># 特殊token</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;BOS token: <span class="hljs-subst">&#123;tokenizer.bos_token&#125;</span>&quot;</span>)  <span class="hljs-comment"># &#x27;&lt;|endoftext|&gt;&#x27;</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;EOS token: <span class="hljs-subst">&#123;tokenizer.eos_token&#125;</span>&quot;</span>)  <span class="hljs-comment"># &#x27;&lt;|endoftext|&gt;&#x27;</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;PAD token: <span class="hljs-subst">&#123;tokenizer.pad_token&#125;</span>&quot;</span>)  <span class="hljs-comment"># None（GPT-2没有PAD）</span><br></code></pre></td></tr></table></figure>
<p><strong>字节级BPE的实现</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># GPT-2使用字节级BPE</span><br>text = <span class="hljs-string">&quot;Hello, 世界! 😀&quot;</span><br><br><span class="hljs-comment"># 1. 转换为字节</span><br>bytes_text = text.encode(<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Bytes: <span class="hljs-subst">&#123;bytes_text&#125;</span>&quot;</span>)<br><span class="hljs-comment"># b&#x27;Hello, \xe4\xb8\x96\xe7\x95\x8c! \xf0\x9f\x98\x80&#x27;</span><br><br><span class="hljs-comment"># 2. 字节到Unicode映射</span><br>byte_encoder = tokenizer.byte_encoder<br>unicode_text = <span class="hljs-string">&#x27;&#x27;</span>.join([byte_encoder[b] <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> bytes_text])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Unicode representation: <span class="hljs-subst">&#123;unicode_text&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 3. BPE分词</span><br>tokens = tokenizer.tokenize(text)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Tokens: <span class="hljs-subst">&#123;tokens&#125;</span>&quot;</span>)<br><span class="hljs-comment"># [&#x27;Hello&#x27;, &#x27;,&#x27;, &#x27; äļ&#x27;, &#x27;ĭ&#x27;, &#x27;ķ&#x27;, &#x27;çĥ&#x27;, &#x27;¼&#x27;, &#x27;!&#x27;, &#x27; ðŁĺ&#x27;, &#x27;Ģ&#x27;]</span><br><br><span class="hljs-comment"># 4. Token IDs</span><br>ids = tokenizer.encode(text)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;IDs: <span class="hljs-subst">&#123;ids&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure></p>
<p><strong>为什么词汇表是50257？</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 256个字节 + 50000个合并 + 1个特殊token</span><br>base_vocab = <span class="hljs-number">256</span>  <span class="hljs-comment"># 所有可能的字节</span><br>num_merges = <span class="hljs-number">50000</span>  <span class="hljs-comment"># BPE合并次数</span><br>special_tokens = <span class="hljs-number">1</span>  <span class="hljs-comment"># &lt;|endoftext|&gt;</span><br><br>total = base_vocab + num_merges + special_tokens<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Total vocab size: <span class="hljs-subst">&#123;total&#125;</span>&quot;</span>)  <span class="hljs-comment"># 50257</span><br></code></pre></td></tr></table></figure></p>
<h4 id="实际使用">1.2 实际使用</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 示例1：基本编码解码</span><br>text = <span class="hljs-string">&quot;The quick brown fox jumps over the lazy dog.&quot;</span><br>tokens = tokenizer.tokenize(text)<br>ids = tokenizer.encode(text)<br>decoded = tokenizer.decode(ids)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Original: <span class="hljs-subst">&#123;text&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Tokens: <span class="hljs-subst">&#123;tokens&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;IDs: <span class="hljs-subst">&#123;ids&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Decoded: <span class="hljs-subst">&#123;decoded&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 示例2：批处理</span><br>texts = [<br>    <span class="hljs-string">&quot;Hello, world!&quot;</span>,<br>    <span class="hljs-string">&quot;How are you?&quot;</span>,<br>    <span class="hljs-string">&quot;I&#x27;m fine, thank you.&quot;</span><br>]<br><br><span class="hljs-comment"># 编码（自动填充）</span><br>encoded = tokenizer(<br>    texts,<br>    padding=<span class="hljs-literal">True</span>,<br>    truncation=<span class="hljs-literal">True</span>,<br>    max_length=<span class="hljs-number">512</span>,<br>    return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span><br>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Input IDs shape: <span class="hljs-subst">&#123;encoded[<span class="hljs-string">&#x27;input_ids&#x27;</span>].shape&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Attention mask shape: <span class="hljs-subst">&#123;encoded[<span class="hljs-string">&#x27;attention_mask&#x27;</span>].shape&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 示例3：生成任务</span><br>prompt = <span class="hljs-string">&quot;Once upon a time&quot;</span><br>input_ids = tokenizer.encode(prompt, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br><br><span class="hljs-comment"># 生成（假设有模型）</span><br><span class="hljs-comment"># output_ids = model.generate(input_ids, max_length=50)</span><br><span class="hljs-comment"># generated_text = tokenizer.decode(output_ids[0])</span><br></code></pre></td></tr></table></figure>
<h3 id="bert-tokenizer深度解析">2. BERT Tokenizer深度解析</h3>
<h4 id="wordpiece特点">2.1 WordPiece特点</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer<br><br>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>)<br><br><span class="hljs-comment"># 关键特性</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Vocab size: <span class="hljs-subst">&#123;tokenizer.vocab_size&#125;</span>&quot;</span>)  <span class="hljs-comment"># 30522</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Do lower case: <span class="hljs-subst">&#123;tokenizer.do_lower_case&#125;</span>&quot;</span>)  <span class="hljs-comment"># True</span><br><br><span class="hljs-comment"># 特殊token</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;CLS token: <span class="hljs-subst">&#123;tokenizer.cls_token&#125;</span>&quot;</span>)  <span class="hljs-comment"># &#x27;[CLS]&#x27;</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;SEP token: <span class="hljs-subst">&#123;tokenizer.sep_token&#125;</span>&quot;</span>)  <span class="hljs-comment"># &#x27;[SEP]&#x27;</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;PAD token: <span class="hljs-subst">&#123;tokenizer.pad_token&#125;</span>&quot;</span>)  <span class="hljs-comment"># &#x27;[PAD]&#x27;</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;MASK token: <span class="hljs-subst">&#123;tokenizer.mask_token&#125;</span>&quot;</span>)  <span class="hljs-comment"># &#x27;[MASK]&#x27;</span><br></code></pre></td></tr></table></figure>
<p><strong>WordPiece的##前缀</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 子词使用##前缀</span><br>text = <span class="hljs-string">&quot;playing football&quot;</span><br>tokens = tokenizer.tokenize(text)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Tokens: <span class="hljs-subst">&#123;tokens&#125;</span>&quot;</span>)<br><span class="hljs-comment"># [&#x27;play&#x27;, &#x27;##ing&#x27;, &#x27;football&#x27;]</span><br><br><span class="hljs-comment"># 完整的编码（包含特殊token）</span><br>encoded = tokenizer.encode(text, add_special_tokens=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Encoded: <span class="hljs-subst">&#123;encoded&#125;</span>&quot;</span>)<br><span class="hljs-comment"># [101, 2652, 2075, 2374, 102]</span><br><span class="hljs-comment"># 101: [CLS], 2652: play, 2075: ##ing, 2374: football, 102: [SEP]</span><br><br><span class="hljs-comment"># 解码</span><br>decoded = tokenizer.decode(encoded)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Decoded: <span class="hljs-subst">&#123;decoded&#125;</span>&quot;</span>)<br><span class="hljs-comment"># &quot;[CLS] playing football [SEP]&quot;</span><br></code></pre></td></tr></table></figure></p>
<h4 id="bert的特殊用法">2.2 BERT的特殊用法</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. 单句分类</span><br>text = <span class="hljs-string">&quot;This movie is great!&quot;</span><br>encoded = tokenizer(<br>    text,<br>    add_special_tokens=<span class="hljs-literal">True</span>,<br>    max_length=<span class="hljs-number">512</span>,<br>    padding=<span class="hljs-string">&#x27;max_length&#x27;</span>,<br>    truncation=<span class="hljs-literal">True</span>,<br>    return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span><br>)<br><span class="hljs-comment"># 格式: [CLS] This movie is great ! [SEP] [PAD] [PAD] ...</span><br><br><span class="hljs-comment"># 2. 句子对任务（如NLI）</span><br>text_a = <span class="hljs-string">&quot;The cat sat on the mat.&quot;</span><br>text_b = <span class="hljs-string">&quot;A cat was sitting on a mat.&quot;</span><br><br>encoded = tokenizer(<br>    text_a,<br>    text_b,<br>    add_special_tokens=<span class="hljs-literal">True</span>,<br>    max_length=<span class="hljs-number">512</span>,<br>    padding=<span class="hljs-string">&#x27;max_length&#x27;</span>,<br>    truncation=<span class="hljs-literal">True</span>,<br>    return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span><br>)<br><span class="hljs-comment"># 格式: [CLS] text_a [SEP] text_b [SEP] [PAD] ...</span><br><br><span class="hljs-comment"># token_type_ids区分两个句子</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Token type IDs: <span class="hljs-subst">&#123;encoded[<span class="hljs-string">&#x27;token_type_ids&#x27;</span>]&#125;</span>&quot;</span>)<br><span class="hljs-comment"># [0, 0, 0, ..., 0, 1, 1, ..., 1, 0, 0, ...]</span><br><span class="hljs-comment">#  ↑ text_a      ↑ text_b      ↑ padding</span><br><br><span class="hljs-comment"># 3. 掩码语言模型</span><br>text = <span class="hljs-string">&quot;The cat sat on the [MASK].&quot;</span><br>encoded = tokenizer.encode(text)<br>mask_token_id = tokenizer.mask_token_id<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Encoded: <span class="hljs-subst">&#123;encoded&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;MASK token ID: <span class="hljs-subst">&#123;mask_token_id&#125;</span>&quot;</span>)<br><span class="hljs-comment"># 模型预测[MASK]位置的token</span><br></code></pre></td></tr></table></figure>
<h3 id="llama-tokenizer深度解析">3. Llama Tokenizer深度解析</h3>
<h4 id="sentencepiece-bpe">3.1 SentencePiece BPE</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> LlamaTokenizer<br><br>tokenizer = LlamaTokenizer.from_pretrained(<span class="hljs-string">&#x27;meta-llama/Llama-2-7b-hf&#x27;</span>)<br><br><span class="hljs-comment"># 关键特性</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Vocab size: <span class="hljs-subst">&#123;tokenizer.vocab_size&#125;</span>&quot;</span>)  <span class="hljs-comment"># 32000</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;BOS token: <span class="hljs-subst">&#123;tokenizer.bos_token&#125;</span>&quot;</span>)  <span class="hljs-comment"># &#x27;&lt;s&gt;&#x27;</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;EOS token: <span class="hljs-subst">&#123;tokenizer.eos_token&#125;</span>&quot;</span>)  <span class="hljs-comment"># &#x27;&lt;/s&gt;&#x27;</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;UNK token: <span class="hljs-subst">&#123;tokenizer.unk_token&#125;</span>&quot;</span>)  <span class="hljs-comment"># &#x27;&lt;unk&gt;&#x27;</span><br></code></pre></td></tr></table></figure>
<p><strong>SentencePiece的▁符号</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">text = <span class="hljs-string">&quot;Hello world&quot;</span><br>tokens = tokenizer.tokenize(text)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Tokens: <span class="hljs-subst">&#123;tokens&#125;</span>&quot;</span>)<br><span class="hljs-comment"># [&#x27;▁Hello&#x27;, &#x27;▁world&#x27;]</span><br><br><span class="hljs-comment"># 注意：▁表示原始空格</span><br>text_no_space = <span class="hljs-string">&quot;Helloworld&quot;</span><br>tokens_no_space = tokenizer.tokenize(text_no_space)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Tokens (no space): <span class="hljs-subst">&#123;tokens_no_space&#125;</span>&quot;</span>)<br><span class="hljs-comment"># [&#x27;▁Hello&#x27;, &#x27;world&#x27;]  # 第二个token没有▁</span><br></code></pre></td></tr></table></figure></p>
<h4 id="llama-2的chat格式">3.2 Llama 2的Chat格式</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Llama 2 Chat使用特殊格式</span><br>B_INST, E_INST = <span class="hljs-string">&quot;[INST]&quot;</span>, <span class="hljs-string">&quot;[/INST]&quot;</span><br>B_SYS, E_SYS = <span class="hljs-string">&quot;&lt;&lt;SYS&gt;&gt;\n&quot;</span>, <span class="hljs-string">&quot;\n&lt;&lt;/SYS&gt;&gt;\n\n&quot;</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">format_llama2_prompt</span>(<span class="hljs-params">system_prompt, user_message</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;格式化Llama 2 Chat提示&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-string">f&quot;&lt;s&gt;<span class="hljs-subst">&#123;B_INST&#125;</span> <span class="hljs-subst">&#123;B_SYS&#125;</span><span class="hljs-subst">&#123;system_prompt&#125;</span><span class="hljs-subst">&#123;E_SYS&#125;</span><span class="hljs-subst">&#123;user_message&#125;</span> <span class="hljs-subst">&#123;E_INST&#125;</span>&quot;</span><br><br><span class="hljs-comment"># 使用</span><br>system_prompt = <span class="hljs-string">&quot;You are a helpful, respectful and honest assistant.&quot;</span><br>user_message = <span class="hljs-string">&quot;What is the capital of France?&quot;</span><br><br>prompt = format_llama2_prompt(system_prompt, user_message)<br><span class="hljs-built_in">print</span>(prompt)<br><br><span class="hljs-comment"># Tokenize</span><br>tokens = tokenizer.encode(prompt)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Token count: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(tokens)&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<hr />
<h2 id="第九部分性能优化">第九部分：性能优化</h2>
<h3 id="编码速度优化">1. 编码速度优化</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">benchmark_tokenizer</span>(<span class="hljs-params">tokenizer, texts, num_runs=<span class="hljs-number">100</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;测试tokenizer性能&quot;&quot;&quot;</span><br>    <br>    <span class="hljs-comment"># 1. 单个文本编码</span><br>    start = time.time()<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_runs):<br>        <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>            _ = tokenizer.encode(text)<br>    single_time = time.time() - start<br>    <br>    <span class="hljs-comment"># 2. 批量编码</span><br>    start = time.time()<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_runs):<br>        _ = tokenizer.batch_encode_plus(texts)<br>    batch_time = time.time() - start<br>    <br>    <span class="hljs-comment"># 3. 并行编码（如果支持）</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(tokenizer, <span class="hljs-string">&#x27;enable_parallelism&#x27;</span>):<br>        tokenizer.enable_parallelism(<span class="hljs-literal">True</span>)<br>        start = time.time()<br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_runs):<br>            _ = tokenizer.batch_encode_plus(texts)<br>        parallel_time = time.time() - start<br>    <span class="hljs-keyword">else</span>:<br>        parallel_time = <span class="hljs-literal">None</span><br>    <br>    <span class="hljs-keyword">return</span> &#123;<br>        <span class="hljs-string">&#x27;single&#x27;</span>: single_time,<br>        <span class="hljs-string">&#x27;batch&#x27;</span>: batch_time,<br>        <span class="hljs-string">&#x27;parallel&#x27;</span>: parallel_time,<br>        <span class="hljs-string">&#x27;speedup_batch&#x27;</span>: single_time / batch_time,<br>        <span class="hljs-string">&#x27;speedup_parallel&#x27;</span>: single_time / parallel_time <span class="hljs-keyword">if</span> parallel_time <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>    &#125;<br><br><span class="hljs-comment"># 测试</span><br>texts = [<span class="hljs-string">&quot;Hello world&quot;</span>] * <span class="hljs-number">1000</span><br>results = benchmark_tokenizer(tokenizer, texts)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Performance Results:&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Single encoding: <span class="hljs-subst">&#123;results[<span class="hljs-string">&#x27;single&#x27;</span>]:<span class="hljs-number">.3</span>f&#125;</span>s&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Batch encoding: <span class="hljs-subst">&#123;results[<span class="hljs-string">&#x27;batch&#x27;</span>]:<span class="hljs-number">.3</span>f&#125;</span>s&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Speedup (batch): <span class="hljs-subst">&#123;results[<span class="hljs-string">&#x27;speedup_batch&#x27;</span>]:<span class="hljs-number">.2</span>f&#125;</span>x&quot;</span>)<br><span class="hljs-keyword">if</span> results[<span class="hljs-string">&#x27;parallel&#x27;</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Parallel encoding: <span class="hljs-subst">&#123;results[<span class="hljs-string">&#x27;parallel&#x27;</span>]:<span class="hljs-number">.3</span>f&#125;</span>s&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Speedup (parallel): <span class="hljs-subst">&#123;results[<span class="hljs-string">&#x27;speedup_parallel&#x27;</span>]:<span class="hljs-number">.2</span>f&#125;</span>x&quot;</span>)<br></code></pre></td></tr></table></figure>
<h3 id="内存优化">2. 内存优化</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. 使用生成器处理大文件</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_large_file</span>(<span class="hljs-params">file_path, tokenizer, batch_size=<span class="hljs-number">1000</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;流式处理大文件&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">read_batches</span>():<br>        batch = []<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f:<br>                batch.append(line.strip())<br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(batch) &gt;= batch_size:<br>                    <span class="hljs-keyword">yield</span> batch<br>                    batch = []<br>            <span class="hljs-keyword">if</span> batch:<br>                <span class="hljs-keyword">yield</span> batch<br>    <br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> read_batches():<br>        encoded = tokenizer.batch_encode_plus(<br>            batch,<br>            padding=<span class="hljs-literal">True</span>,<br>            truncation=<span class="hljs-literal">True</span>,<br>            max_length=<span class="hljs-number">512</span>,<br>            return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span><br>        )<br>        <span class="hljs-comment"># 处理encoded batch</span><br>        <span class="hljs-keyword">yield</span> encoded<br><br><span class="hljs-comment"># 2. 缓存常用编码</span><br><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> lru_cache<br><br><span class="hljs-meta">@lru_cache(<span class="hljs-params">maxsize=<span class="hljs-number">10000</span></span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cached_encode</span>(<span class="hljs-params">text, tokenizer_name</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;缓存编码结果&quot;&quot;&quot;</span><br>    tokenizer = load_tokenizer(tokenizer_name)<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">tuple</span>(tokenizer.encode(text))<br><br><span class="hljs-comment"># 3. 使用更紧凑的数据类型</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 默认：int64</span><br>ids_int64 = np.array(tokenizer.encode(text), dtype=np.int64)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Size (int64): <span class="hljs-subst">&#123;ids_int64.nbytes&#125;</span> bytes&quot;</span>)<br><br><span class="hljs-comment"># 优化：int32（词汇表&lt;2^31）</span><br>ids_int32 = np.array(tokenizer.encode(text), dtype=np.int32)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Size (int32): <span class="hljs-subst">&#123;ids_int32.nbytes&#125;</span> bytes&quot;</span>)<br><br><span class="hljs-comment"># 进一步优化：int16（词汇表&lt;2^15）</span><br><span class="hljs-keyword">if</span> tokenizer.vocab_size &lt; <span class="hljs-number">32768</span>:<br>    ids_int16 = np.array(tokenizer.encode(text), dtype=np.int16)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Size (int16): <span class="hljs-subst">&#123;ids_int16.nbytes&#125;</span> bytes&quot;</span>)<br></code></pre></td></tr></table></figure>
<h3 id="分布式tokenization">3. 分布式Tokenization</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> multiprocessing <span class="hljs-keyword">import</span> Pool<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_chunk</span>(<span class="hljs-params">args</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;处理一个数据块&quot;&quot;&quot;</span><br>    texts, tokenizer_path = args<br>    <br>    <span class="hljs-comment"># 在子进程中加载tokenizer</span><br>    tokenizer = load_tokenizer(tokenizer_path)<br>    <br>    results = []<br>    <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>        encoded = tokenizer.encode(text)<br>        results.append(encoded)<br>    <br>    <span class="hljs-keyword">return</span> results<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parallel_tokenize</span>(<span class="hljs-params">texts, tokenizer_path, num_workers=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;并行tokenization&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> num_workers <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        num_workers = os.cpu_count()<br>    <br>    <span class="hljs-comment"># 分割数据</span><br>    chunk_size = <span class="hljs-built_in">len</span>(texts) // num_workers<br>    chunks = [<br>        texts[i:i + chunk_size]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(texts), chunk_size)<br>    ]<br>    <br>    <span class="hljs-comment"># 并行处理</span><br>    <span class="hljs-keyword">with</span> Pool(num_workers) <span class="hljs-keyword">as</span> pool:<br>        args = [(chunk, tokenizer_path) <span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> chunks]<br>        results = pool.<span class="hljs-built_in">map</span>(tokenize_chunk, args)<br>    <br>    <span class="hljs-comment"># 合并结果</span><br>    all_encoded = []<br>    <span class="hljs-keyword">for</span> chunk_results <span class="hljs-keyword">in</span> results:<br>        all_encoded.extend(chunk_results)<br>    <br>    <span class="hljs-keyword">return</span> all_encoded<br><br><span class="hljs-comment"># 使用</span><br>texts = load_large_corpus()<br>encoded = parallel_tokenize(texts, <span class="hljs-string">&#x27;my_tokenizer.model&#x27;</span>, num_workers=<span class="hljs-number">8</span>)<br></code></pre></td></tr></table></figure>
<hr />
<h2 id="总结">总结</h2>
<h3 id="核心要点回顾">核心要点回顾</h3>
<ol type="1">
<li><strong>Tokenization是LLM的第一步</strong>：
<ul>
<li>将文本转换为数字序列</li>
<li>平衡词汇表大小和序列长度</li>
<li>影响模型的效率和能力</li>
</ul></li>
<li><strong>三种主流算法</strong>：
<ul>
<li><strong>BPE</strong>：简单高效，自底向上合并</li>
<li><strong>WordPiece</strong>：基于似然度，BERT使用</li>
<li><strong>Unigram</strong>：自顶向下删除，灵活鲁棒</li>
</ul></li>
<li><strong>SentencePiece是生产标准</strong>：
<ul>
<li>语言无关</li>
<li>完全可逆</li>
<li>支持子词正则化</li>
</ul></li>
<li><strong>关键设计决策</strong>：
<ul>
<li>词汇表大小：30k-50k是平衡点</li>
<li>特殊token：PAD, UNK, BOS, EOS等</li>
<li>预处理：Unicode规范化、空格处理</li>
</ul></li>
<li><strong>多语言挑战</strong>：
<ul>
<li>词汇表分配不均</li>
<li>需要语言采样和字符覆盖率调整</li>
<li>Unigram对多语言更友好</li>
</ul></li>
<li><strong>最佳实践</strong>：
<ul>
<li>版本控制和元数据</li>
<li>全面的测试（可逆性、特殊字符）</li>
<li>性能优化（批处理、并行、缓存）</li>
</ul></li>
</ol>
<h3 id="实践建议">实践建议</h3>
<p><strong>选择tokenizer</strong>： <figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">英文为主 → <span class="hljs-keyword">BPE </span>(GPT风格)<br>多任务 → WordPiece (<span class="hljs-keyword">BERT风格)</span><br><span class="hljs-keyword"></span>多语言 → Unigram (<span class="hljs-built_in">T5</span>风格)<br>生产环境 → SentencePiece<br></code></pre></td></tr></table></figure></p>
<p><strong>词汇表大小</strong>： <figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs subunit">小模型(&lt;1B) → 16k<span class="hljs-string">-32</span>k<br>中等模型(1B<span class="hljs-string">-10</span>B) → 32k<span class="hljs-string">-50</span>k<br>大模型(&gt;10B) → 50k<span class="hljs-string">-100</span>k<br>多语言 → 100k<span class="hljs-string">-256</span>k<br></code></pre></td></tr></table></figure></p>
<p><strong>训练流程</strong>： <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">1.</span> 准备和清洗语料<br><span class="hljs-bullet">2.</span> 选择算法和参数<br><span class="hljs-bullet">3.</span> 训练tokenizer<br><span class="hljs-bullet">4.</span> 评估性能（压缩率、覆盖率）<br><span class="hljs-bullet">5.</span> 测试边界情况<br><span class="hljs-bullet">6.</span> 保存元数据和版本信息<br></code></pre></td></tr></table></figure></p>
<h3 id="下一步学习">下一步学习</h3>
<p><strong>第二讲预告</strong>：模型架构 - Transformer详解 - 注意力机制
- 位置编码 - 前馈网络</p>
<p><strong>推荐资源</strong>： - <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.07909">Neural Machine Translation of
Rare Words with Subword Units</a> - BPE原始论文 - <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.06226">SentencePiece: A simple and
language independent approach</a> - SentencePiece论文 - <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.10959">Subword Regularization</a> -
Unigram论文 - <a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/">Hugging
Face Tokenizers文档</a> - <a
target="_blank" rel="noopener" href="https://github.com/google/sentencepiece">SentencePiece
GitHub</a></p>
<hr />
<h2 id="附录快速参考">附录：快速参考</h2>
<h3 id="a.-常用代码片段">A. 常用代码片段</h3>
<p><strong>训练BPE tokenizer</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer<br><span class="hljs-keyword">from</span> tokenizers.models <span class="hljs-keyword">import</span> BPE<br><span class="hljs-keyword">from</span> tokenizers.trainers <span class="hljs-keyword">import</span> BpeTrainer<br><br>tokenizer = Tokenizer(BPE(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))<br>trainer = BpeTrainer(vocab_size=<span class="hljs-number">30000</span>, special_tokens=[<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[BOS]&quot;</span>, <span class="hljs-string">&quot;[EOS]&quot;</span>])<br>tokenizer.train(files=[<span class="hljs-string">&quot;corpus.txt&quot;</span>], trainer=trainer)<br>tokenizer.save(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)<br></code></pre></td></tr></table></figure></p>
<p><strong>训练SentencePiece</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> sentencepiece <span class="hljs-keyword">as</span> spm<br><br>spm.SentencePieceTrainer.train(<br>    <span class="hljs-built_in">input</span>=<span class="hljs-string">&#x27;corpus.txt&#x27;</span>,<br>    model_prefix=<span class="hljs-string">&#x27;sp_model&#x27;</span>,<br>    vocab_size=<span class="hljs-number">32000</span>,<br>    model_type=<span class="hljs-string">&#x27;bpe&#x27;</span>,<br>    character_coverage=<span class="hljs-number">0.9995</span><br>)<br></code></pre></td></tr></table></figure></p>
<p><strong>使用Hugging Face tokenizer</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer<br><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;gpt2&#x27;</span>)<br>encoded = tokenizer(<span class="hljs-string">&quot;Hello, world!&quot;</span>, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br>decoded = tokenizer.decode(encoded[<span class="hljs-string">&#x27;input_ids&#x27;</span>][<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure></p>
<h3 id="b.-调试检查清单">B. 调试检查清单</h3>
<ul class="task-list">
<li><label><input
type="checkbox" />可逆性：<code>text == tokenizer.decode(tokenizer.encode(text))</code></label></li>
<li><label><input
type="checkbox" />特殊字符：测试emoji、Unicode、控制字符</label></li>
<li><label><input
type="checkbox" />空字符串：<code>tokenizer.encode("")</code>应该返回空或只有特殊token</label></li>
<li><label><input
type="checkbox" />长文本：测试超长文本的处理</label></li>
<li><label><input
type="checkbox" />批处理：验证批处理和单个处理结果一致</label></li>
<li><label><input
type="checkbox" />多语言：测试不同语言的编码效率</label></li>
<li><label><input
type="checkbox" />版本一致性：确保训练和推理使用相同版本</label></li>
</ul>
<h3 id="c.-性能基准">C. 性能基准</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">操作</th>
<th style="text-align: left;">GPT-2</th>
<th style="text-align: left;">BERT</th>
<th style="text-align: left;">Llama</th>
<th style="text-align: left;">备注</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">编码速度</td>
<td style="text-align: left;">~10k tokens/s</td>
<td style="text-align: left;">~8k tokens/s</td>
<td style="text-align: left;">~12k tokens/s</td>
<td style="text-align: left;">单线程</td>
</tr>
<tr class="even">
<td style="text-align: left;">解码速度</td>
<td style="text-align: left;">~15k tokens/s</td>
<td style="text-align: left;">~12k tokens/s</td>
<td style="text-align: left;">~18k tokens/s</td>
<td style="text-align: left;">单线程</td>
</tr>
<tr class="odd">
<td style="text-align: left;">内存占用</td>
<td style="text-align: left;">~200MB</td>
<td style="text-align: left;">~150MB</td>
<td style="text-align: left;">~100MB</td>
<td style="text-align: left;">模型文件</td>
</tr>
<tr class="even">
<td style="text-align: left;">加载时间</td>
<td style="text-align: left;">~0.5s</td>
<td style="text-align: left;">~0.3s</td>
<td style="text-align: left;">~0.2s</td>
<td style="text-align: left;">首次加载</td>
</tr>
</tbody>
</table>
<p>希望这份详细的Tokenization笔记能帮助你深入理解大模型的第一步！</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/linux/" class="category-chain-item">linux</a>
  
  
    <span>></span>
    
  <a href="/categories/linux/drivers/" class="category-chain-item">drivers</a>
  
  
    <span>></span>
    
  <a href="/categories/linux/drivers/gpu/" class="category-chain-item">gpu</a>
  
  
    <span>></span>
    
  <a href="/categories/linux/drivers/gpu/stanford-cs336/" class="category-chain-item">stanford-cs336</a>
  
  

  

  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/git/">#git</a>
      
        <a href="/tags/architect/">#architect</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>大模型从0到1｜第一讲：概述和Tokenization</div>
      <div>https://realwujing.github.io/linux/drivers/gpu/stanford-cs336/大模型从0到1｜第一讲：概述和tokenization/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Wu Jing</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年11月22日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/linux/drivers/gpu/stanford-cs336/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BB%8E0%E5%88%B01%EF%BD%9C%E7%AC%AC%E4%BA%8C%E8%AE%B2%EF%BC%9Apytorch%E6%89%8B%E6%8A%8A%E6%89%8B%E6%90%AD%E5%BB%BALLM/" title="大模型从0到1｜第二讲：PyTorch手把手搭建LLM">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">大模型从0到1｜第二讲：PyTorch手把手搭建LLM</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/linux/drivers/%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8/" title="设备驱动">
                        <span class="hidden-mobile">设备驱动</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="gitalk-container"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#gitalk-container', function() {
      Fluid.utils.createCssLink('/css/gitalk.css')
      Fluid.utils.createScript('https://lib.baomitu.com/gitalk/1.8.0/gitalk.min.js', function() {
        var options = Object.assign(
          {"clientID":"c11f8471a6ae4d3eea12","clientSecret":"87bfa232882af2b005f4c3352132dd418bf6d113","repo":"realwujing.github.io","owner":"realwujing","admin":["realwujing"],"language":"zh-CN","labels":["Gitalk"],"perPage":10,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":false,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token"},
          {
            id: '0eb7c58142e7784f514b389194a075cd'
          }
        )
        var gitalk = new Gitalk(options);
        gitalk.render('gitalk-container');
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.events.registerRefreshCallback(function() {
      if ('mermaid' in window) {
        mermaid.init();
      }
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
