

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicon.jpg">
  <link rel="icon" href="/images/favicon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Wu Jing">
  <meta name="keywords" content="3d, Linux-0.11-yuan-xy, ThreadPool, acpi, algorithm, architect, assembly, baohua, binary-analysis, books, boot, bpf, bugs, ceph, cmake-objdump, console, container, cpp, dbus, deb, debug, deepin, develop, disk, distro, docker, drivers, extern, fluid, fs, gdb, git, go, gpu, grtrace, grub, hello_world, hexo, irq, java, javascript, jenkins, k8s, kdump, kernel, kickstart, kms, kvm, linux, linux-0.11-debug, log, ltp, markdown, minifs, mm, modules, monitoring, mutex, my-project, net, nginx, patent, perf, perf-event, performance, phytium, pkg, port-forward, power, proc, protobuf, protobuf_example, proxy, pthread, pulseaudio, python, qemu, qprocess_wget, qt-learning, rcu, redis, rpm-ostree, runoob-vue3-test, security, shell, sound, sources, stanford-cs336, stap, struct, svn, sync, sysrq_trigger, task, testing, thread, tick, todolist, tools, udl, unixbench, uts_namespace, valgrind, vim, virsh, virt, xisai">
  
    <meta name="description" content="大模型从0到1｜第三讲：详解现代LLM基础架构课程信息：CS336 | 讲师：Tatsu H | 幻灯片总数：68  Part 1: 课程概览 (Introduction)Page 1: 标题页  内容: 课程名称 “Lecture 3: Everything you didn’t want to know about LM architecture and training”。 解析: 这一讲主">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型从0到1｜第三讲：详解现代LLM基础架构">
<meta property="og:url" content="https://realwujing.github.io/linux/drivers/gpu/stanford-cs336/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BB%8E0%E5%88%B01%EF%BD%9C%E7%AC%AC%E4%B8%89%E8%AE%B2%EF%BC%9A%E8%AF%A6%E8%A7%A3%E7%8E%B0%E4%BB%A3LLM%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84/index.html">
<meta property="og:site_name" content="WuJing&#39;s Blog">
<meta property="og:description" content="大模型从0到1｜第三讲：详解现代LLM基础架构课程信息：CS336 | 讲师：Tatsu H | 幻灯片总数：68  Part 1: 课程概览 (Introduction)Page 1: 标题页  内容: 课程名称 “Lecture 3: Everything you didn’t want to know about LM architecture and training”。 解析: 这一讲主">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_01.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_02.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_03.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_04.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_05.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_06.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_07.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_08.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_09.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_10.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_11.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_12.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_13.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_14.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_15.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_16.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_17.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_18.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_19.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_20.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_21.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_22.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_23.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_24.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_25.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_26.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_27.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_28.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_29.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_30.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_31.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_32.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_33.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_34.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_35.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_36.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_37.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_38.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_39.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_40.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_41.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_42.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_43.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_44.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_45.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_46.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_47.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_48.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_49.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_50.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_51.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_52.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_53.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_54.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_55.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_56.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_57.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_58.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_59.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_60.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_61.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_62.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_63.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_64.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_65.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_66.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_67.png">
<meta property="og:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_68.png">
<meta property="article:published_time" content="2025-11-23T18:43:38.000Z">
<meta property="article:modified_time" content="2025-11-24T20:54:15.000Z">
<meta property="article:author" content="Wu Jing">
<meta property="article:tag" content="architect">
<meta property="article:tag" content="drivers">
<meta property="article:tag" content="git">
<meta property="article:tag" content="go">
<meta property="article:tag" content="gpu">
<meta property="article:tag" content="linux">
<meta property="article:tag" content="log">
<meta property="article:tag" content="mm">
<meta property="article:tag" content="stanford-cs336">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://realwujing.github.io/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_01.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>大模型从0到1｜第三讲：详解现代LLM基础架构 - WuJing&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="/css/custom.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"realwujing.github.io","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"6b5123e146041483d13bdfaeb6e42a76","google":"UA-265632133-1","gtag":"G-E7BV6T4RCW","tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?6b5123e146041483d13bdfaeb6e42a76";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  
    <!-- Google Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript('https://www.google-analytics.com/analytics.js', function() {
          window.ga = window.ga || function() { (ga.q = ga.q || []).push(arguments) };
          ga.l = +new Date;
          ga('create', 'UA-265632133-1', 'auto');
          ga('send', 'pageview');
        });
      }
    </script>
  

  
    <!-- Google gtag.js -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript('https://www.googletagmanager.com/gtag/js?id=G-E7BV6T4RCW', function() {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-E7BV6T4RCW');
        });
      }
    </script>
  

  

  

  

  



  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>WuJing&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="大模型从0到1｜第三讲：详解现代LLM基础架构"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-11-23 18:43" pubdate>
          2025年11月23日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.3k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          61 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">大模型从0到1｜第三讲：详解现代LLM基础架构</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：2025年11月24日 晚上
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <h1 id="大模型从0到1｜第三讲：详解现代LLM基础架构"><a href="#大模型从0到1｜第三讲：详解现代LLM基础架构" class="headerlink" title="大模型从0到1｜第三讲：详解现代LLM基础架构"></a>大模型从0到1｜第三讲：详解现代LLM基础架构</h1><p><strong>课程信息</strong>：CS336 | <strong>讲师</strong>：Tatsu H | <strong>幻灯片总数</strong>：68</p>
<hr>
<h2 id="Part-1-课程概览-Introduction"><a href="#Part-1-课程概览-Introduction" class="headerlink" title="Part 1: 课程概览 (Introduction)"></a>Part 1: 课程概览 (Introduction)</h2><p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_01.png" srcset="/img/loading.gif" lazyload><br><strong>Page 1: 标题页</strong></p>
<ul>
<li><strong>内容:</strong> 课程名称 “Lecture 3: Everything you didn’t want to know about LM architecture and training”。</li>
<li><strong>解析:</strong> 这一讲主要关注那些我们在调用 API 时看不到，但自己训练模型时必须决定的“肮脏细节”。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_02.png" srcset="/img/loading.gif" lazyload><br><strong>Page 2: 课程目标 (Outline &amp; Goals)</strong></p>
<ul>
<li><strong>内容:</strong><ol>
<li>回顾标准 Transformer。</li>
<li>分析主流大模型（Big LMs）的架构共性。</li>
<li>探讨常见的架构变体（Variations）及其取舍。</li>
</ol>
</li>
<li><strong>核心:</strong> “Today’s theme: the best way to learn is hands-on experience… try to learn from others.”（从前人的实验中总结最佳实践）。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_03.png" srcset="/img/loading.gif" lazyload><br><strong>Page 3: 后勤通知 (Logistics)</strong></p>
<ul>
<li><strong>内容:</strong> 加入 Slack，检查作业版本等行政事项。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_04.png" srcset="/img/loading.gif" lazyload><br><strong>Page 4: 技术大纲</strong></p>
<ul>
<li><strong>内容:</strong> <ul>
<li><strong>架构变体:</strong> 激活函数、FFN 设计、Attention 变体、位置编码。</li>
<li><strong>超参数:</strong> 哪些参数重要（Scale）？哪些不重要？</li>
<li><strong>稳定性:</strong> 如何防止训练崩溃（Stability tricks）。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="Part-2-架构演进与共性-The-Landscape"><a href="#Part-2-架构演进与共性-The-Landscape" class="headerlink" title="Part 2: 架构演进与共性 (The Landscape)"></a>Part 2: 架构演进与共性 (The Landscape)</h2><p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_05.png" srcset="/img/loading.gif" lazyload><br><strong>Page 5: 原始 Transformer (The Original)</strong></p>
<ul>
<li><strong>内容:</strong> 经典的 Attention is All You Need 架构图。</li>
<li><strong>关键特征:</strong> <ul>
<li><strong>Post-Norm:</strong> Add &amp; Norm 在子层之后。</li>
<li><strong>Sinusoidal:</strong> 正弦位置编码。</li>
<li><strong>ReLU:</strong> 激活函数。</li>
<li><strong>Bias:</strong> 全连接层带有偏置项。</li>
</ul>
</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_06.png" srcset="/img/loading.gif" lazyload><br><strong>Page 6: 现代架构的“标准答案” (Modern Variant)</strong></p>
<ul>
<li><strong>内容:</strong> 这就是你们在作业中要实现的架构（类似 Llama）。</li>
<li><strong>改进点:</strong><ul>
<li><strong>Pre-Norm:</strong> Norm 放在子层之前。</li>
<li><strong>RMSNorm:</strong> 替代 LayerNorm。</li>
<li><strong>RoPE:</strong> 旋转位置编码。</li>
<li><strong>SwiGLU:</strong> 替代 ReLU。</li>
<li><strong>No Bias:</strong> 去掉 Bias。</li>
</ul>
</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_07.png" srcset="/img/loading.gif" lazyload><br><strong>Page 7: 模型大爆发</strong></p>
<ul>
<li><strong>内容:</strong> 列举了近年来发布的各种模型（Llama 3, Nemotron, Qwen 1.5, Gemma 等）。</li>
<li><strong>解析:</strong> 虽然名字不同，但它们在架构上惊人地相似，大多是对 Page 6 所述架构的微调。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_08.png" srcset="/img/loading.gif" lazyload><br><strong>Page 8: 架构统计大表 (The Data)</strong></p>
<ul>
<li><strong>内容:</strong> 一个详细的表格，横轴是模型（GPT-3, PaLM, Llama, Chinchilla 等），纵轴是配置（Norm 类型, Pos Emb, Activation）。</li>
<li><strong>趋势:</strong> 可以明显看到从早期的混乱（各种尝试）到后期的统一（Pre-norm, RoPE, SwiGLU 占主导）。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_09.png" srcset="/img/loading.gif" lazyload><br><strong>Page 9: 架构共性总结</strong></p>
<ul>
<li><strong>内容:</strong> High level view。<ul>
<li><strong>Pre-norm:</strong> 现在的绝对主流。</li>
<li><strong>RMSNorm:</strong> Llama 系列, PaLM, Gopher 使用。</li>
<li><strong>SwiGLU:</strong> Llama 系列, PaLM, Mistral 使用。</li>
<li><strong>RoPE:</strong> 几乎所有现代模型（除了 ALiBi 的少数拥护者）都使用。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="Part-3-归一化层详解-Normalization"><a href="#Part-3-归一化层详解-Normalization" class="headerlink" title="Part 3: 归一化层详解 (Normalization)"></a>Part 3: 归一化层详解 (Normalization)</h2><p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_10.png" srcset="/img/loading.gif" lazyload><br><strong>Page 10: Pre-Norm vs Post-Norm 结构图</strong></p>
<ul>
<li><strong>左图 (Post-LN):</strong> $x &#x3D; Norm(x + f(x))$。BERT 时代的主流。</li>
<li><strong>右图 (Pre-LN):</strong> $x &#x3D; x + f(Norm(x))$。GPT-2 之后的主流。</li>
<li><strong>区别:</strong> Pre-LN 保证了残差主干（Identity path）的纯净，梯度可以直接流到底。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_11.png" srcset="/img/loading.gif" lazyload><br><strong>Page 11: 为什么选 Pre-Norm? (原理)</strong></p>
<ul>
<li><strong>内容:</strong> 引用 Xiong 2020。</li>
<li><strong>核心:</strong> Pre-LN 使得深层网络的梯度范数（Gradient Norm）更稳定。它消除了梯度爆炸&#x2F;消失的风险，使得我们可以使用更大的学习率，并且不需要很长时间的 Warmup。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_12.png" srcset="/img/loading.gif" lazyload><br><strong>Page 12: Pre&#x2F;Post Norm 的实验对比</strong></p>
<ul>
<li><strong>内容:</strong> 训练曲线图 (IWSLT)。</li>
<li><strong>结论:</strong> Post-LN（蓝线）如果没有精心设计的 Warmup 很容易炸或者是收敛慢；Pre-LN（红&#x2F;绿线）则非常稳健，收敛更快。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_13.png" srcset="/img/loading.gif" lazyload><br><strong>Page 13: 梯度范数可视化</strong></p>
<ul>
<li><strong>内容:</strong> 图表展示了不同层级的梯度大小。</li>
<li><strong>解析:</strong> Post-LN 的梯度在反向传播时，到底层会变得非常小（梯度消失）；Pre-LN 的梯度在所有层级保持一致。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_14.png" srcset="/img/loading.gif" lazyload><br><strong>Page 14: 新趋势 - Double Norm?</strong></p>
<ul>
<li><strong>内容:</strong> 提到了一些最新的模型（如 Grok, Gemma 2）。</li>
<li><strong>做法:</strong> 它们在 Pre-Norm 的基础上，又在残差块的输出位置加了一个 Norm。这是一种“双保险”，为了在大规模训练时获得极致的稳定性。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_15.png" srcset="/img/loading.gif" lazyload><br><strong>Page 15: LayerNorm 回顾</strong></p>
<ul>
<li><strong>内容:</strong> 标准 LayerNorm 的公式。<ul>
<li>需要计算 Mean $\mu$ 和 Variance $\sigma^2$。</li>
<li>操作：Centering (减均值) + Scaling (除标准差)。</li>
</ul>
</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_16.png" srcset="/img/loading.gif" lazyload><br><strong>Page 16: RMSNorm (Root Mean Square Norm)</strong></p>
<ul>
<li><strong>内容:</strong> 公式：$y &#x3D; \frac{x}{\text{RMS}(x)} * \gamma$。</li>
<li><strong>核心:</strong> <strong>去掉了减均值的操作</strong>。只做缩放（Rescaling），不做中心化（Re-centering）。</li>
<li><strong>使用者:</strong> LLaMA, PaLM, T5。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_17.png" srcset="/img/loading.gif" lazyload><br><strong>Page 17: 为什么 RMSNorm 有效？</strong></p>
<ul>
<li><strong>内容:</strong> 引用 Bjorck et al 2018。</li>
<li><strong>解释:</strong> 实验发现 LayerNorm 的成功主要归功于 Scaling（缩放不变性），Mean（平移不变性）其实没啥用。既然没用，为了速度就干掉它。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_18.png" srcset="/img/loading.gif" lazyload><br><strong>Page 18: RMSNorm 的速度优势</strong></p>
<ul>
<li><strong>内容:</strong> 性能对比表。</li>
<li><strong>结论:</strong> 少了均值计算，RMSNorm 在 GPU 上通常能带来 <strong>10% - 40%</strong> 的速度提升（取决于实现细节）。这是大家在这个时代选择它的主要原因——<strong>快</strong>。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_19.png" srcset="/img/loading.gif" lazyload><br><strong>Page 19: 移除偏置项 (No Bias)</strong></p>
<ul>
<li><strong>内容:</strong> 许多现代模型（PaLM, Llama）移除了所有 Linear 层和 Norm 层的 Bias 参数。</li>
<li><strong>原因:</strong><ol>
<li>对性能（PPL）几乎无影响。</li>
<li>提升训练稳定性（Stability）。</li>
<li>稍微减少一点显存和通信量。</li>
</ol>
</li>
</ul>
<hr>
<h2 id="Part-4-激活函数-Activations"><a href="#Part-4-激活函数-Activations" class="headerlink" title="Part 4: 激活函数 (Activations)"></a>Part 4: 激活函数 (Activations)</h2><p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_20.png" srcset="/img/loading.gif" lazyload><br><strong>Page 20: 激活函数概览</strong></p>
<ul>
<li><strong>内容:</strong> 标题页。我们将看到从 ReLU 到 SwiGLU 的演进。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_21.png" srcset="/img/loading.gif" lazyload><br><strong>Page 21: 经典激活函数</strong></p>
<ul>
<li><strong>ReLU:</strong> $max(0, x)$。简单，稀疏。</li>
<li><strong>GeLU:</strong> $x \Phi(x)$。BERT&#x2F;GPT 使用。平滑，非线性更强。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_22.png" srcset="/img/loading.gif" lazyload><br><strong>Page 22: GLU (Gated Linear Units) 家族</strong></p>
<ul>
<li><strong>内容:</strong> 介绍“门控”机制。<ul>
<li>普通 FFN: $Act(xW_1)W_2$</li>
<li>GLU FFN: $(Act(xW_1) \odot xV_1) W_2$</li>
</ul>
</li>
<li><strong>解析:</strong> 引入了额外的投影层 $V_1$ 作为“门”，通过逐元素乘法控制信息流。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_23.png" srcset="/img/loading.gif" lazyload><br><strong>Page 23: GeGLU</strong></p>
<ul>
<li><strong>内容:</strong> 激活函数用 GeLU 的 GLU。<ul>
<li>公式: $\text{GeLU}(xW) \odot (xV) W_2$</li>
<li>代表: PaLM, T5。</li>
</ul>
</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_24.png" srcset="/img/loading.gif" lazyload><br><strong>Page 24: SwiGLU (The Winner)</strong></p>
<ul>
<li><strong>内容:</strong> 激活函数用 Swish 的 GLU。<ul>
<li>公式: $\text{Swish}(xW) \odot (xV) W_2$</li>
<li>代表: <strong>LLaMA</strong>, Mistral。</li>
</ul>
</li>
<li><strong>注意:</strong> 因为多了 $V$ 矩阵，为了参数量公平对比，通常会把 FFN 的中间维度从 $4d$ 降到 $\frac{8}{3}d$ (约 2.67d)。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_25.png" srcset="/img/loading.gif" lazyload><br><strong>Page 25: GLU 真的更好吗？</strong></p>
<ul>
<li><strong>内容:</strong> 引用 Shazeer 2020 论文图表。</li>
<li><strong>结论:</strong> 在相同 FLOPs 下，GLU 变体（ReGLU, SwiGLU）始终优于非门控版本（ReLU, GeLU）。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_26.png" srcset="/img/loading.gif" lazyload><br><strong>Page 26: 更多实验证据</strong></p>
<ul>
<li><strong>内容:</strong> 另一组 PPL 对比。SwiGLU 表现最佳。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_27.png" srcset="/img/loading.gif" lazyload><br><strong>Page 27: 激活函数总结</strong></p>
<ul>
<li><strong>结论:</strong> SwiGLU 提供了最佳的性能&#x2F;计算比。虽然计算稍微繁琐（多一次矩阵乘），但效果值得。目前是 Llama-like 架构的标配。</li>
</ul>
<hr>
<h2 id="Part-5-并行层与位置编码-Parallel-Layers-Pos-Embs"><a href="#Part-5-并行层与位置编码-Parallel-Layers-Pos-Embs" class="headerlink" title="Part 5: 并行层与位置编码 (Parallel Layers &amp; Pos Embs)"></a>Part 5: 并行层与位置编码 (Parallel Layers &amp; Pos Embs)</h2><p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_28.png" srcset="/img/loading.gif" lazyload><br><strong>Page 28: 串行 (Serial) vs 并行 (Parallel) 块</strong></p>
<ul>
<li><strong>Serial (标准):</strong> 先算 Attention，加残差，Norm，再算 MLP，加残差。</li>
<li><strong>Parallel:</strong> Attention 和 MLP <strong>同时</strong>计算（共享同一个 Norm 后的输入），然后把它们的结果一起加到残差上。<ul>
<li>$x_{out} &#x3D; x + Attn(LN(x)) + MLP(LN(x))$</li>
</ul>
</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_29.png" srcset="/img/loading.gif" lazyload><br><strong>Page 29: 并行层的权衡</strong></p>
<ul>
<li><strong>优点:</strong> 训练速度快约 **15%**。因为矩阵乘法可以融合，通信可以合并。PaLM 和 GPT-J 采用了这种设计。</li>
<li><strong>缺点:</strong> 性能略微下降（Worse Quality）。</li>
<li><strong>现状:</strong> 大多数追求极致性能的模型（Llama）依然使用<strong>串行</strong>架构。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_30.png" srcset="/img/loading.gif" lazyload><br><strong>Page 30: 架构小结 (Summary so far)</strong></p>
<ul>
<li><strong>必选:</strong> Pre-norm (RMSNorm)。</li>
<li><strong>首选:</strong> SwiGLU。</li>
<li><strong>可选:</strong> Parallel Layers（为了速度），No Bias（为了稳定）。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_31.png" srcset="/img/loading.gif" lazyload><br><strong>Page 31: 位置编码的演变</strong></p>
<ul>
<li><strong>绝对位置:</strong> Sine&#x2F;Cosine (Transformer), Learned (GPT-3)。</li>
<li><strong>相对位置:</strong> T5, ALiBi。</li>
<li><strong>旋转位置 (RoPE):</strong> LLaMA, PaLM。现在是 RoPE 的天下。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_32.png" srcset="/img/loading.gif" lazyload><br><strong>Page 32: RoPE 的核心理念</strong></p>
<ul>
<li><strong>目标:</strong> 我们希望 Attention 机制能自然地捕捉 Token 之间的<strong>相对距离</strong> $(m-n)$，而不是关心它们的绝对位置 $m, n$。</li>
<li><strong>方法:</strong> 寻找一种变换，使得 $\langle f(q, m), f(k, n) \rangle &#x3D; g(q, k, m-n)$。RoPE 通过复数旋转完美解决了这个问题。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_33.png" srcset="/img/loading.gif" lazyload><br><strong>Page 33: RoPE 的直观图示</strong></p>
<ul>
<li><strong>视觉:</strong> 二维平面上的向量旋转。</li>
<li><strong>原理解析:</strong> 将 Embedding 向量两两分组，视为复数。根据该 Token 的位置 $m$，将复数向量旋转 $m\theta$ 角度。两个向量点积时，角度相减，自然得到相对位置信息 $(m-n)\theta$。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_34.png" srcset="/img/loading.gif" lazyload><br><strong>Page 34: RoPE 数学形式</strong></p>
<ul>
<li><strong>内容:</strong> 旋转矩阵 $R_{\Theta}$ 是块对角的。</li>
<li><strong>特点:</strong> 这种稀疏结构意味着我们可以快速计算旋转，不需要进行庞大的矩阵乘法。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_35.png" srcset="/img/loading.gif" lazyload><br><strong>Page 35: RoPE 代码与外推</strong></p>
<ul>
<li><strong>实现:</strong> 在每一层计算 Attention Score 之前，即时对 Q 和 K 进行旋转。</li>
<li><strong>外推性 (Extrapolation):</strong> RoPE 在处理比训练长度更长的序列时，表现优于绝对位置编码，但仍有衰减。</li>
</ul>
<hr>
<h2 id="Part-6-超参数调优-Hyperparameters"><a href="#Part-6-超参数调优-Hyperparameters" class="headerlink" title="Part 6: 超参数调优 (Hyperparameters)"></a>Part 6: 超参数调优 (Hyperparameters)</h2><p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_36.png" srcset="/img/loading.gif" lazyload><br><strong>Page 36: 哪些参数重要？</strong></p>
<ul>
<li>FFN 维度？Head 数量？Head 大小？词表大小？</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_37.png" srcset="/img/loading.gif" lazyload><br><strong>Page 37: FFN 膨胀系数</strong></p>
<ul>
<li><strong>标准:</strong> $d_{ff} &#x3D; 4 d_{model}$。</li>
<li><strong>SwiGLU:</strong> 由于参数变多了，通常设为 $\frac{8}{3} d_{model} \approx 2.66 d_{model}$，以保持总参数量与标准版一致。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_38.png" srcset="/img/loading.gif" lazyload><br><strong>Page 38: Head Dimension (头维度)</strong></p>
<ul>
<li><strong>黄金标准:</strong> <strong>128</strong>。</li>
<li><strong>观察:</strong> 无论是 7B 的 Llama 还是 175B 的 GPT-3，大家都在用 <code>head_dim=128</code>。</li>
<li><strong>原因:</strong> 硬件亲和性。128 完美契合 GPU Tensor Core 的计算分块大小，效率最高。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_39.png" srcset="/img/loading.gif" lazyload><br><strong>Page 39: 为什么是 1-1 Ratio?</strong></p>
<ul>
<li><strong>内容:</strong> 引用 Bhojanapalli et al 2020。</li>
<li><strong>结论:</strong> 保持 <code>n_heads * head_dim = d_model</code> 是最佳实践。虽然理论上可以解耦，但没什么好处。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_40.png" srcset="/img/loading.gif" lazyload><br><strong>Page 40: 深宽比 (Aspect Ratio)</strong></p>
<ul>
<li><strong>内容:</strong> 模型该深（Deep）还是该宽（Wide）？</li>
<li><strong>结论:</strong> 在很大范围内，性能对形状不敏感。只要参数量够了就行。</li>
<li><strong>Sweet Spot:</strong> 通常 $d_{model} \approx 128 \times n_{layers}$。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_41.png" srcset="/img/loading.gif" lazyload><br><strong>Page 41: 深度限制</strong></p>
<ul>
<li><strong>提示:</strong> 不要让模型太深（例如超过 100 层）。太深会导致推理延迟高，且流水线并行（Pipeline Parallelism）更难切分。稍微宽一点对系统更友好。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_42.png" srcset="/img/loading.gif" lazyload><br><strong>Page 42: 词表大小 (Vocab Size)</strong></p>
<ul>
<li><strong>趋势:</strong> <strong>变大</strong>。<ul>
<li>GPT-2: 50k</li>
<li>Llama 2: 32k</li>
<li>Llama 3: <strong>128k</strong></li>
<li>Qwen: 150k</li>
</ul>
</li>
<li><strong>原因:</strong> 大词表 &#x3D; 压缩率高 &#x3D; 同样 2048 长度包含更多信息 &#x3D; 推理更快。虽然 Embedding 层参数多了，但值得。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_43.png" srcset="/img/loading.gif" lazyload><br><strong>Page 43: 正则化 (Regularization)</strong></p>
<ul>
<li><strong>问题:</strong> 这么多数据，还需要防过拟合吗？</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_44.png" srcset="/img/loading.gif" lazyload><br><strong>Page 44: Dropout 的消失</strong></p>
<ul>
<li><strong>内容:</strong> <strong>预训练期间 Dropout &#x3D; 0</strong>。</li>
<li><strong>原因:</strong> 我们的数据量太大了（Trillions tokens），模型根本记不住，不存在过拟合。Dropout 只会浪费计算资源并减慢收敛。Llama, PaLM 全都把 Dropout 关了。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_45.png" srcset="/img/loading.gif" lazyload><br><strong>Page 45: Weight Decay 依然存在</strong></p>
<ul>
<li><strong>内容:</strong> <strong>Weight Decay &#x3D; 0.1</strong>。</li>
<li><strong>原因:</strong> 它不仅是正则化，更重要的是它影响**优化动态 (Optimization Dynamics)**。有 Weight Decay，SGD&#x2F;Adam 才能收敛到更平滑的极小值点。这个不能关。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_46.png" srcset="/img/loading.gif" lazyload><br><strong>Page 46: 超参数总结</strong></p>
<ul>
<li>FFN Mult: 2.67 (SwiGLU)</li>
<li>Head Dim: 128</li>
<li>Dropout: 0</li>
<li>Weight Decay: 0.1</li>
</ul>
<hr>
<h2 id="Part-7-训练稳定性与推理优化-Stability-Inference"><a href="#Part-7-训练稳定性与推理优化-Stability-Inference" class="headerlink" title="Part 7: 训练稳定性与推理优化 (Stability &amp; Inference)"></a>Part 7: 训练稳定性与推理优化 (Stability &amp; Inference)</h2><p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_47.png" srcset="/img/loading.gif" lazyload><br><strong>Page 47: 稳定性技巧 (Stability Tricks)</strong></p>
<ul>
<li><strong>背景:</strong> 训练大模型最怕遇到 <strong>Loss Spike</strong>（Loss 突然暴涨），这通常意味着你要回滚几十个小时的 Checkpoint，甚至从头再来。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_48.png" srcset="/img/loading.gif" lazyload><br><strong>Page 48: 元凶 - Softmax 溢出</strong></p>
<ul>
<li><strong>内容:</strong> Softmax 里的 <code>exp(x)</code> 对大数值非常敏感。如果 Logits 稍微大一点，<code>exp</code> 就会溢出 FP16 的范围（NaN），导致训练崩溃。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_49.png" srcset="/img/loading.gif" lazyload><br><strong>Page 49: Trick 1 - z-loss</strong></p>
<ul>
<li><strong>来源:</strong> PaLM 论文。</li>
<li><strong>方法:</strong> 加一个辅助 Loss：$L_{aux} &#x3D; 10^{-4} \log^2 Z$ ($Z$是Softmax分母)。</li>
<li><strong>作用:</strong> 惩罚过大的 Logits 总和，强制 Softmax 的 Logits 保持在 0 附近，防止爆炸。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_50.png" srcset="/img/loading.gif" lazyload><br><strong>Page 50: Trick 2 - QK Norm</strong></p>
<ul>
<li><strong>来源:</strong> ViT-22B, Gilmer et al.</li>
<li><strong>方法:</strong> 在计算 $Q \cdot K^T$ 之前，先对 Query 和 Key 向量做 LayerNorm。</li>
<li><strong>作用:</strong> 无论 Embedding 维度多大，强制 Q 和 K 的尺度保持不变，从而稳定 Attention Score。DCLM 和 Gemma 2 都在用。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_51.png" srcset="/img/loading.gif" lazyload><br><strong>Page 51: Trick 3 - Logit Soft-capping</strong></p>
<ul>
<li><strong>来源:</strong> Gemma 2。</li>
<li><strong>方法:</strong> $logits &#x3D; C \cdot \tanh(logits &#x2F; C)$。</li>
<li><strong>作用:</strong> 用 <code>tanh</code> 物理截断 Logits，使其永远不会超过 $C$（例如 50）。这是一种硬约束，彻底消除了溢出可能。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_52.png" srcset="/img/loading.gif" lazyload><br><strong>Page 52: 推理优化 - Attention 瓶颈</strong></p>
<ul>
<li><strong>问题:</strong> 推理时，KV Cache 占用大量显存，且搬运 KV Cache 是 IO 瓶颈。MHA (Multi-Head Attention) 每一层都要加载巨大的 KV 矩阵。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_53.png" srcset="/img/loading.gif" lazyload><br><strong>Page 53: MQA (Multi-Query Attention)</strong></p>
<ul>
<li><strong>机制:</strong> 整个模型的所有 Query Heads 共享<strong>同一对</strong> K 和 V Head。</li>
<li><strong>优势:</strong> KV Cache 大小减少了 $N_{head}$ 倍。推理飞快。</li>
<li><strong>劣势:</strong> 模型表达能力受损，质量下降。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_54.png" srcset="/img/loading.gif" lazyload><br><strong>Page 54: GQA (Grouped-Query Attention)</strong></p>
<ul>
<li><strong>机制:</strong> 折中方案。将 Query Heads 分组（例如 8 组），每组共享一对 K&#x2F;V Head。</li>
<li><strong>优势:</strong> 速度接近 MQA，质量接近 MHA。</li>
<li><strong>地位:</strong> <strong>Llama 2&#x2F;3, Mistral 的标配</strong>。现在基本上没人用标准 MHA 了。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_55.png" srcset="/img/loading.gif" lazyload><br><strong>Page 55: MQA vs GQA 性能图</strong></p>
<ul>
<li><strong>内容:</strong> 图表显示 GQA 在保持高吞吐量的同时，PPL 损失极小。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_56.png" srcset="/img/loading.gif" lazyload><br><strong>Page 56: SWA (Sliding Window Attention)</strong></p>
<ul>
<li><strong>机制:</strong> 强行限制 Attention 只看最近的 $W$ 个 Token（如 4096）。</li>
<li><strong>代表:</strong> Mistral 7B。</li>
<li><strong>作用:</strong> 节省计算量，变成 $O(N)$ 复杂度。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_57.png" srcset="/img/loading.gif" lazyload><br><strong>Page 57: 混合 Attention (Interleaving)</strong></p>
<ul>
<li><strong>机制:</strong> Gemma 2 的做法。例如：层 1 用 SWA（局部），层 2 用 Full Attention（全局），交替进行。</li>
<li><strong>作用:</strong> 既节省了计算，又保留了长距离建模能力。</li>
</ul>
<hr>
<h2 id="Part-8-总结-Conclusion"><a href="#Part-8-总结-Conclusion" class="headerlink" title="Part 8: 总结 (Conclusion)"></a>Part 8: 总结 (Conclusion)</h2><p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_58.png" srcset="/img/loading.gif" lazyload><br><strong>Page 58: 架构收敛 (Convergence)</strong></p>
<ul>
<li><strong>核心结论:</strong> 2024-2025 年的 LLM 架构已经高度趋同，被称为 <strong>“Llama-like”</strong> 架构：<ul>
<li><strong>Pre-RMSNorm</strong></li>
<li><strong>SwiGLU</strong></li>
<li><strong>RoPE</strong></li>
<li><strong>GQA</strong></li>
<li><strong>No Bias</strong></li>
</ul>
</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_59.png" srcset="/img/loading.gif" lazyload><br><strong>Page 59: 为什么收敛到这套配置？</strong></p>
<ul>
<li><strong>Pre-norm:</strong> 为了梯度流（训练深层模型）。</li>
<li><strong>RMSNorm&#x2F;SwiGLU:</strong> 为了计算效率和微小的性能增益。</li>
<li><strong>RoPE:</strong> 为了位置外推性。</li>
<li><strong>GQA:</strong> 为了推理成本。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_60.png" srcset="/img/loading.gif" lazyload><br><strong>Page 60: 警惕盲从 (Cargo Culting)</strong></p>
<ul>
<li><strong>警告:</strong> 虽然我们要学习最佳实践，但不要盲目复制。很多配置（如 128 head dim）可能是因为硬件原因，也可能只是习惯，未必是科学上的最优解。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_61.png" srcset="/img/loading.gif" lazyload><br><strong>Page 61: 没讲到的内容</strong></p>
<ul>
<li><strong>Encoder-Decoder (T5):</strong> 现在的 LLM 几乎全是 Decoder-only。</li>
<li><strong>MoE:</strong> 下节课重点讲。</li>
<li><strong>SSM&#x2F;Linear Attn:</strong> 很有趣，但还未动摇 Transformer 的统治地位。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_62.png" srcset="/img/loading.gif" lazyload><br><strong>Page 62: 总结</strong></p>
<ul>
<li>架构设计是<strong>系统工程</strong>（考虑硬件效率）、<strong>优化理论</strong>（考虑梯度流）和<strong>经验主义</strong>（看谁跑分高）的结合。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_63.png" srcset="/img/loading.gif" lazyload><br><strong>Page 63: 推荐阅读</strong></p>
<ul>
<li>Llama 1&#x2F;2&#x2F;3 论文、PaLM 论文、Gopher 论文。重点读 Appendix 中的实验细节。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_64.png" srcset="/img/loading.gif" lazyload><br><strong>Page 64: 作业预告</strong></p>
<ul>
<li>实现一个带有 RoPE 和 RMSNorm 的现代 Transformer。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_65.png" srcset="/img/loading.gif" lazyload><br><strong>Page 65: 下节课预告</strong></p>
<ul>
<li>Lecture 4: **Mixture of Experts (MoE)**。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_66.png" srcset="/img/loading.gif" lazyload><br><strong>Page 66: Q&amp;A</strong></p>
<ul>
<li>问答环节。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_67.png" srcset="/img/loading.gif" lazyload><br><strong>Page 67: 参考文献 1</strong></p>
<ul>
<li>论文列表。</li>
</ul>
<p><img src="/images/linux/drivers/gpu/stanford-cs336/arch/2025%20Lecture%203%20-%20architecture_68.png" srcset="/img/loading.gif" lazyload><br><strong>Page 68: 参考文献 2 &#x2F; 结束</strong></p>
<ul>
<li>论文列表。课程结束。</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/linux/" class="category-chain-item">linux</a>
  
  
    <span>></span>
    
  <a href="/categories/linux/drivers/" class="category-chain-item">drivers</a>
  
  
    <span>></span>
    
  <a href="/categories/linux/drivers/gpu/" class="category-chain-item">gpu</a>
  
  
    <span>></span>
    
  <a href="/categories/linux/drivers/gpu/stanford-cs336/" class="category-chain-item">stanford-cs336</a>
  
  

  

  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/architect/">#architect</a>
      
        <a href="/tags/drivers/">#drivers</a>
      
        <a href="/tags/git/">#git</a>
      
        <a href="/tags/go/">#go</a>
      
        <a href="/tags/gpu/">#gpu</a>
      
        <a href="/tags/linux/">#linux</a>
      
        <a href="/tags/log/">#log</a>
      
        <a href="/tags/mm/">#mm</a>
      
        <a href="/tags/stanford-cs336/">#stanford-cs336</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>大模型从0到1｜第三讲：详解现代LLM基础架构</div>
      <div>https://realwujing.github.io/linux/drivers/gpu/stanford-cs336/大模型从0到1｜第三讲：详解现代LLM基础架构/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Wu Jing</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年11月23日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/linux/drivers/gpu/stanford-cs336/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BB%8E0%E5%88%B01%EF%BD%9C%E7%AC%AC%E5%9B%9B%E8%AE%B2%EF%BC%9A%E8%AF%A6%E8%A7%A3MoE%E6%9E%B6%E6%9E%84/" title="大模型从0到1｜第四讲：详解MoE架构">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">大模型从0到1｜第四讲：详解MoE架构</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/linux/drivers/gpu/stanford-cs336/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BB%8E0%E5%88%B01%EF%BD%9C%E7%AC%AC%E4%BA%8C%E8%AE%B2%EF%BC%9Apytorch%E6%89%8B%E6%8A%8A%E6%89%8B%E6%90%AD%E5%BB%BALLM/" title="大模型从0到1｜第二讲：PyTorch手把手搭建LLM">
                        <span class="hidden-mobile">大模型从0到1｜第二讲：PyTorch手把手搭建LLM</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="gitalk-container"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#gitalk-container', function() {
      Fluid.utils.createCssLink('/css/gitalk.css')
      Fluid.utils.createScript('https://lib.baomitu.com/gitalk/1.8.0/gitalk.min.js', function() {
        var options = Object.assign(
          {"clientID":"c11f8471a6ae4d3eea12","clientSecret":"87bfa232882af2b005f4c3352132dd418bf6d113","repo":"realwujing.github.io","owner":"realwujing","admin":["realwujing"],"language":"zh-CN","labels":["Gitalk"],"perPage":10,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":false,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token"},
          {
            id: '6e60f7c038d87d4ccbdd2d731b119613'
          }
        )
        var gitalk = new Gitalk(options);
        gitalk.render('gitalk-container');
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.events.registerRefreshCallback(function() {
      if ('mermaid' in window) {
        mermaid.init();
      }
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
