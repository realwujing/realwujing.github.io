# å¤§æ¨¡å‹ä»0åˆ°1ï½œç¬¬ä¸€è®²ï¼šæ¦‚è¿°å’ŒTokenization

> è¯¾ç¨‹é“¾æ¥ï¼š[Stanford CS336 Spring 2025 - Lecture 1](https://stanford-cs336.github.io/spring2025-lectures/?trace=var/traces/lecture_01.json)

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šè¯¾ç¨‹æ¦‚è¿°ä¸åŠ¨æœº

### 1. ä¸ºä»€ä¹ˆéœ€è¦è¿™é—¨è¯¾ï¼Ÿ

**å½“å‰AIæ•™è‚²çš„ç©ºç™½**ï¼š
- å¤§å¤šæ•°è¯¾ç¨‹æ•™ä½ å¦‚ä½•**ä½¿ç”¨**å¤§æ¨¡å‹ï¼ˆè°ƒç”¨APIã€prompt engineeringï¼‰
- ä¸€äº›è¯¾ç¨‹æ•™ä½ å¦‚ä½•**å¾®è°ƒ**ç°æœ‰æ¨¡å‹ï¼ˆfine-tuningï¼‰
- ä½†å¾ˆå°‘æœ‰è¯¾ç¨‹æ•™ä½ å¦‚ä½•**ä»é›¶æ„å»º**ä¸€ä¸ªå¤§æ¨¡å‹

**CS336çš„ç‹¬ç‰¹å®šä½**ï¼š
```
å…¶ä»–è¯¾ç¨‹: ä½¿ç”¨è€…è§†è§’ â†’ å¦‚ä½•è°ƒç”¨æ¨¡å‹
CS336:    æ„å»ºè€…è§†è§’ â†’ å¦‚ä½•åˆ›é€ æ¨¡å‹
```

**è¯¾ç¨‹ç›®æ ‡**ï¼š
1. ç†è§£å¤§æ¨¡å‹çš„æ¯ä¸€ä¸ªç»„ä»¶
2. èƒ½å¤Ÿä»é›¶å®ç°ä¸€ä¸ªå®Œæ•´çš„è¯­è¨€æ¨¡å‹
3. æŒæ¡è®­ç»ƒã€è¯„ä¼°ã€éƒ¨ç½²çš„å®Œæ•´æµç¨‹
4. ç†è§£è®¾è®¡å†³ç­–èƒŒåçš„åŸç†

### 2. è¯¾ç¨‹ç»“æ„ï¼šä»0åˆ°1çš„å®Œæ•´è·¯å¾„

**ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ç»„ä»¶ï¼ˆWeeks 1-3ï¼‰**
```
Week 1: Tokenization          â† æœ¬è®²
Week 2: Model Architecture    
Week 3: Training Basics
```

**ç¬¬äºŒé˜¶æ®µï¼šé¢„è®­ç»ƒï¼ˆWeeks 4-6ï¼‰**
```
Week 4: Data Processing
Week 5: Optimization
Week 6: Distributed Training
```

**ç¬¬ä¸‰é˜¶æ®µï¼šå¯¹é½ä¸åº”ç”¨ï¼ˆWeeks 7-9ï¼‰**
```
Week 7: Instruction Tuning
Week 8: RLHF
Week 9: Evaluation & Deployment
```

**ç¬¬å››é˜¶æ®µï¼šå‰æ²¿è¯é¢˜ï¼ˆWeeks 10-12ï¼‰**
```
Week 10: Scaling Laws
Week 11: Efficient Training
Week 12: Future Directions
```

### 3. å­¦ä¹ æˆæœ

å®Œæˆæœ¬è¯¾ç¨‹åï¼Œä½ å°†èƒ½å¤Ÿï¼š

**ç†è®ºå±‚é¢**ï¼š
- è§£é‡Šæ¯ä¸ªç»„ä»¶çš„è®¾è®¡åŸç†
- ç†è§£ä¸åŒæ¶æ„é€‰æ‹©çš„æƒè¡¡
- æŒæ¡è®­ç»ƒå¤§æ¨¡å‹çš„å…³é”®æŠ€æœ¯

**å®è·µå±‚é¢**ï¼š
- ä»é›¶å®ç°ä¸€ä¸ªTransformeræ¨¡å‹
- è®­ç»ƒä¸€ä¸ªå°è§„æ¨¡ä½†å®Œæ•´çš„è¯­è¨€æ¨¡å‹
- è¯„ä¼°å’Œä¼˜åŒ–æ¨¡å‹æ€§èƒ½

**å·¥ç¨‹å±‚é¢**ï¼š
- å¤„ç†å¤§è§„æ¨¡æ•°æ®
- å®ç°åˆ†å¸ƒå¼è®­ç»ƒ
- ä¼˜åŒ–æ¨ç†æ€§èƒ½


---

## ç¬¬äºŒéƒ¨åˆ†ï¼šä»€ä¹ˆæ˜¯Tokenizationï¼Ÿ

### 1. æ ¸å¿ƒæ¦‚å¿µ

**å®šä¹‰**ï¼šTokenizationæ˜¯å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„æ•°å­—åºåˆ—çš„è¿‡ç¨‹ã€‚

**å®Œæ•´æµç¨‹**ï¼š
```
åŸå§‹æ–‡æœ¬ (Raw Text)
    â†“
åˆ†è¯ (Tokenization)
    â†“
Tokenåºåˆ— (Token Sequence)
    â†“
Token IDåºåˆ— (Token ID Sequence)
    â†“
åµŒå…¥å‘é‡ (Embedding Vectors)
    â†“
æ¨¡å‹è¾“å…¥ (Model Input)
```

**ç¤ºä¾‹**ï¼š
```python
# åŸå§‹æ–‡æœ¬
text = "Hello, world!"

# åˆ†è¯
tokens = ["Hello", ",", " world", "!"]

# Token IDs
token_ids = [15496, 11, 995, 0]

# åµŒå…¥å‘é‡ï¼ˆå‡è®¾d_model=768ï¼‰
embeddings = [
    [0.123, -0.456, ..., 0.789],  # "Hello"çš„768ç»´å‘é‡
    [0.234, -0.567, ..., 0.890],  # ","çš„768ç»´å‘é‡
    [0.345, -0.678, ..., 0.901],  # " world"çš„768ç»´å‘é‡
    [0.456, -0.789, ..., 0.012],  # "!"çš„768ç»´å‘é‡
]
```

### 2. ä¸ºä»€ä¹ˆéœ€è¦Tokenizationï¼Ÿ

**é—®é¢˜1ï¼šç¥ç»ç½‘ç»œåªèƒ½å¤„ç†æ•°å­—**
```
æ–‡æœ¬: "cat"
âŒ ä¸èƒ½ç›´æ¥è¾“å…¥ç¥ç»ç½‘ç»œ
âœ“ éœ€è¦è½¬æ¢ä¸ºæ•°å­—: [3415]
âœ“ å†è½¬æ¢ä¸ºå‘é‡: [0.1, -0.2, 0.3, ...]
```

**é—®é¢˜2ï¼šç›´æ¥ä½¿ç”¨å­—ç¬¦æ•ˆç‡ä½**
```
æ–‡æœ¬: "The cat sat on the mat"
å­—ç¬¦çº§: ['T','h','e',' ','c','a','t',' ','s','a','t',' ','o','n',' ','t','h','e',' ','m','a','t']
é•¿åº¦: 22ä¸ªå­—ç¬¦

å­è¯çº§: ['The', ' cat', ' sat', ' on', ' the', ' mat']
é•¿åº¦: 6ä¸ªtoken
```

**é—®é¢˜3ï¼šç›´æ¥ä½¿ç”¨å•è¯è¯æ±‡è¡¨çˆ†ç‚¸**
```
è‹±è¯­å•è¯æ•°: ~170,000
åŠ ä¸Šå˜å½¢: ~500,000+
åŠ ä¸Šä¸“æœ‰åè¯: æ— é™

å­è¯æ–¹æ¡ˆ: 30,000-50,000ä¸ªtokenå³å¯è¦†ç›–
```

### 3. Tokenç²’åº¦çš„é€‰æ‹©

| ç²’åº¦ | è¯æ±‡è¡¨å¤§å° | åºåˆ—é•¿åº¦ | ä¼˜ç‚¹ | ç¼ºç‚¹ | ä½¿ç”¨åœºæ™¯ |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **å­—ç¬¦çº§** | å¾ˆå°ï¼ˆ~100ï¼‰ | å¾ˆé•¿ | æ— OOVï¼Œç®€å• | åºåˆ—å¤ªé•¿ï¼Œéš¾å­¦ä¹  | æ‹¼å†™æ£€æŸ¥ |
| **å­è¯çº§** | ä¸­ç­‰ï¼ˆ30k-50kï¼‰ | ä¸­ç­‰ | å¹³è¡¡å¥½ | éœ€è¦è®­ç»ƒ | **ç°ä»£LLM** |
| **å•è¯çº§** | å¾ˆå¤§ï¼ˆ>100kï¼‰ | çŸ­ | è¯­ä¹‰æ¸…æ™° | OOVä¸¥é‡ | ä¼ ç»ŸNLP |
| **å­—èŠ‚çº§** | 256 | æœ€é•¿ | é€šç”¨æ€§å¼º | åºåˆ—æé•¿ | å¤šè¯­è¨€ |

**å®é™…å¯¹æ¯”**ï¼š
```python
text = "The tokenization process is important."

# å­—ç¬¦çº§ï¼ˆåŒ…æ‹¬ç©ºæ ¼ï¼‰
char_tokens = ['T','h','e',' ','t','o','k','e','n','i','z','a','t','i','o','n',' ','p','r','o','c','e','s','s',' ','i','s',' ','i','m','p','o','r','t','a','n','t','.']
print(f"å­—ç¬¦çº§é•¿åº¦: {len(char_tokens)}")  # 38

# å­è¯çº§ï¼ˆGPT-2 tokenizerï¼‰
subword_tokens = ['The', ' token', 'ization', ' process', ' is', ' important', '.']
print(f"å­è¯çº§é•¿åº¦: {len(subword_tokens)}")  # 7

# å•è¯çº§
word_tokens = ['The', 'tokenization', 'process', 'is', 'important', '.']
print(f"å•è¯çº§é•¿åº¦: {len(word_tokens)}")  # 6
```

**ä¸ºä»€ä¹ˆå­è¯çº§æ˜¯æœ€ä½³é€‰æ‹©ï¼Ÿ**

1. **å¤„ç†æœªçŸ¥è¯**ï¼š
```
å•è¯çº§: "ChatGPT" â†’ [UNK]ï¼ˆæœªçŸ¥ï¼‰
å­è¯çº§: "ChatGPT" â†’ ["Chat", "G", "PT"]ï¼ˆå¯ä»¥è¡¨ç¤ºï¼‰
```

2. **å­¦ä¹ è¯æ ¹å’Œè¯ç¼€**ï¼š
```
"play", "player", "playing", "played"
å­è¯çº§å¯ä»¥å­¦åˆ° "play" æ˜¯å…±åŒçš„è¯æ ¹
```

3. **å¤šè¯­è¨€æ”¯æŒ**ï¼š
```
è‹±è¯­: "hello" â†’ ["hello"]
ä¸­æ–‡: "ä½ å¥½" â†’ ["ä½ ", "å¥½"]
éƒ½å¯ä»¥ç”¨åŒä¸€ä¸ªtokenizerå¤„ç†
```


---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸»æµTokenizationç®—æ³•

### 1. BPE (Byte-Pair Encoding)

#### 1.1 ç®—æ³•åŸç†

**å†å²**ï¼šBPEæœ€åˆæ˜¯1994å¹´æå‡ºçš„æ•°æ®å‹ç¼©ç®—æ³•ï¼Œ2016å¹´è¢«Sennrichç­‰äººå¼•å…¥NLPã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šä»å­—ç¬¦å¼€å§‹ï¼Œè¿­ä»£åœ°åˆå¹¶æœ€é¢‘ç¹å‡ºç°çš„ç›¸é‚»å­—ç¬¦å¯¹ã€‚

**ç®—æ³•æµç¨‹**ï¼š
```
1. åˆå§‹åŒ–ï¼šå°†æ–‡æœ¬åˆ†å‰²ä¸ºå­—ç¬¦
2. ç»Ÿè®¡ï¼šè®¡ç®—æ‰€æœ‰ç›¸é‚»å­—ç¬¦å¯¹çš„é¢‘ç‡
3. åˆå¹¶ï¼šé€‰æ‹©é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹ï¼Œåˆå¹¶ä¸ºæ–°token
4. æ›´æ–°ï¼šæ›´æ–°è¯æ±‡è¡¨å’Œæ–‡æœ¬
5. é‡å¤ï¼šé‡å¤æ­¥éª¤2-4ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡è¯æ±‡è¡¨å¤§å°
```

#### 1.2 è¯¦ç»†ç¤ºä¾‹

**è®­ç»ƒè¯­æ–™**ï¼š
```
"low low low low low"
"lower lower"
"newest newest newest newest newest newest"
```

**è®­ç»ƒè¿‡ç¨‹**ï¼š

**æ­¥éª¤0ï¼šåˆå§‹åŒ–**
```
è¯æ±‡è¡¨: ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', '</w>']
æ–‡æœ¬è¡¨ç¤º:
"l o w </w>" (å‡ºç°5æ¬¡)
"l o w e r </w>" (å‡ºç°2æ¬¡)
"n e w e s t </w>" (å‡ºç°6æ¬¡)
```

**æ­¥éª¤1ï¼šç»Ÿè®¡å­—ç¬¦å¯¹é¢‘ç‡**
```
'l o': 7æ¬¡ (5æ¬¡åœ¨low, 2æ¬¡åœ¨lower)
'o w': 7æ¬¡
'w </w>': 5æ¬¡
'w e': 8æ¬¡ (2æ¬¡åœ¨lower, 6æ¬¡åœ¨newest)
'e w': 6æ¬¡
'e s': 6æ¬¡
's t': 6æ¬¡
't </w>': 6æ¬¡
...
```

**æ­¥éª¤2ï¼šåˆå¹¶æœ€é¢‘ç¹çš„å¯¹ 'w e' (8æ¬¡)**
```
è¯æ±‡è¡¨: ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', '</w>', 'we']
æ–‡æœ¬è¡¨ç¤º:
"l o w </w>" (5æ¬¡)
"l o we r </w>" (2æ¬¡)
"n e we s t </w>" (6æ¬¡)
```

**æ­¥éª¤3ï¼šç»§ç»­åˆå¹¶ 'e we' (6æ¬¡)**
```
è¯æ±‡è¡¨: [..., 'we', 'ewe']
æ–‡æœ¬è¡¨ç¤º:
"l o w </w>" (5æ¬¡)
"l o we r </w>" (2æ¬¡)
"n ewe s t </w>" (6æ¬¡)
```

**æ­¥éª¤4ï¼šåˆå¹¶ 'l o' (7æ¬¡)**
```
è¯æ±‡è¡¨: [..., 'lo']
æ–‡æœ¬è¡¨ç¤º:
"lo w </w>" (5æ¬¡)
"lo we r </w>" (2æ¬¡)
"n ewe s t </w>" (6æ¬¡)
```

**ç»§ç»­è¿­ä»£...**

**æœ€ç»ˆè¯æ±‡è¡¨å¯èƒ½åŒ…å«**ï¼š
```
åŸºç¡€å­—ç¬¦: ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', '</w>']
åˆå¹¶çš„token: ['lo', 'low', 'we', 'ewe', 'est', 'west', 'newest', ...]
```

#### 1.3 Pythonå®ç°

```python
import re
from collections import defaultdict, Counter

class BPETokenizer:
    def __init__(self, vocab_size=1000):
        self.vocab_size = vocab_size
        self.vocab = {}
        self.merges = []
    
    def get_stats(self, vocab):
        """ç»Ÿè®¡ç›¸é‚»tokenå¯¹çš„é¢‘ç‡"""
        pairs = defaultdict(int)
        for word, freq in vocab.items():
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pairs[symbols[i], symbols[i + 1]] += freq
        return pairs
    
    def merge_vocab(self, pair, vocab):
        """åˆå¹¶æŒ‡å®šçš„tokenå¯¹"""
        new_vocab = {}
        bigram = ' '.join(pair)
        replacement = ''.join(pair)
        
        for word in vocab:
            new_word = word.replace(bigram, replacement)
            new_vocab[new_word] = vocab[word]
        
        return new_vocab
    
    def train(self, texts):
        """è®­ç»ƒBPE tokenizer"""
        # 1. åˆå§‹åŒ–è¯æ±‡è¡¨ï¼ˆå­—ç¬¦çº§ï¼‰
        vocab = defaultdict(int)
        for text in texts:
            words = text.split()
            for word in words:
                # æ·»åŠ è¯å°¾æ ‡è®°
                vocab[' '.join(list(word)) + ' </w>'] += 1
        
        # 2. è¿­ä»£åˆå¹¶
        num_merges = self.vocab_size - len(set(''.join(texts)))
        
        for i in range(num_merges):
            pairs = self.get_stats(vocab)
            if not pairs:
                break
            
            # é€‰æ‹©é¢‘ç‡æœ€é«˜çš„pair
            best_pair = max(pairs, key=pairs.get)
            vocab = self.merge_vocab(best_pair, vocab)
            
            # è®°å½•åˆå¹¶æ“ä½œ
            self.merges.append(best_pair)
            
            if (i + 1) % 100 == 0:
                print(f"Merge {i+1}: {best_pair} (freq: {pairs[best_pair]})")
        
        # 3. æ„å»ºæœ€ç»ˆè¯æ±‡è¡¨
        self.vocab = self._build_vocab(vocab)
    
    def _build_vocab(self, vocab):
        """ä»è®­ç»ƒç»“æœæ„å»ºè¯æ±‡è¡¨"""
        tokens = set()
        for word in vocab.keys():
            tokens.update(word.split())
        return {token: idx for idx, token in enumerate(sorted(tokens))}
    
    def tokenize(self, text):
        """ä½¿ç”¨è®­ç»ƒå¥½çš„BPEè¿›è¡Œåˆ†è¯"""
        words = text.split()
        tokens = []
        
        for word in words:
            # åˆå§‹åŒ–ä¸ºå­—ç¬¦çº§
            word_tokens = list(word) + ['</w>']
            
            # åº”ç”¨å­¦åˆ°çš„åˆå¹¶è§„åˆ™
            for pair in self.merges:
                i = 0
                while i < len(word_tokens) - 1:
                    if (word_tokens[i], word_tokens[i + 1]) == pair:
                        word_tokens = (word_tokens[:i] + 
                                     [''.join(pair)] + 
                                     word_tokens[i + 2:])
                    else:
                        i += 1
            
            tokens.extend(word_tokens)
        
        return tokens
    
    def encode(self, text):
        """å°†æ–‡æœ¬ç¼–ç ä¸ºtoken IDs"""
        tokens = self.tokenize(text)
        return [self.vocab.get(token, self.vocab.get('<unk>', 0)) 
                for token in tokens]
    
    def decode(self, token_ids):
        """å°†token IDsè§£ç ä¸ºæ–‡æœ¬"""
        inv_vocab = {v: k for k, v in self.vocab.items()}
        tokens = [inv_vocab.get(tid, '<unk>') for tid in token_ids]
        text = ''.join(tokens).replace('</w>', ' ').strip()
        return text

# ä½¿ç”¨ç¤ºä¾‹
texts = [
    "low low low low low",
    "lower lower",
    "newest newest newest newest newest newest",
    "widest widest widest"
]

tokenizer = BPETokenizer(vocab_size=50)
tokenizer.train(texts)

# æµ‹è¯•
test_text = "lowest newer"
tokens = tokenizer.tokenize(test_text)
print(f"Tokens: {tokens}")

ids = tokenizer.encode(test_text)
print(f"Token IDs: {ids}")

decoded = tokenizer.decode(ids)
print(f"Decoded: {decoded}")
```

#### 1.4 Byte-Level BPE

**GPT-2å¼•å…¥çš„æ”¹è¿›**ï¼šåœ¨å­—èŠ‚çº§åˆ«è€Œä¸æ˜¯å­—ç¬¦çº§åˆ«æ“ä½œã€‚

**ä¼˜åŠ¿**ï¼š
```
å­—ç¬¦çº§BPE: éœ€è¦å¤„ç†æ‰€æœ‰Unicodeå­—ç¬¦ï¼ˆ>100kï¼‰
å­—èŠ‚çº§BPE: åªéœ€è¦å¤„ç†256ä¸ªå­—èŠ‚

ç¤ºä¾‹ï¼š
"ä½ å¥½" (Unicodeå­—ç¬¦)
â†’ UTF-8ç¼–ç : [0xE4, 0xBD, 0xA0, 0xE5, 0xA5, 0xBD]
â†’ 6ä¸ªå­—èŠ‚
â†’ å¯ä»¥ç”¨256ä¸ªåŸºç¡€tokenè¡¨ç¤ºä»»ä½•æ–‡æœ¬
```

**å®ç°**ï¼š
```python
def bytes_to_unicode():
    """
    åˆ›å»ºå­—èŠ‚åˆ°Unicodeå­—ç¬¦çš„æ˜ å°„
    é¿å…ä½¿ç”¨æ§åˆ¶å­—ç¬¦å’Œç©ºç™½å­—ç¬¦
    """
    bs = (list(range(ord("!"), ord("~") + 1)) + 
          list(range(ord("Â¡"), ord("Â¬") + 1)) + 
          list(range(ord("Â®"), ord("Ã¿") + 1)))
    
    cs = bs[:]
    n = 0
    for b in range(2**8):
        if b not in bs:
            bs.append(b)
            cs.append(2**8 + n)
            n += 1
    
    cs = [chr(n) for n in cs]
    return dict(zip(bs, cs))

# GPT-2ä½¿ç”¨çš„æ˜ å°„
byte_encoder = bytes_to_unicode()
byte_decoder = {v: k for k, v in byte_encoder.items()}

def encode_text(text):
    """å°†æ–‡æœ¬ç¼–ç ä¸ºå­—èŠ‚åºåˆ—"""
    return [byte_encoder[b] for b in text.encode('utf-8')]

def decode_bytes(byte_tokens):
    """å°†å­—èŠ‚åºåˆ—è§£ç ä¸ºæ–‡æœ¬"""
    byte_array = bytes([byte_decoder[c] for c in byte_tokens])
    return byte_array.decode('utf-8', errors='replace')
```


### 2. WordPiece

#### 2.1 ç®—æ³•åŸç†

**å†å²**ï¼šç”±Googleå¼€å‘ï¼Œç”¨äºBERTç­‰æ¨¡å‹ã€‚

**ä¸BPEçš„åŒºåˆ«**ï¼š
```
BPE:       é€‰æ‹©é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹
WordPiece: é€‰æ‹©èƒ½æœ€å¤§åŒ–è¯­è¨€æ¨¡å‹ä¼¼ç„¶åº¦çš„å­—ç¬¦å¯¹
```

**åˆå¹¶å‡†åˆ™**ï¼š
```
score(x, y) = log P(xy) - log P(x) - log P(y)
            = log [P(xy) / (P(x) Ã— P(y))]

é€‰æ‹©scoreæœ€å¤§çš„pairè¿›è¡Œåˆå¹¶
```

**ç›´è§‰ç†è§£**ï¼š
- å¦‚æœxå’Œyç»å¸¸ä¸€èµ·å‡ºç°ï¼ŒP(xy)ä¼šå¾ˆå¤§
- å¦‚æœå®ƒä»¬ç‹¬ç«‹å‡ºç°ä¹Ÿå¾ˆé¢‘ç¹ï¼ŒP(x)å’ŒP(y)ä¹Ÿå¤§
- scoreè¡¡é‡çš„æ˜¯"ä¸€èµ·å‡ºç°"ç›¸å¯¹äº"ç‹¬ç«‹å‡ºç°"çš„å¢ç›Š

#### 2.2 å®ç°ç¤ºä¾‹

```python
import math
from collections import defaultdict

class WordPieceTokenizer:
    def __init__(self, vocab_size=1000):
        self.vocab_size = vocab_size
        self.vocab = {}
    
    def get_pair_scores(self, vocab):
        """è®¡ç®—æ¯ä¸ªpairçš„score"""
        # ç»Ÿè®¡å•ä¸ªtokenå’Œpairçš„é¢‘ç‡
        token_freq = defaultdict(int)
        pair_freq = defaultdict(int)
        total = 0
        
        for word, freq in vocab.items():
            symbols = word.split()
            for symbol in symbols:
                token_freq[symbol] += freq
                total += freq
            
            for i in range(len(symbols) - 1):
                pair = (symbols[i], symbols[i + 1])
                pair_freq[pair] += freq
        
        # è®¡ç®—score
        scores = {}
        for pair, freq in pair_freq.items():
            x, y = pair
            # score = log(P(xy) / (P(x) * P(y)))
            p_xy = freq / total
            p_x = token_freq[x] / total
            p_y = token_freq[y] / total
            
            if p_x > 0 and p_y > 0:
                scores[pair] = math.log(p_xy / (p_x * p_y))
        
        return scores
    
    def train(self, texts):
        """è®­ç»ƒWordPiece tokenizer"""
        # åˆå§‹åŒ–
        vocab = defaultdict(int)
        for text in texts:
            words = text.split()
            for word in words:
                vocab[' '.join(list(word)) + ' </w>'] += 1
        
        # è¿­ä»£åˆå¹¶
        num_merges = self.vocab_size - len(set(''.join(texts)))
        
        for i in range(num_merges):
            scores = self.get_pair_scores(vocab)
            if not scores:
                break
            
            # é€‰æ‹©scoreæœ€é«˜çš„pair
            best_pair = max(scores, key=scores.get)
            vocab = self.merge_vocab(best_pair, vocab)
            
            if (i + 1) % 100 == 0:
                print(f"Merge {i+1}: {best_pair} (score: {scores[best_pair]:.4f})")
        
        self.vocab = self._build_vocab(vocab)
    
    def merge_vocab(self, pair, vocab):
        """åˆå¹¶æŒ‡å®šçš„pair"""
        new_vocab = {}
        bigram = ' '.join(pair)
        replacement = ''.join(pair)
        
        for word in vocab:
            new_word = word.replace(bigram, replacement)
            new_vocab[new_word] = vocab[word]
        
        return new_vocab
    
    def _build_vocab(self, vocab):
        """æ„å»ºè¯æ±‡è¡¨"""
        tokens = set()
        for word in vocab.keys():
            tokens.update(word.split())
        return {token: idx for idx, token in enumerate(sorted(tokens))}
```

#### 2.3 WordPieceçš„ç‰¹æ®Šæ ‡è®°

**BERTä½¿ç”¨çš„æ ‡è®°**ï¼š
```python
# å­è¯å‰ç¼€
"playing" â†’ ["play", "##ing"]
"unhappiness" â†’ ["un", "##happiness"]

# ç‰¹æ®Štoken
[CLS]: å¥å­å¼€å§‹
[SEP]: å¥å­åˆ†éš”
[MASK]: æ©ç ï¼ˆç”¨äºMLMé¢„è®­ç»ƒï¼‰
[PAD]: å¡«å……
[UNK]: æœªçŸ¥è¯
```

**ç¤ºä¾‹**ï¼š
```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

text = "playing football"
tokens = tokenizer.tokenize(text)
print(tokens)  # ['play', '##ing', 'football']

# å®Œæ•´ç¼–ç ï¼ˆåŒ…å«ç‰¹æ®Štokenï¼‰
encoded = tokenizer.encode(text, add_special_tokens=True)
print(encoded)  # [101, 2652, 2075, 2374, 102]
# 101: [CLS], 2652: play, 2075: ##ing, 2374: football, 102: [SEP]
```

### 3. Unigram Language Model

#### 3.1 ç®—æ³•åŸç†

**æ ¸å¿ƒæ€æƒ³**ï¼šä¸BPE/WordPieceç›¸åï¼Œä»å¤§è¯æ±‡è¡¨å¼€å§‹ï¼Œé€æ­¥åˆ é™¤ã€‚

**ç®—æ³•æµç¨‹**ï¼š
```
1. åˆå§‹åŒ–ï¼šåˆ›å»ºä¸€ä¸ªå¤§çš„å€™é€‰è¯æ±‡è¡¨ï¼ˆæ‰€æœ‰å­ä¸²ï¼‰
2. è®­ç»ƒï¼šä½¿ç”¨EMç®—æ³•ä¼°è®¡æ¯ä¸ªtokençš„æ¦‚ç‡
3. å‰ªæï¼šåˆ é™¤å¯¹ä¼¼ç„¶åº¦è´¡çŒ®æœ€å°çš„token
4. é‡å¤ï¼šé‡å¤æ­¥éª¤2-3ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡å¤§å°
```

**æ•°å­¦åŸºç¡€**ï¼š

ç»™å®šè¯æ±‡è¡¨Vå’Œæ–‡æœ¬Xï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–ï¼š
```
L(X) = Î£ log P(x) for x in X

å…¶ä¸­ P(x) = Î£ P(segmentation) Ã— P(tokens in segmentation)
```

å¯¹äºæ¯ä¸ªè¯xï¼Œå¯èƒ½æœ‰å¤šç§åˆ†è¯æ–¹å¼ï¼š
```
"unhappiness"å¯èƒ½çš„åˆ†è¯ï¼š
- ["unhappiness"]
- ["un", "happiness"]
- ["un", "happy", "ness"]
- ["u", "n", "h", "a", "p", "p", "i", "n", "e", "s", "s"]
```

#### 3.2 å®ç°ç¤ºä¾‹

```python
import math
from collections import defaultdict

class UnigramTokenizer:
    def __init__(self, vocab_size=1000):
        self.vocab_size = vocab_size
        self.vocab = {}
    
    def initialize_vocab(self, texts):
        """åˆå§‹åŒ–å€™é€‰è¯æ±‡è¡¨"""
        vocab = set()
        
        for text in texts:
            words = text.split()
            for word in words:
                # æ·»åŠ æ‰€æœ‰å¯èƒ½çš„å­ä¸²
                for i in range(len(word)):
                    for j in range(i + 1, len(word) + 1):
                        vocab.add(word[i:j])
        
        # åˆå§‹åŒ–æ¦‚ç‡ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰
        prob = 1.0 / len(vocab)
        return {token: prob for token in vocab}
    
    def get_best_segmentation(self, word, vocab):
        """ä½¿ç”¨åŠ¨æ€è§„åˆ’æ‰¾åˆ°æœ€ä½³åˆ†è¯"""
        n = len(word)
        # dp[i] = (æœ€å¤§logæ¦‚ç‡, åˆ†è¯æ–¹æ¡ˆ)
        dp = [(-float('inf'), [])] * (n + 1)
        dp[0] = (0.0, [])
        
        for i in range(1, n + 1):
            for j in range(i):
                token = word[j:i]
                if token in vocab:
                    score = dp[j][0] + math.log(vocab[token])
                    if score > dp[i][0]:
                        dp[i] = (score, dp[j][1] + [token])
        
        return dp[n][1]
    
    def em_step(self, texts, vocab):
        """EMç®—æ³•çš„ä¸€æ­¥"""
        # E-step: è®¡ç®—æœŸæœ›è®¡æ•°
        token_count = defaultdict(float)
        total_count = 0
        
        for text in texts:
            words = text.split()
            for word in words:
                segmentation = self.get_best_segmentation(word, vocab)
                for token in segmentation:
                    token_count[token] += 1
                    total_count += 1
        
        # M-step: æ›´æ–°æ¦‚ç‡
        new_vocab = {}
        for token in vocab:
            new_vocab[token] = token_count[token] / total_count if total_count > 0 else 0
        
        return new_vocab
    
    def compute_loss(self, texts, vocab):
        """è®¡ç®—å½“å‰è¯æ±‡è¡¨çš„æŸå¤±"""
        total_loss = 0
        for text in texts:
            words = text.split()
            for word in words:
                segmentation = self.get_best_segmentation(word, vocab)
                word_loss = sum(math.log(vocab[token]) for token in segmentation)
                total_loss += word_loss
        return total_loss
    
    def train(self, texts, num_iterations=10):
        """è®­ç»ƒUnigram tokenizer"""
        # 1. åˆå§‹åŒ–å¤§è¯æ±‡è¡¨
        vocab = self.initialize_vocab(texts)
        print(f"Initial vocab size: {len(vocab)}")
        
        # 2. EMè¿­ä»£
        for iteration in range(num_iterations):
            vocab = self.em_step(texts, vocab)
            loss = self.compute_loss(texts, vocab)
            print(f"Iteration {iteration + 1}, Loss: {loss:.2f}")
        
        # 3. å‰ªæåˆ°ç›®æ ‡å¤§å°
        while len(vocab) > self.vocab_size:
            # è®¡ç®—åˆ é™¤æ¯ä¸ªtokençš„æŸå¤±å¢åŠ 
            loss_increase = {}
            current_loss = self.compute_loss(texts, vocab)
            
            for token in list(vocab.keys()):
                if len(token) == 1:  # ä¿ç•™å•å­—ç¬¦
                    continue
                
                # ä¸´æ—¶åˆ é™¤token
                temp_vocab = vocab.copy()
                del temp_vocab[token]
                
                # é‡æ–°å½’ä¸€åŒ–
                total_prob = sum(temp_vocab.values())
                temp_vocab = {k: v / total_prob for k, v in temp_vocab.items()}
                
                # è®¡ç®—æŸå¤±å¢åŠ 
                new_loss = self.compute_loss(texts, temp_vocab)
                loss_increase[token] = new_loss - current_loss
            
            # åˆ é™¤æŸå¤±å¢åŠ æœ€å°çš„token
            if loss_increase:
                token_to_remove = min(loss_increase, key=loss_increase.get)
                del vocab[token_to_remove]
                
                # é‡æ–°å½’ä¸€åŒ–
                total_prob = sum(vocab.values())
                vocab = {k: v / total_prob for k, v in vocab.items()}
                
                if len(vocab) % 100 == 0:
                    print(f"Vocab size: {len(vocab)}, Removed: {token_to_remove}")
        
        self.vocab = vocab
    
    def tokenize(self, text):
        """åˆ†è¯"""
        words = text.split()
        tokens = []
        for word in words:
            segmentation = self.get_best_segmentation(word, self.vocab)
            tokens.extend(segmentation)
        return tokens
```

#### 3.3 Unigramçš„ä¼˜åŠ¿

**1. å¤šç§åˆ†è¯æ–¹å¼**ï¼š
```python
# Unigramå¯ä»¥ä¸ºåŒä¸€ä¸ªè¯æä¾›å¤šç§åˆ†è¯
# åœ¨è®­ç»ƒæ—¶å¯ä»¥é‡‡æ ·ä¸åŒçš„åˆ†è¯æ–¹å¼ï¼Œå¢åŠ é²æ£’æ€§

word = "unhappiness"
possible_segmentations = [
    (["unhappiness"], 0.3),
    (["un", "happiness"], 0.5),
    (["un", "happy", "ness"], 0.2)
]

# å¯ä»¥æ ¹æ®æ¦‚ç‡é‡‡æ ·
import random
def sample_segmentation(word, vocab):
    # è·å–æ‰€æœ‰å¯èƒ½çš„åˆ†è¯åŠå…¶æ¦‚ç‡
    # æ ¹æ®æ¦‚ç‡é‡‡æ ·
    pass
```

**2. å¯¹å™ªå£°æ›´é²æ£’**ï¼š
```python
# å³ä½¿æœ‰æ‹¼å†™é”™è¯¯ï¼Œä¹Ÿèƒ½æ‰¾åˆ°åˆç†çš„åˆ†è¯
"hapiness" â†’ ["hap", "i", "ness"]  # ç¼ºå°‘ä¸€ä¸ªp
"happinesss" â†’ ["happiness", "s"]  # å¤šäº†ä¸€ä¸ªs
```

**3. æ›´çµæ´»çš„è¯æ±‡è¡¨**ï¼š
```python
# å¯ä»¥è½»æ¾è°ƒæ•´è¯æ±‡è¡¨å¤§å°
# ä¸éœ€è¦é‡æ–°è®­ç»ƒï¼Œåªéœ€è¦è°ƒæ•´å‰ªæé˜ˆå€¼
```


### 4. ä¸‰ç§ç®—æ³•å¯¹æ¯”æ€»ç»“

| ç‰¹æ€§ | BPE | WordPiece | Unigram |
| :--- | :--- | :--- | :--- |
| **è®­ç»ƒæ–¹å‘** | è‡ªåº•å‘ä¸Šï¼ˆåˆå¹¶ï¼‰ | è‡ªåº•å‘ä¸Šï¼ˆåˆå¹¶ï¼‰ | è‡ªé¡¶å‘ä¸‹ï¼ˆåˆ é™¤ï¼‰ |
| **åˆå¹¶/åˆ é™¤å‡†åˆ™** | é¢‘ç‡æœ€é«˜ | ä¼¼ç„¶åº¦å¢ç›Šæœ€å¤§ | ä¼¼ç„¶åº¦æŸå¤±æœ€å° |
| **åˆ†è¯ç¡®å®šæ€§** | ç¡®å®šæ€§ | ç¡®å®šæ€§ | å¯ä»¥æ¦‚ç‡é‡‡æ · |
| **è®­ç»ƒå¤æ‚åº¦** | O(nÂ²) | O(nÂ²) | O(nÂ³) |
| **å®ç°éš¾åº¦** | ç®€å• | ä¸­ç­‰ | å¤æ‚ |
| **å¤šè¯­è¨€æ”¯æŒ** | å¥½ | å¥½ | æœ€å¥½ |
| **å¯¹å™ªå£°é²æ£’æ€§** | ä¸­ç­‰ | ä¸­ç­‰ | å¥½ |
| **ä»£è¡¨æ¨¡å‹** | GPTç³»åˆ—, Llama | BERT, DistilBERT | T5, mBART, XLNet |

**æ€§èƒ½å¯¹æ¯”å®éªŒ**ï¼š
```python
# åœ¨ç›¸åŒè¯­æ–™ä¸Šè®­ç»ƒä¸‰ç§tokenizer
corpus = load_corpus("wikitext-103")

# BPE
bpe = BPETokenizer(vocab_size=32000)
bpe.train(corpus)
bpe_compression = evaluate_compression(bpe, test_corpus)
bpe_coverage = evaluate_coverage(bpe, test_corpus)

# WordPiece
wp = WordPieceTokenizer(vocab_size=32000)
wp.train(corpus)
wp_compression = evaluate_compression(wp, test_corpus)
wp_coverage = evaluate_coverage(wp, test_corpus)

# Unigram
uni = UnigramTokenizer(vocab_size=32000)
uni.train(corpus)
uni_compression = evaluate_compression(uni, test_corpus)
uni_coverage = evaluate_coverage(uni, test_corpus)

# ç»“æœï¼ˆç¤ºä¾‹ï¼‰
"""
ç®—æ³•        å‹ç¼©ç‡    è¦†ç›–ç‡    è®­ç»ƒæ—¶é—´
BPE         4.2      99.5%     10min
WordPiece   4.3      99.6%     15min
Unigram     4.4      99.7%     25min
"""
```

---

## ç¬¬å››éƒ¨åˆ†ï¼šSentencePiece - ç”Ÿäº§çº§å®ç°

### 1. ä¸ºä»€ä¹ˆéœ€è¦SentencePieceï¼Ÿ

**ä¼ ç»Ÿtokenizerçš„é—®é¢˜**ï¼š
```python
# ä¼ ç»Ÿæµç¨‹
text = "Hello world"
# 1. é¢„åˆ†è¯ï¼ˆä¾èµ–è¯­è¨€ï¼‰
words = text.split()  # ["Hello", "world"]
# 2. å­è¯åˆ†è¯
tokens = subword_tokenize(words)  # ["Hello", "world"]

# é—®é¢˜ï¼š
# - ä¾èµ–ç©ºæ ¼åˆ†è¯ï¼ˆä¸­æ–‡ã€æ—¥æ–‡æ€ä¹ˆåŠï¼Ÿï¼‰
# - é¢„å¤„ç†æ­¥éª¤ä¸å¯é€†ï¼ˆä¸¢å¤±äº†åŸå§‹æ ¼å¼ï¼‰
# - éš¾ä»¥å¤„ç†å¤šè¯­è¨€
```

**SentencePieceçš„è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# SentencePieceæµç¨‹
text = "Hello world"
# ç›´æ¥å¤„ç†åŸå§‹æ–‡æœ¬ï¼Œå°†ç©ºæ ¼è§†ä¸ºç‰¹æ®Šå­—ç¬¦
tokens = sp.encode(text)  # ["â–Hello", "â–world"]

# ä¼˜åŠ¿ï¼š
# - è¯­è¨€æ— å…³ï¼ˆä¸éœ€è¦é¢„åˆ†è¯ï¼‰
# - å®Œå…¨å¯é€†ï¼ˆå¯ä»¥å®Œç¾è¿˜åŸåŸæ–‡ï¼‰
# - ç»Ÿä¸€å¤„ç†æ‰€æœ‰è¯­è¨€
```

### 2. SentencePieceçš„æ ¸å¿ƒç‰¹æ€§

#### 2.1 è¯­è¨€æ— å…³æ€§

**ç©ºæ ¼å¤„ç†**ï¼š
```python
import sentencepiece as spm

# è®­ç»ƒ
spm.SentencePieceTrainer.train(
    input='corpus.txt',
    model_prefix='sp_model',
    vocab_size=32000,
    character_coverage=0.9995,  # å­—ç¬¦è¦†ç›–ç‡
    model_type='bpe'  # æˆ– 'unigram'
)

# ä½¿ç”¨
sp = spm.SentencePieceProcessor()
sp.load('sp_model.model')

# è‹±æ–‡ï¼ˆæœ‰ç©ºæ ¼ï¼‰
text_en = "Hello world"
tokens_en = sp.encode_as_pieces(text_en)
print(tokens_en)  # ['â–Hello', 'â–world']

# ä¸­æ–‡ï¼ˆæ— ç©ºæ ¼ï¼‰
text_zh = "ä½ å¥½ä¸–ç•Œ"
tokens_zh = sp.encode_as_pieces(text_zh)
print(tokens_zh)  # ['â–ä½ å¥½', 'ä¸–ç•Œ']

# æ··åˆ
text_mix = "Hello ä¸–ç•Œ"
tokens_mix = sp.encode_as_pieces(text_mix)
print(tokens_mix)  # ['â–Hello', 'â–ä¸–ç•Œ']
```

**â–ç¬¦å·çš„å«ä¹‰**ï¼š
```
â– (U+2581) è¡¨ç¤ºåŸå§‹æ–‡æœ¬ä¸­çš„ç©ºæ ¼
è¿™æ ·å¯ä»¥åŒºåˆ†ï¼š
"hello world" â†’ ["â–hello", "â–world"]
"helloworld"  â†’ ["â–helloworld"]
```

#### 2.2 å®Œå…¨å¯é€†æ€§

```python
# ç¼–ç 
text = "Hello, world! How are you?"
ids = sp.encode_as_ids(text)
pieces = sp.encode_as_pieces(text)

print(f"Original: {text}")
print(f"IDs: {ids}")
print(f"Pieces: {pieces}")

# è§£ç 
decoded_from_ids = sp.decode_ids(ids)
decoded_from_pieces = sp.decode_pieces(pieces)

print(f"Decoded from IDs: {decoded_from_ids}")
print(f"Decoded from pieces: {decoded_from_pieces}")

# éªŒè¯å¯é€†æ€§
assert text == decoded_from_ids
assert text == decoded_from_pieces
```

#### 2.3 å­è¯æ­£åˆ™åŒ–ï¼ˆSubword Regularizationï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨è®­ç»ƒæ—¶ï¼Œå¯¹åŒä¸€ä¸ªè¯ä½¿ç”¨ä¸åŒçš„åˆ†è¯æ–¹å¼ï¼Œå¢åŠ æ¨¡å‹é²æ£’æ€§ã€‚

```python
# å¯ç”¨å­è¯æ­£åˆ™åŒ–
sp = spm.SentencePieceProcessor()
sp.load('sp_model.model')

text = "unhappiness"

# ç¡®å®šæ€§åˆ†è¯
tokens_deterministic = sp.encode_as_pieces(text)
print(f"Deterministic: {tokens_deterministic}")

# éšæœºé‡‡æ ·ä¸åŒçš„åˆ†è¯ï¼ˆç”¨äºè®­ç»ƒï¼‰
for i in range(5):
    tokens_sampled = sp.sample_encode_as_pieces(text, nbest_size=-1, alpha=0.1)
    print(f"Sample {i+1}: {tokens_sampled}")

# è¾“å‡ºç¤ºä¾‹ï¼š
"""
Deterministic: ['â–un', 'happiness']
Sample 1: ['â–un', 'happiness']
Sample 2: ['â–un', 'happy', 'ness']
Sample 3: ['â–unhappiness']
Sample 4: ['â–un', 'hap', 'pi', 'ness']
Sample 5: ['â–un', 'happiness']
"""
```

**å‚æ•°è¯´æ˜**ï¼š
```python
# nbest_size: è€ƒè™‘çš„top-kåˆ†è¯æ–¹æ¡ˆ
# nbest_size=-1: è€ƒè™‘æ‰€æœ‰å¯èƒ½çš„åˆ†è¯
# nbest_size=10: åªè€ƒè™‘æ¦‚ç‡æœ€é«˜çš„10ç§

# alpha: å¹³æ»‘å‚æ•°
# alpha=0.0: æ€»æ˜¯é€‰æ‹©æœ€ä¼˜åˆ†è¯ï¼ˆç¡®å®šæ€§ï¼‰
# alpha=1.0: å®Œå…¨éšæœº
# alpha=0.1: è½»å¾®éšæœºåŒ–ï¼ˆæ¨èï¼‰
```

### 3. SentencePieceå®æˆ˜

#### 3.1 è®­ç»ƒè‡ªå®šä¹‰tokenizer

```python
import sentencepiece as spm
import os

# å‡†å¤‡è®­ç»ƒæ•°æ®
def prepare_corpus(input_files, output_file):
    """åˆå¹¶å¤šä¸ªæ–‡ä»¶ä¸ºè®­ç»ƒè¯­æ–™"""
    with open(output_file, 'w', encoding='utf-8') as outf:
        for input_file in input_files:
            with open(input_file, 'r', encoding='utf-8') as inf:
                outf.write(inf.read())

# è®­ç»ƒå‚æ•°
train_params = {
    'input': 'corpus.txt',
    'model_prefix': 'my_tokenizer',
    'vocab_size': 32000,
    'character_coverage': 0.9995,
    'model_type': 'bpe',  # æˆ– 'unigram', 'char', 'word'
    
    # ç‰¹æ®Štoken
    'pad_id': 0,
    'unk_id': 1,
    'bos_id': 2,
    'eos_id': 3,
    'pad_piece': '[PAD]',
    'unk_piece': '[UNK]',
    'bos_piece': '[BOS]',
    'eos_piece': '[EOS]',
    
    # é¢å¤–çš„ç‰¹æ®Štoken
    'user_defined_symbols': ['[MASK]', '[CLS]', '[SEP]'],
    
    # è®­ç»ƒå‚æ•°
    'num_threads': 16,
    'max_sentence_length': 16384,
    'shuffle_input_sentence': True,
    'train_extremely_large_corpus': False,
    
    # å­—ç¬¦è¦†ç›–ç‡ï¼ˆé‡è¦ï¼ï¼‰
    # 0.9995: é€‚åˆå¤§å¤šæ•°è¯­è¨€
    # 1.0: åŒ…å«æ‰€æœ‰å­—ç¬¦ï¼ˆå¯èƒ½åŒ…å«å™ªå£°ï¼‰
    'character_coverage': 0.9995,
    
    # åˆ†è¯å‚æ•°
    'split_by_whitespace': True,
    'split_by_number': True,
    'split_by_unicode_script': True,
    
    # æ§åˆ¶è¯æ±‡è¡¨
    'max_sentencepiece_length': 16,
    'byte_fallback': True,  # ä½¿ç”¨å­—èŠ‚å›é€€å¤„ç†æœªçŸ¥å­—ç¬¦
}

# è®­ç»ƒ
spm.SentencePieceTrainer.train(**train_params)

print("Training completed!")
print(f"Model saved: my_tokenizer.model")
print(f"Vocab saved: my_tokenizer.vocab")
```

#### 3.2 åŠ è½½å’Œä½¿ç”¨

```python
# åŠ è½½æ¨¡å‹
sp = spm.SentencePieceProcessor()
sp.load('my_tokenizer.model')

# åŸºæœ¬ä¿¡æ¯
print(f"Vocab size: {sp.vocab_size()}")
print(f"BOS ID: {sp.bos_id()}")
print(f"EOS ID: {sp.eos_id()}")
print(f"PAD ID: {sp.pad_id()}")
print(f"UNK ID: {sp.unk_id()}")

# ç¼–ç 
text = "Hello, world! This is a test."

# æ–¹æ³•1: ç¼–ç ä¸ºpieces
pieces = sp.encode_as_pieces(text)
print(f"Pieces: {pieces}")

# æ–¹æ³•2: ç¼–ç ä¸ºIDs
ids = sp.encode_as_ids(text)
print(f"IDs: {ids}")

# æ–¹æ³•3: åŒæ—¶è·å–pieceså’ŒIDs
pieces_and_ids = [(sp.id_to_piece(id), id) for id in ids]
print(f"Pieces and IDs: {pieces_and_ids}")

# è§£ç 
decoded = sp.decode_ids(ids)
print(f"Decoded: {decoded}")

# éªŒè¯
assert text == decoded
```

#### 3.3 æ‰¹å¤„ç†

```python
# æ‰¹é‡ç¼–ç 
texts = [
    "Hello, world!",
    "How are you?",
    "I'm fine, thank you."
]

# ç¼–ç ä¸ºIDs
ids_batch = [sp.encode_as_ids(text) for text in texts]

# å¡«å……åˆ°ç›¸åŒé•¿åº¦
max_len = max(len(ids) for ids in ids_batch)
padded_ids = [
    ids + [sp.pad_id()] * (max_len - len(ids))
    for ids in ids_batch
]

print(f"Padded batch shape: {len(padded_ids)} x {max_len}")

# è§£ç 
decoded_texts = [sp.decode_ids(ids) for ids in ids_batch]
for original, decoded in zip(texts, decoded_texts):
    assert original == decoded
```

#### 3.4 ä¸Hugging Faceé›†æˆ

```python
from transformers import PreTrainedTokenizerFast
from tokenizers import SentencePieceBPETokenizer

# æ–¹æ³•1: ç›´æ¥ä½¿ç”¨SentencePiece
import sentencepiece as spm

class SPTokenizer:
    def __init__(self, model_path):
        self.sp = spm.SentencePieceProcessor()
        self.sp.load(model_path)
    
    def __call__(self, text, **kwargs):
        return {
            'input_ids': self.sp.encode_as_ids(text),
            'attention_mask': [1] * len(self.sp.encode_as_ids(text))
        }
    
    def decode(self, ids, **kwargs):
        return self.sp.decode_ids(ids)

# æ–¹æ³•2: è½¬æ¢ä¸ºHugging Faceæ ¼å¼
# éœ€è¦å…ˆå¯¼å‡ºä¸ºJSONæ ¼å¼
tokenizer = PreTrainedTokenizerFast(
    tokenizer_file="my_tokenizer.json",
    bos_token="[BOS]",
    eos_token="[EOS]",
    unk_token="[UNK]",
    pad_token="[PAD]",
)
```


---

## ç¬¬äº”éƒ¨åˆ†ï¼šTokenizationçš„å…³é”®è®¾è®¡å†³ç­–

### 1. è¯æ±‡è¡¨å¤§å°ï¼ˆVocabulary Sizeï¼‰

#### 1.1 æƒè¡¡åˆ†æ

**å°è¯æ±‡è¡¨ï¼ˆå¦‚8k-16kï¼‰**ï¼š
```
ä¼˜ç‚¹ï¼š
- åµŒå…¥çŸ©é˜µå°ï¼ˆvocab_size Ã— d_modelï¼‰
- è®­ç»ƒæ›´å¿«
- æ˜¾å­˜å ç”¨å°‘

ç¼ºç‚¹ï¼š
- åºåˆ—æ›´é•¿ï¼ˆæ¯ä¸ªè¯è¢«åˆ†æˆæ›´å¤štokenï¼‰
- æ³¨æ„åŠ›è®¡ç®—æˆæœ¬é«˜ï¼ˆO(nÂ²)ï¼‰
- ä¿¡æ¯å¯†åº¦ä½
```

**å¤§è¯æ±‡è¡¨ï¼ˆå¦‚100k-256kï¼‰**ï¼š
```
ä¼˜ç‚¹ï¼š
- åºåˆ—æ›´çŸ­
- ä¿¡æ¯å¯†åº¦é«˜
- æ›´å¥½çš„å¤šè¯­è¨€æ”¯æŒ

ç¼ºç‚¹ï¼š
- åµŒå…¥çŸ©é˜µå·¨å¤§
- è®­ç»ƒæ…¢
- å®¹æ˜“è¿‡æ‹Ÿåˆ
```

**å®éªŒå¯¹æ¯”**ï¼š
```python
# åœ¨ç›¸åŒæ–‡æœ¬ä¸Šæµ‹è¯•ä¸åŒè¯æ±‡è¡¨å¤§å°
text = "The quick brown fox jumps over the lazy dog"

# è¯æ±‡è¡¨å¤§å°: 1000
tokenizer_1k = train_tokenizer(corpus, vocab_size=1000)
tokens_1k = tokenizer_1k.encode(text)
print(f"Vocab 1k: {len(tokens_1k)} tokens")  # ä¾‹å¦‚: 15 tokens

# è¯æ±‡è¡¨å¤§å°: 10000
tokenizer_10k = train_tokenizer(corpus, vocab_size=10000)
tokens_10k = tokenizer_10k.encode(text)
print(f"Vocab 10k: {len(tokens_10k)} tokens")  # ä¾‹å¦‚: 10 tokens

# è¯æ±‡è¡¨å¤§å°: 50000
tokenizer_50k = train_tokenizer(corpus, vocab_size=50000)
tokens_50k = tokenizer_50k.encode(text)
print(f"Vocab 50k: {len(tokens_50k)} tokens")  # ä¾‹å¦‚: 9 tokens

# è®¡ç®—æˆæœ¬å¯¹æ¯”
def compute_cost(vocab_size, seq_len, d_model=768, num_layers=12):
    # åµŒå…¥å±‚å‚æ•°
    embedding_params = vocab_size * d_model
    
    # æ³¨æ„åŠ›è®¡ç®—ï¼ˆç®€åŒ–ï¼‰
    attention_flops = num_layers * seq_len * seq_len * d_model
    
    return {
        'embedding_params': embedding_params,
        'attention_flops': attention_flops,
        'total_cost': embedding_params + attention_flops
    }

for vocab_size, seq_len in [(1000, 15), (10000, 10), (50000, 9)]:
    cost = compute_cost(vocab_size, seq_len)
    print(f"Vocab {vocab_size}, Seq {seq_len}: {cost}")
```

#### 1.2 ä¸»æµæ¨¡å‹çš„é€‰æ‹©

| æ¨¡å‹ | è¯æ±‡è¡¨å¤§å° | åŸå›  |
| :--- | :--- | :--- |
| **GPT-2** | 50,257 | å­—èŠ‚çº§BPEï¼Œè¦†ç›–æ‰€æœ‰Unicode |
| **GPT-3** | 50,257 | ä¸GPT-2ç›¸åŒ |
| **BERT** | 30,522 | WordPieceï¼Œè‹±æ–‡ä¸ºä¸» |
| **RoBERTa** | 50,265 | å­—èŠ‚çº§BPE |
| **T5** | 32,000 | SentencePiece Unigram |
| **Llama** | 32,000 | SentencePiece BPE |
| **Llama 2** | 32,000 | ä¸Llamaç›¸åŒ |
| **PaLM** | 256,000 | å¤§è¯æ±‡è¡¨ï¼Œå¤šè¯­è¨€ |
| **GPT-4** | ~100,000 | æ¨æµ‹ï¼Œæ”¯æŒæ›´å¤šè¯­è¨€ |

**è¶‹åŠ¿è§‚å¯Ÿ**ï¼š
```
æ—©æœŸæ¨¡å‹ï¼ˆ2018-2019ï¼‰: 30k-50k
ç°ä»£æ¨¡å‹ï¼ˆ2020-2022ï¼‰: 32k-50kï¼ˆæ ‡å‡†åŒ–ï¼‰
å¤šè¯­è¨€æ¨¡å‹ï¼ˆ2022+ï¼‰: 100k-256k
```

#### 1.3 å¦‚ä½•é€‰æ‹©è¯æ±‡è¡¨å¤§å°ï¼Ÿ

**å†³ç­–æ ‘**ï¼š
```python
def choose_vocab_size(
    languages,           # æ”¯æŒçš„è¯­è¨€
    domain,             # é¢†åŸŸï¼ˆé€šç”¨/ä¸“ä¸šï¼‰
    model_size,         # æ¨¡å‹å‚æ•°é‡
    compute_budget      # è®¡ç®—é¢„ç®—
):
    if len(languages) == 1 and languages[0] == 'english':
        # å•è¯­è¨€è‹±æ–‡
        if model_size < 1e9:  # <1Bå‚æ•°
            return 16000
        elif model_size < 10e9:  # 1B-10B
            return 32000
        else:  # >10B
            return 50000
    
    elif len(languages) <= 5:
        # å°‘æ•°è¯­è¨€
        return 32000
    
    else:
        # å¤šè¯­è¨€
        if compute_budget == 'low':
            return 64000
        elif compute_budget == 'medium':
            return 128000
        else:
            return 256000
    
    # é¢†åŸŸè°ƒæ•´
    if domain == 'code':
        # ä»£ç éœ€è¦æ›´å¤§çš„è¯æ±‡è¡¨
        return int(base_size * 1.5)
    elif domain == 'medical':
        # åŒ»å­¦æœ¯è¯­å¤š
        return int(base_size * 1.3)
    
    return base_size
```

### 2. ç‰¹æ®ŠTokençš„è®¾è®¡

#### 2.1 å¿…éœ€çš„ç‰¹æ®ŠToken

```python
class SpecialTokens:
    # æ ¸å¿ƒç‰¹æ®Štoken
    PAD = '[PAD]'    # ID: 0, å¡«å……token
    UNK = '[UNK]'    # ID: 1, æœªçŸ¥token
    BOS = '[BOS]'    # ID: 2, åºåˆ—å¼€å§‹
    EOS = '[EOS]'    # ID: 3, åºåˆ—ç»“æŸ
    
    # å¯é€‰ç‰¹æ®Štoken
    CLS = '[CLS]'    # åˆ†ç±»tokenï¼ˆBERTï¼‰
    SEP = '[SEP]'    # åˆ†éš”tokenï¼ˆBERTï¼‰
    MASK = '[MASK]'  # æ©ç tokenï¼ˆMLMï¼‰
```

**ä½¿ç”¨åœºæ™¯**ï¼š

**1. PAD - å¡«å……**ï¼š
```python
# æ‰¹å¤„ç†æ—¶å¯¹é½åºåˆ—é•¿åº¦
texts = ["Hello", "Hello world", "Hi"]
tokenized = [tokenizer.encode(t) for t in texts]

# ä¸åŒé•¿åº¦
print(tokenized)
# [[15496], [15496, 995], [17250]]

# å¡«å……åˆ°ç›¸åŒé•¿åº¦
max_len = max(len(t) for t in tokenized)
padded = [
    t + [tokenizer.pad_id] * (max_len - len(t))
    for t in tokenized
]

print(padded)
# [[15496, 0, 0], [15496, 995, 0], [17250, 0, 0]]
```

**2. BOS/EOS - åºåˆ—è¾¹ç•Œ**ï¼š
```python
# ç”Ÿæˆä»»åŠ¡
prompt = "Once upon a time"
input_ids = [tokenizer.bos_id] + tokenizer.encode(prompt)

# ç”Ÿæˆ
generated_ids = model.generate(input_ids)

# æ£€æµ‹ç»“æŸ
if generated_ids[-1] == tokenizer.eos_id:
    print("Generation completed")
```

**3. MASK - æ©ç è¯­è¨€æ¨¡å‹**ï¼š
```python
# BERTé¢„è®­ç»ƒ
text = "The cat sat on the mat"
tokens = tokenizer.encode(text)

# éšæœºæ©ç 15%çš„token
import random
masked_tokens = tokens.copy()
for i in range(len(masked_tokens)):
    if random.random() < 0.15:
        masked_tokens[i] = tokenizer.mask_id

# è®­ç»ƒç›®æ ‡ï¼šé¢„æµ‹è¢«æ©ç çš„token
```

#### 2.2 è‡ªå®šä¹‰ç‰¹æ®ŠToken

```python
# ä¸ºç‰¹å®šä»»åŠ¡æ·»åŠ ç‰¹æ®Štoken
custom_tokens = [
    '[INST]',      # æŒ‡ä»¤å¼€å§‹
    '[/INST]',     # æŒ‡ä»¤ç»“æŸ
    '[SYS]',       # ç³»ç»Ÿæ¶ˆæ¯
    '[/SYS]',      # ç³»ç»Ÿæ¶ˆæ¯ç»“æŸ
    '[USER]',      # ç”¨æˆ·æ¶ˆæ¯
    '[ASSISTANT]', # åŠ©æ‰‹æ¶ˆæ¯
]

# æ·»åŠ åˆ°tokenizer
tokenizer.add_special_tokens({'additional_special_tokens': custom_tokens})

# ä½¿ç”¨
prompt = "[INST] What is the capital of France? [/INST]"
tokens = tokenizer.encode(prompt)
```

**Llama 2çš„Chatæ ¼å¼**ï¼š
```python
# Llama 2ä½¿ç”¨ç‰¹æ®Šæ ¼å¼
template = """<s>[INST] <<SYS>>
{system_prompt}
<</SYS>>

{user_message} [/INST]"""

# ç¤ºä¾‹
conversation = template.format(
    system_prompt="You are a helpful assistant.",
    user_message="What is the capital of France?"
)

# Tokenize
tokens = tokenizer.encode(conversation)
```

### 3. é¢„å¤„ç†ç­–ç•¥

#### 3.1 Unicodeè§„èŒƒåŒ–

```python
import unicodedata

def normalize_text(text, form='NFKC'):
    """
    Unicodeè§„èŒƒåŒ–
    
    Forms:
    - NFC: Canonical Decomposition, followed by Canonical Composition
    - NFD: Canonical Decomposition
    - NFKC: Compatibility Decomposition, followed by Canonical Composition
    - NFKD: Compatibility Decomposition
    """
    return unicodedata.normalize(form, text)

# ç¤ºä¾‹
text1 = "cafÃ©"  # ä½¿ç”¨ç»„åˆå­—ç¬¦ Ã©
text2 = "cafÃ©"  # ä½¿ç”¨ e + é‡éŸ³ç¬¦

print(f"Text1 length: {len(text1)}")  # 4
print(f"Text2 length: {len(text2)}")  # 5

# è§„èŒƒåŒ–åç›¸åŒ
norm1 = normalize_text(text1, 'NFC')
norm2 = normalize_text(text2, 'NFC')
print(f"Normalized equal: {norm1 == norm2}")  # True
```

#### 3.2 å¤§å°å†™å¤„ç†

```python
# ç­–ç•¥1: ä¿ç•™åŸå§‹å¤§å°å†™ï¼ˆGPTç³»åˆ—ï¼‰
text = "Hello World"
tokens = tokenizer.encode(text)  # ["Hello", " World"]

# ç­–ç•¥2: è½¬æ¢ä¸ºå°å†™ï¼ˆBERT uncasedï¼‰
text = "Hello World"
text_lower = text.lower()
tokens = tokenizer.encode(text_lower)  # ["hello", " world"]

# ç­–ç•¥3: æ··åˆç­–ç•¥
# - ä¿ç•™ä¸“æœ‰åè¯çš„å¤§å°å†™
# - å…¶ä»–è¯è½¬å°å†™
def smart_lowercase(text):
    words = text.split()
    result = []
    for word in words:
        if is_proper_noun(word):
            result.append(word)
        else:
            result.append(word.lower())
    return ' '.join(result)
```

#### 3.3 ç©ºç™½å­—ç¬¦å¤„ç†

```python
import re

def normalize_whitespace(text):
    """è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦"""
    # 1. æ›¿æ¢å„ç§ç©ºç™½å­—ç¬¦ä¸ºæ ‡å‡†ç©ºæ ¼
    text = re.sub(r'[\t\n\r\f\v]', ' ', text)
    
    # 2. åˆå¹¶å¤šä¸ªè¿ç»­ç©ºæ ¼
    text = re.sub(r' +', ' ', text)
    
    # 3. å»é™¤é¦–å°¾ç©ºæ ¼
    text = text.strip()
    
    return text

# ç¤ºä¾‹
text = "Hello    world\n\nHow  are\tyou?"
normalized = normalize_whitespace(text)
print(f"Normalized: '{normalized}'")
# "Hello world How are you?"
```

#### 3.4 æ•°å­—å¤„ç†

```python
# ç­–ç•¥1: ä¿ç•™åŸå§‹æ•°å­—ï¼ˆGPTï¼‰
text = "The price is $123.45"
tokens = tokenizer.encode(text)
# ["The", " price", " is", " $", "123", ".", "45"]

# ç­–ç•¥2: æ•°å­—åˆ†ç¦»ï¼ˆæŸäº›tokenizerï¼‰
# "123" â†’ ["1", "2", "3"]

# ç­–ç•¥3: æ•°å­—è§„èŒƒåŒ–
def normalize_numbers(text):
    # å°†æ‰€æœ‰æ•°å­—æ›¿æ¢ä¸ºç‰¹æ®Štoken
    text = re.sub(r'\d+', '[NUM]', text)
    return text

text = "I have 3 apples and 5 oranges"
normalized = normalize_numbers(text)
# "I have [NUM] apples and [NUM] oranges"

# ç­–ç•¥4: æ•°å­—åˆ†ç»„
def group_digits(text):
    # æŒ‰ä½æ•°åˆ†ç»„
    def replace_number(match):
        num = match.group()
        if len(num) <= 2:
            return num
        else:
            # é•¿æ•°å­—åˆ†ç»„
            return ' '.join(num[i:i+2] for i in range(0, len(num), 2))
    
    return re.sub(r'\d+', replace_number, text)

text = "The year is 2024"
grouped = group_digits(text)
# "The year is 20 24"
```


---

## ç¬¬å…­éƒ¨åˆ†ï¼šå¤šè¯­è¨€Tokenization

### 1. å¤šè¯­è¨€çš„æŒ‘æˆ˜

#### 1.1 ä¸åŒè¯­è¨€çš„ç‰¹ç‚¹

```python
# è‹±è¯­ï¼šç©ºæ ¼åˆ†éš”ï¼Œå­—æ¯è¡¨å°
text_en = "Hello world"
# ç‰¹ç‚¹ï¼š26ä¸ªå­—æ¯ï¼Œæ˜ç¡®çš„è¯è¾¹ç•Œ

# ä¸­æ–‡ï¼šæ— ç©ºæ ¼ï¼Œå­—ç¬¦å¤š
text_zh = "ä½ å¥½ä¸–ç•Œ"
# ç‰¹ç‚¹ï¼šæ•°åƒä¸ªå¸¸ç”¨å­—ï¼Œæ— æ˜ç¡®è¯è¾¹ç•Œ

# æ—¥è¯­ï¼šæ··åˆæ–‡å­—ç³»ç»Ÿ
text_ja = "ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ"
# ç‰¹ç‚¹ï¼šå¹³å‡åã€ç‰‡å‡åã€æ±‰å­—æ··ç”¨

# é˜¿æ‹‰ä¼¯è¯­ï¼šä»å³åˆ°å·¦
text_ar = "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…"
# ç‰¹ç‚¹ï¼šRTLä¹¦å†™ï¼Œè¿å†™

# éŸ©è¯­ï¼šéŸ³èŠ‚æ–‡å­—
text_ko = "ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„"
# ç‰¹ç‚¹ï¼šéŸ³èŠ‚ç»„åˆï¼Œæœ‰ç©ºæ ¼
```

#### 1.2 è¯æ±‡è¡¨åˆ†é…ä¸å‡

**é—®é¢˜**ï¼š
```python
# åœ¨å¤šè¯­è¨€è¯­æ–™ä¸Šè®­ç»ƒtokenizer
corpus = {
    'english': 1000000,  # 100ä¸‡å¥
    'chinese': 100000,   # 10ä¸‡å¥
    'swahili': 10000,    # 1ä¸‡å¥
}

# è®­ç»ƒåçš„è¯æ±‡è¡¨åˆ†å¸ƒ
vocab_distribution = {
    'english': 25000,  # 78%çš„è¯æ±‡è¡¨
    'chinese': 6000,   # 19%
    'swahili': 1000,   # 3%
}

# ç»“æœï¼š
# - è‹±è¯­ï¼šé«˜æ•ˆç¼–ç ï¼ˆå¹³å‡4ä¸ªå­—ç¬¦/tokenï¼‰
# - ä¸­æ–‡ï¼šä¸­ç­‰æ•ˆç‡ï¼ˆå¹³å‡2ä¸ªå­—ç¬¦/tokenï¼‰
# - æ–¯ç“¦å¸Œé‡Œè¯­ï¼šä½æ•ˆï¼ˆæ¥è¿‘å­—ç¬¦çº§ï¼‰
```

**è§£å†³æ–¹æ¡ˆ1ï¼šè¯­è¨€é‡‡æ ·**
```python
def sample_corpus(corpus, target_distribution):
    """
    æŒ‰ç›®æ ‡åˆ†å¸ƒé‡‡æ ·è¯­æ–™
    """
    sampled = {}
    for lang, target_ratio in target_distribution.items():
        # è¿‡é‡‡æ ·ä½èµ„æºè¯­è¨€
        if lang in ['swahili', 'zulu', ...]:
            sample_ratio = target_ratio * 2
        else:
            sample_ratio = target_ratio
        
        sampled[lang] = corpus[lang][:int(len(corpus[lang]) * sample_ratio)]
    
    return sampled

# ä½¿ç”¨
target_dist = {
    'english': 0.4,   # 40%
    'chinese': 0.3,   # 30%
    'swahili': 0.3,   # 30%ï¼ˆè¿‡é‡‡æ ·ï¼‰
}

balanced_corpus = sample_corpus(corpus, target_dist)
```

**è§£å†³æ–¹æ¡ˆ2ï¼šå­—ç¬¦è¦†ç›–ç‡**
```python
# SentencePieceçš„character_coverageå‚æ•°
spm.SentencePieceTrainer.train(
    input='multilingual_corpus.txt',
    model_prefix='multilingual_tokenizer',
    vocab_size=250000,  # æ›´å¤§çš„è¯æ±‡è¡¨
    character_coverage=0.9995,  # è¦†ç›–99.95%çš„å­—ç¬¦
    # è¿™ä¼šç¡®ä¿ä½é¢‘å­—ç¬¦ä¹Ÿè¢«åŒ…å«
)
```

### 2. å¤šè¯­è¨€Tokenizerçš„æœ€ä½³å®è·µ

#### 2.1 ä½¿ç”¨SentencePiece

```python
import sentencepiece as spm

# è®­ç»ƒå¤šè¯­è¨€tokenizer
spm.SentencePieceTrainer.train(
    input='multilingual_corpus.txt',
    model_prefix='multilingual_sp',
    vocab_size=250000,
    character_coverage=0.9995,
    model_type='unigram',  # Unigramå¯¹å¤šè¯­è¨€æ›´å‹å¥½
    
    # å¤šè¯­è¨€è®¾ç½®
    split_by_unicode_script=True,  # æŒ‰Unicodeè„šæœ¬åˆ†å‰²
    split_by_whitespace=True,
    split_by_number=True,
    
    # å­—èŠ‚å›é€€
    byte_fallback=True,  # å¤„ç†æœªçŸ¥å­—ç¬¦
    
    # é‡‡æ ·
    input_sentence_size=10000000,  # ä½¿ç”¨1000ä¸‡å¥è®­ç»ƒ
    shuffle_input_sentence=True,
)
```

#### 2.2 è¯„ä¼°å¤šè¯­è¨€æ€§èƒ½

```python
def evaluate_multilingual_tokenizer(tokenizer, test_corpora):
    """
    è¯„ä¼°tokenizeråœ¨ä¸åŒè¯­è¨€ä¸Šçš„æ€§èƒ½
    """
    results = {}
    
    for lang, corpus in test_corpora.items():
        # 1. å‹ç¼©ç‡
        total_chars = sum(len(text) for text in corpus)
        total_tokens = sum(len(tokenizer.encode(text)) for text in corpus)
        compression_rate = total_chars / total_tokens
        
        # 2. æœªçŸ¥tokenæ¯”ä¾‹
        unk_count = 0
        for text in corpus:
            tokens = tokenizer.encode(text)
            unk_count += tokens.count(tokenizer.unk_id)
        unk_ratio = unk_count / total_tokens
        
        # 3. å¹³å‡tokené•¿åº¦
        avg_token_len = total_chars / total_tokens
        
        results[lang] = {
            'compression_rate': compression_rate,
            'unk_ratio': unk_ratio,
            'avg_token_length': avg_token_len,
        }
    
    return results

# ä½¿ç”¨
test_corpora = {
    'english': load_corpus('en'),
    'chinese': load_corpus('zh'),
    'arabic': load_corpus('ar'),
    'swahili': load_corpus('sw'),
}

results = evaluate_multilingual_tokenizer(tokenizer, test_corpora)

# æ‰“å°ç»“æœ
for lang, metrics in results.items():
    print(f"\n{lang}:")
    print(f"  Compression rate: {metrics['compression_rate']:.2f}")
    print(f"  UNK ratio: {metrics['unk_ratio']:.2%}")
    print(f"  Avg token length: {metrics['avg_token_length']:.2f}")

# æœŸæœ›ç»“æœï¼ˆç¤ºä¾‹ï¼‰ï¼š
"""
english:
  Compression rate: 4.2
  UNK ratio: 0.01%
  Avg token length: 4.2

chinese:
  Compression rate: 2.1
  UNK ratio: 0.05%
  Avg token length: 2.1

arabic:
  Compression rate: 3.5
  UNK ratio: 0.02%
  Avg token length: 3.5

swahili:
  Compression rate: 3.8
  UNK ratio: 0.10%
  Avg token length: 3.8
"""
```

### 3. ä»£ç å’Œç‰¹æ®Šé¢†åŸŸ

#### 3.1 ä»£ç Tokenization

**æŒ‘æˆ˜**ï¼š
```python
# ä»£ç æœ‰ç‰¹æ®Šçš„ç»“æ„
code = """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
"""

# éœ€è¦ä¿ç•™ï¼š
# - ç¼©è¿›ï¼ˆç©ºæ ¼/tabï¼‰
# - æ ‡è¯†ç¬¦ï¼ˆå˜é‡åã€å‡½æ•°åï¼‰
# - å…³é”®å­—ï¼ˆdef, if, returnï¼‰
# - è¿ç®—ç¬¦ï¼ˆ+, -, <=ï¼‰
# - æ‹¬å·å’Œæ ‡ç‚¹
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. è®­ç»ƒä»£ç ä¸“ç”¨tokenizer
spm.SentencePieceTrainer.train(
    input='code_corpus.txt',  # åŒ…å«å¤šç§ç¼–ç¨‹è¯­è¨€
    model_prefix='code_tokenizer',
    vocab_size=50000,
    
    # ä»£ç ç‰¹å®šè®¾ç½®
    split_by_whitespace=False,  # ä¿ç•™ç¼©è¿›
    treat_whitespace_as_suffix=False,
    
    # ä¿ç•™ç‰¹æ®Šå­—ç¬¦
    user_defined_symbols=[
        '==', '!=', '<=', '>=',  # è¿ç®—ç¬¦
        '->', '=>',              # ç®­å¤´
        '...', '...',            # çœç•¥å·
    ],
    
    # ä¸åˆ†å‰²æ•°å­—
    split_by_number=False,
)

# 2. ä½¿ç”¨ä¸“é—¨çš„ä»£ç tokenizerï¼ˆå¦‚CodeBERTï¼‰
from transformers import RobertaTokenizer

tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')
tokens = tokenizer.tokenize(code)
print(tokens)
```

#### 3.2 æ•°å­¦å…¬å¼

```python
# LaTeXå…¬å¼
formula = r"\frac{-b \pm \sqrt{b^2 - 4ac}}{2a}"

# æŒ‘æˆ˜ï¼š
# - ç‰¹æ®Šç¬¦å·ï¼š\, {, }, ^, _
# - å‘½ä»¤ï¼š\frac, \sqrt, \pm
# - éœ€è¦ä¿æŒç»“æ„

# è§£å†³æ–¹æ¡ˆï¼šæ·»åŠ æ•°å­¦ç¬¦å·åˆ°è¯æ±‡è¡¨
math_symbols = [
    r'\frac', r'\sqrt', r'\sum', r'\int',
    r'\alpha', r'\beta', r'\gamma',
    r'\pm', r'\times', r'\div',
]

tokenizer.add_tokens(math_symbols)
```

---

## ç¬¬ä¸ƒéƒ¨åˆ†ï¼šTokenizationçš„é™·é˜±å’Œæœ€ä½³å®è·µ

### 1. å¸¸è§é™·é˜±

#### 1.1 è®­ç»ƒ-æ¨ç†ä¸ä¸€è‡´

**é—®é¢˜**ï¼š
```python
# è®­ç»ƒæ—¶
train_tokenizer = load_tokenizer('v1.0')
train_data = tokenizer.encode(texts)

# å‡ ä¸ªæœˆåï¼Œæ¨ç†æ—¶
inference_tokenizer = load_tokenizer('v2.0')  # âŒ ç‰ˆæœ¬ä¸åŒï¼
test_data = tokenizer.encode(texts)

# ç»“æœï¼štoken IDsä¸åŒ¹é…ï¼Œæ¨¡å‹æ€§èƒ½ä¸‹é™
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. ç‰ˆæœ¬æ§åˆ¶
class TokenizerConfig:
    version = "1.0.0"
    vocab_size = 32000
    model_type = "bpe"
    # ... å…¶ä»–é…ç½®

# 2. ä¿å­˜tokenizerå’Œæ¨¡å‹ä¸€èµ·
model_dir = "my_model_v1"
tokenizer.save(f"{model_dir}/tokenizer")
model.save(f"{model_dir}/model")

# 3. åœ¨æ¨¡å‹é…ç½®ä¸­è®°å½•tokenizerä¿¡æ¯
config = {
    'model_name': 'my_gpt',
    'tokenizer_version': '1.0.0',
    'tokenizer_hash': compute_hash(tokenizer),
}
```

#### 1.2 ç‰¹æ®Šå­—ç¬¦å¤„ç†ä¸å½“

**é—®é¢˜**ï¼š
```python
# è¡¨æƒ…ç¬¦å·
text = "I love Python! ğŸ˜ğŸ"

# ä¸å½“å¤„ç†
tokens_bad = tokenizer.encode(text)
# å¯èƒ½ç»“æœï¼š["I", " love", " Python", "!", " ", "ï¿½", "ï¿½", "ï¿½", "ï¿½"]

# æ­£ç¡®å¤„ç†ï¼ˆå­—èŠ‚çº§BPEï¼‰
tokens_good = byte_level_tokenizer.encode(text)
# ["I", " love", " Python", "!", " ğŸ˜", "ğŸ"]
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# ä½¿ç”¨å­—èŠ‚çº§BPEæˆ–è®¾ç½®byte_fallback
spm.SentencePieceTrainer.train(
    input='corpus.txt',
    model_prefix='tokenizer',
    vocab_size=32000,
    byte_fallback=True,  # å…³é”®ï¼
    character_coverage=0.9995,
)
```

#### 1.3 ç©ºæ ¼å¤„ç†ä¸ä¸€è‡´

**é—®é¢˜**ï¼š
```python
# ä¸åŒçš„ç©ºæ ¼å¤„ç†
text1 = "Hello world"
text2 = "Hello  world"  # ä¸¤ä¸ªç©ºæ ¼

# æŸäº›tokenizer
tokens1 = tokenizer.encode(text1)  # ["Hello", " world"]
tokens2 = tokenizer.encode(text2)  # ["Hello", "  world"]  # ä¸åŒï¼

# è§£ç å
decoded1 = tokenizer.decode(tokens1)  # "Hello world"
decoded2 = tokenizer.decode(tokens2)  # "Hello  world"
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# é¢„å¤„ç†ï¼šè§„èŒƒåŒ–ç©ºæ ¼
def preprocess(text):
    # åˆå¹¶å¤šä¸ªç©ºæ ¼
    text = re.sub(r' +', ' ', text)
    return text

text = preprocess(text)
tokens = tokenizer.encode(text)
```

### 2. æœ€ä½³å®è·µ

#### 2.1 Tokenizerè®­ç»ƒæµç¨‹

```python
class TokenizerTrainingPipeline:
    def __init__(self, config):
        self.config = config
    
    def prepare_corpus(self, raw_files):
        """1. å‡†å¤‡è®­ç»ƒè¯­æ–™"""
        print("Step 1: Preparing corpus...")
        
        # åˆå¹¶æ–‡ä»¶
        corpus_file = 'training_corpus.txt'
        with open(corpus_file, 'w', encoding='utf-8') as outf:
            for file in raw_files:
                with open(file, 'r', encoding='utf-8') as inf:
                    for line in inf:
                        # æ¸…æ´—
                        line = self.clean_text(line)
                        if line.strip():
                            outf.write(line + '\n')
        
        return corpus_file
    
    def clean_text(self, text):
        """2. æ–‡æœ¬æ¸…æ´—"""
        # Unicodeè§„èŒƒåŒ–
        text = unicodedata.normalize('NFKC', text)
        
        # å»é™¤æ§åˆ¶å­—ç¬¦
        text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C' or ch in '\n\t')
        
        # è§„èŒƒåŒ–ç©ºç™½
        text = re.sub(r'[ \t]+', ' ', text)
        
        return text
    
    def train(self, corpus_file):
        """3. è®­ç»ƒtokenizer"""
        print("Step 2: Training tokenizer...")
        
        spm.SentencePieceTrainer.train(
            input=corpus_file,
            model_prefix=self.config['model_prefix'],
            vocab_size=self.config['vocab_size'],
            character_coverage=self.config['character_coverage'],
            model_type=self.config['model_type'],
            
            # ç‰¹æ®Štoken
            pad_id=0,
            unk_id=1,
            bos_id=2,
            eos_id=3,
            
            # è®­ç»ƒå‚æ•°
            num_threads=16,
            train_extremely_large_corpus=True,
            shuffle_input_sentence=True,
        )
    
    def evaluate(self, test_corpus):
        """4. è¯„ä¼°tokenizer"""
        print("Step 3: Evaluating tokenizer...")
        
        sp = spm.SentencePieceProcessor()
        sp.load(f"{self.config['model_prefix']}.model")
        
        metrics = {
            'compression_rate': [],
            'unk_ratio': [],
        }
        
        for text in test_corpus:
            tokens = sp.encode_as_ids(text)
            metrics['compression_rate'].append(len(text) / len(tokens))
            metrics['unk_ratio'].append(tokens.count(sp.unk_id()) / len(tokens))
        
        return {
            'avg_compression_rate': np.mean(metrics['compression_rate']),
            'avg_unk_ratio': np.mean(metrics['unk_ratio']),
        }
    
    def save_metadata(self):
        """5. ä¿å­˜å…ƒæ•°æ®"""
        print("Step 4: Saving metadata...")
        
        metadata = {
            'version': '1.0.0',
            'config': self.config,
            'training_date': datetime.now().isoformat(),
            'corpus_stats': self.get_corpus_stats(),
        }
        
        with open(f"{self.config['model_prefix']}_metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)

# ä½¿ç”¨
config = {
    'model_prefix': 'my_tokenizer',
    'vocab_size': 32000,
    'character_coverage': 0.9995,
    'model_type': 'bpe',
}

pipeline = TokenizerTrainingPipeline(config)
corpus_file = pipeline.prepare_corpus(raw_files)
pipeline.train(corpus_file)
metrics = pipeline.evaluate(test_corpus)
pipeline.save_metadata()

print(f"Training completed!")
print(f"Metrics: {metrics}")
```

#### 2.2 Tokenizeræµ‹è¯•

```python
import unittest

class TokenizerTests(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.tokenizer = load_tokenizer('my_tokenizer')
    
    def test_reversibility(self):
        """æµ‹è¯•å¯é€†æ€§"""
        texts = [
            "Hello, world!",
            "ä½ å¥½ï¼Œä¸–ç•Œï¼",
            "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…",
            "Hello ä¸–ç•Œ Ù…Ø±Ø­Ø¨Ø§",
        ]
        
        for text in texts:
            encoded = self.tokenizer.encode(text)
            decoded = self.tokenizer.decode(encoded)
            self.assertEqual(text, decoded, f"Failed for: {text}")
    
    def test_special_tokens(self):
        """æµ‹è¯•ç‰¹æ®Štoken"""
        # BOS/EOS
        text = "Hello"
        encoded = self.tokenizer.encode(text, add_special_tokens=True)
        self.assertEqual(encoded[0], self.tokenizer.bos_id)
        self.assertEqual(encoded[-1], self.tokenizer.eos_id)
        
        # PAD
        self.assertIsNotNone(self.tokenizer.pad_id)
    
    def test_empty_string(self):
        """æµ‹è¯•ç©ºå­—ç¬¦ä¸²"""
        encoded = self.tokenizer.encode("")
        self.assertEqual(len(encoded), 0)
    
    def test_long_text(self):
        """æµ‹è¯•é•¿æ–‡æœ¬"""
        long_text = "Hello " * 10000
        encoded = self.tokenizer.encode(long_text)
        decoded = self.tokenizer.decode(encoded)
        self.assertEqual(long_text, decoded)
    
    def test_special_characters(self):
        """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦"""
        special_texts = [
            "Hello\nWorld",  # æ¢è¡Œ
            "Hello\tWorld",  # Tab
            "HelloğŸ˜€World",  # Emoji
            "Hello\u200bWorld",  # é›¶å®½ç©ºæ ¼
        ]
        
        for text in special_texts:
            encoded = self.tokenizer.encode(text)
            decoded = self.tokenizer.decode(encoded)
            self.assertEqual(text, decoded)
    
    def test_consistency(self):
        """æµ‹è¯•ä¸€è‡´æ€§"""
        text = "Hello, world!"
        
        # å¤šæ¬¡ç¼–ç åº”è¯¥å¾—åˆ°ç›¸åŒç»“æœ
        encoded1 = self.tokenizer.encode(text)
        encoded2 = self.tokenizer.encode(text)
        self.assertEqual(encoded1, encoded2)

# è¿è¡Œæµ‹è¯•
if __name__ == '__main__':
    unittest.main()
```


---

## ç¬¬å…«éƒ¨åˆ†ï¼šå®æˆ˜æ¡ˆä¾‹åˆ†æ

### 1. GPT-2/GPT-3 Tokenizeræ·±åº¦è§£æ

#### 1.1 è®¾è®¡ç‰¹ç‚¹

```python
from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# å…³é”®ç‰¹æ€§
print(f"Vocab size: {tokenizer.vocab_size}")  # 50257
print(f"Model max length: {tokenizer.model_max_length}")  # 1024

# ç‰¹æ®Štoken
print(f"BOS token: {tokenizer.bos_token}")  # '<|endoftext|>'
print(f"EOS token: {tokenizer.eos_token}")  # '<|endoftext|>'
print(f"PAD token: {tokenizer.pad_token}")  # Noneï¼ˆGPT-2æ²¡æœ‰PADï¼‰
```

**å­—èŠ‚çº§BPEçš„å®ç°**ï¼š
```python
# GPT-2ä½¿ç”¨å­—èŠ‚çº§BPE
text = "Hello, ä¸–ç•Œ! ğŸ˜€"

# 1. è½¬æ¢ä¸ºå­—èŠ‚
bytes_text = text.encode('utf-8')
print(f"Bytes: {bytes_text}")
# b'Hello, \xe4\xb8\x96\xe7\x95\x8c! \xf0\x9f\x98\x80'

# 2. å­—èŠ‚åˆ°Unicodeæ˜ å°„
byte_encoder = tokenizer.byte_encoder
unicode_text = ''.join([byte_encoder[b] for b in bytes_text])
print(f"Unicode representation: {unicode_text}")

# 3. BPEåˆ†è¯
tokens = tokenizer.tokenize(text)
print(f"Tokens: {tokens}")
# ['Hello', ',', ' Ã¤Ä¼', 'Ä­', 'Ä·', 'Ã§Ä¥', 'Â¼', '!', ' Ã°ÅÄº', 'Ä¢']

# 4. Token IDs
ids = tokenizer.encode(text)
print(f"IDs: {ids}")
```

**ä¸ºä»€ä¹ˆè¯æ±‡è¡¨æ˜¯50257ï¼Ÿ**
```python
# 256ä¸ªå­—èŠ‚ + 50000ä¸ªåˆå¹¶ + 1ä¸ªç‰¹æ®Štoken
base_vocab = 256  # æ‰€æœ‰å¯èƒ½çš„å­—èŠ‚
num_merges = 50000  # BPEåˆå¹¶æ¬¡æ•°
special_tokens = 1  # <|endoftext|>

total = base_vocab + num_merges + special_tokens
print(f"Total vocab size: {total}")  # 50257
```

#### 1.2 å®é™…ä½¿ç”¨

```python
# ç¤ºä¾‹1ï¼šåŸºæœ¬ç¼–ç è§£ç 
text = "The quick brown fox jumps over the lazy dog."
tokens = tokenizer.tokenize(text)
ids = tokenizer.encode(text)
decoded = tokenizer.decode(ids)

print(f"Original: {text}")
print(f"Tokens: {tokens}")
print(f"IDs: {ids}")
print(f"Decoded: {decoded}")

# ç¤ºä¾‹2ï¼šæ‰¹å¤„ç†
texts = [
    "Hello, world!",
    "How are you?",
    "I'm fine, thank you."
]

# ç¼–ç ï¼ˆè‡ªåŠ¨å¡«å……ï¼‰
encoded = tokenizer(
    texts,
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors='pt'
)

print(f"Input IDs shape: {encoded['input_ids'].shape}")
print(f"Attention mask shape: {encoded['attention_mask'].shape}")

# ç¤ºä¾‹3ï¼šç”Ÿæˆä»»åŠ¡
prompt = "Once upon a time"
input_ids = tokenizer.encode(prompt, return_tensors='pt')

# ç”Ÿæˆï¼ˆå‡è®¾æœ‰æ¨¡å‹ï¼‰
# output_ids = model.generate(input_ids, max_length=50)
# generated_text = tokenizer.decode(output_ids[0])
```

### 2. BERT Tokenizeræ·±åº¦è§£æ

#### 2.1 WordPieceç‰¹ç‚¹

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# å…³é”®ç‰¹æ€§
print(f"Vocab size: {tokenizer.vocab_size}")  # 30522
print(f"Do lower case: {tokenizer.do_lower_case}")  # True

# ç‰¹æ®Štoken
print(f"CLS token: {tokenizer.cls_token}")  # '[CLS]'
print(f"SEP token: {tokenizer.sep_token}")  # '[SEP]'
print(f"PAD token: {tokenizer.pad_token}")  # '[PAD]'
print(f"MASK token: {tokenizer.mask_token}")  # '[MASK]'
```

**WordPieceçš„##å‰ç¼€**ï¼š
```python
# å­è¯ä½¿ç”¨##å‰ç¼€
text = "playing football"
tokens = tokenizer.tokenize(text)
print(f"Tokens: {tokens}")
# ['play', '##ing', 'football']

# å®Œæ•´çš„ç¼–ç ï¼ˆåŒ…å«ç‰¹æ®Štokenï¼‰
encoded = tokenizer.encode(text, add_special_tokens=True)
print(f"Encoded: {encoded}")
# [101, 2652, 2075, 2374, 102]
# 101: [CLS], 2652: play, 2075: ##ing, 2374: football, 102: [SEP]

# è§£ç 
decoded = tokenizer.decode(encoded)
print(f"Decoded: {decoded}")
# "[CLS] playing football [SEP]"
```

#### 2.2 BERTçš„ç‰¹æ®Šç”¨æ³•

```python
# 1. å•å¥åˆ†ç±»
text = "This movie is great!"
encoded = tokenizer(
    text,
    add_special_tokens=True,
    max_length=512,
    padding='max_length',
    truncation=True,
    return_tensors='pt'
)
# æ ¼å¼: [CLS] This movie is great ! [SEP] [PAD] [PAD] ...

# 2. å¥å­å¯¹ä»»åŠ¡ï¼ˆå¦‚NLIï¼‰
text_a = "The cat sat on the mat."
text_b = "A cat was sitting on a mat."

encoded = tokenizer(
    text_a,
    text_b,
    add_special_tokens=True,
    max_length=512,
    padding='max_length',
    truncation=True,
    return_tensors='pt'
)
# æ ¼å¼: [CLS] text_a [SEP] text_b [SEP] [PAD] ...

# token_type_idsåŒºåˆ†ä¸¤ä¸ªå¥å­
print(f"Token type IDs: {encoded['token_type_ids']}")
# [0, 0, 0, ..., 0, 1, 1, ..., 1, 0, 0, ...]
#  â†‘ text_a      â†‘ text_b      â†‘ padding

# 3. æ©ç è¯­è¨€æ¨¡å‹
text = "The cat sat on the [MASK]."
encoded = tokenizer.encode(text)
mask_token_id = tokenizer.mask_token_id

print(f"Encoded: {encoded}")
print(f"MASK token ID: {mask_token_id}")
# æ¨¡å‹é¢„æµ‹[MASK]ä½ç½®çš„token
```

### 3. Llama Tokenizeræ·±åº¦è§£æ

#### 3.1 SentencePiece BPE

```python
from transformers import LlamaTokenizer

tokenizer = LlamaTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')

# å…³é”®ç‰¹æ€§
print(f"Vocab size: {tokenizer.vocab_size}")  # 32000
print(f"BOS token: {tokenizer.bos_token}")  # '<s>'
print(f"EOS token: {tokenizer.eos_token}")  # '</s>'
print(f"UNK token: {tokenizer.unk_token}")  # '<unk>'
```

**SentencePieceçš„â–ç¬¦å·**ï¼š
```python
text = "Hello world"
tokens = tokenizer.tokenize(text)
print(f"Tokens: {tokens}")
# ['â–Hello', 'â–world']

# æ³¨æ„ï¼šâ–è¡¨ç¤ºåŸå§‹ç©ºæ ¼
text_no_space = "Helloworld"
tokens_no_space = tokenizer.tokenize(text_no_space)
print(f"Tokens (no space): {tokens_no_space}")
# ['â–Hello', 'world']  # ç¬¬äºŒä¸ªtokenæ²¡æœ‰â–
```

#### 3.2 Llama 2çš„Chatæ ¼å¼

```python
# Llama 2 Chatä½¿ç”¨ç‰¹æ®Šæ ¼å¼
B_INST, E_INST = "[INST]", "[/INST]"
B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"

def format_llama2_prompt(system_prompt, user_message):
    """æ ¼å¼åŒ–Llama 2 Chatæç¤º"""
    return f"<s>{B_INST} {B_SYS}{system_prompt}{E_SYS}{user_message} {E_INST}"

# ä½¿ç”¨
system_prompt = "You are a helpful, respectful and honest assistant."
user_message = "What is the capital of France?"

prompt = format_llama2_prompt(system_prompt, user_message)
print(prompt)

# Tokenize
tokens = tokenizer.encode(prompt)
print(f"Token count: {len(tokens)}")
```

---

## ç¬¬ä¹éƒ¨åˆ†ï¼šæ€§èƒ½ä¼˜åŒ–

### 1. ç¼–ç é€Ÿåº¦ä¼˜åŒ–

```python
import time
import numpy as np

def benchmark_tokenizer(tokenizer, texts, num_runs=100):
    """æµ‹è¯•tokenizeræ€§èƒ½"""
    
    # 1. å•ä¸ªæ–‡æœ¬ç¼–ç 
    start = time.time()
    for _ in range(num_runs):
        for text in texts:
            _ = tokenizer.encode(text)
    single_time = time.time() - start
    
    # 2. æ‰¹é‡ç¼–ç 
    start = time.time()
    for _ in range(num_runs):
        _ = tokenizer.batch_encode_plus(texts)
    batch_time = time.time() - start
    
    # 3. å¹¶è¡Œç¼–ç ï¼ˆå¦‚æœæ”¯æŒï¼‰
    if hasattr(tokenizer, 'enable_parallelism'):
        tokenizer.enable_parallelism(True)
        start = time.time()
        for _ in range(num_runs):
            _ = tokenizer.batch_encode_plus(texts)
        parallel_time = time.time() - start
    else:
        parallel_time = None
    
    return {
        'single': single_time,
        'batch': batch_time,
        'parallel': parallel_time,
        'speedup_batch': single_time / batch_time,
        'speedup_parallel': single_time / parallel_time if parallel_time else None,
    }

# æµ‹è¯•
texts = ["Hello world"] * 1000
results = benchmark_tokenizer(tokenizer, texts)

print("Performance Results:")
print(f"Single encoding: {results['single']:.3f}s")
print(f"Batch encoding: {results['batch']:.3f}s")
print(f"Speedup (batch): {results['speedup_batch']:.2f}x")
if results['parallel']:
    print(f"Parallel encoding: {results['parallel']:.3f}s")
    print(f"Speedup (parallel): {results['speedup_parallel']:.2f}x")
```

### 2. å†…å­˜ä¼˜åŒ–

```python
# 1. ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†å¤§æ–‡ä»¶
def tokenize_large_file(file_path, tokenizer, batch_size=1000):
    """æµå¼å¤„ç†å¤§æ–‡ä»¶"""
    def read_batches():
        batch = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                batch.append(line.strip())
                if len(batch) >= batch_size:
                    yield batch
                    batch = []
            if batch:
                yield batch
    
    for batch in read_batches():
        encoded = tokenizer.batch_encode_plus(
            batch,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors='pt'
        )
        # å¤„ç†encoded batch
        yield encoded

# 2. ç¼“å­˜å¸¸ç”¨ç¼–ç 
from functools import lru_cache

@lru_cache(maxsize=10000)
def cached_encode(text, tokenizer_name):
    """ç¼“å­˜ç¼–ç ç»“æœ"""
    tokenizer = load_tokenizer(tokenizer_name)
    return tuple(tokenizer.encode(text))

# 3. ä½¿ç”¨æ›´ç´§å‡‘çš„æ•°æ®ç±»å‹
import numpy as np

# é»˜è®¤ï¼šint64
ids_int64 = np.array(tokenizer.encode(text), dtype=np.int64)
print(f"Size (int64): {ids_int64.nbytes} bytes")

# ä¼˜åŒ–ï¼šint32ï¼ˆè¯æ±‡è¡¨<2^31ï¼‰
ids_int32 = np.array(tokenizer.encode(text), dtype=np.int32)
print(f"Size (int32): {ids_int32.nbytes} bytes")

# è¿›ä¸€æ­¥ä¼˜åŒ–ï¼šint16ï¼ˆè¯æ±‡è¡¨<2^15ï¼‰
if tokenizer.vocab_size < 32768:
    ids_int16 = np.array(tokenizer.encode(text), dtype=np.int16)
    print(f"Size (int16): {ids_int16.nbytes} bytes")
```

### 3. åˆ†å¸ƒå¼Tokenization

```python
from multiprocessing import Pool
import os

def tokenize_chunk(args):
    """å¤„ç†ä¸€ä¸ªæ•°æ®å—"""
    texts, tokenizer_path = args
    
    # åœ¨å­è¿›ç¨‹ä¸­åŠ è½½tokenizer
    tokenizer = load_tokenizer(tokenizer_path)
    
    results = []
    for text in texts:
        encoded = tokenizer.encode(text)
        results.append(encoded)
    
    return results

def parallel_tokenize(texts, tokenizer_path, num_workers=None):
    """å¹¶è¡Œtokenization"""
    if num_workers is None:
        num_workers = os.cpu_count()
    
    # åˆ†å‰²æ•°æ®
    chunk_size = len(texts) // num_workers
    chunks = [
        texts[i:i + chunk_size]
        for i in range(0, len(texts), chunk_size)
    ]
    
    # å¹¶è¡Œå¤„ç†
    with Pool(num_workers) as pool:
        args = [(chunk, tokenizer_path) for chunk in chunks]
        results = pool.map(tokenize_chunk, args)
    
    # åˆå¹¶ç»“æœ
    all_encoded = []
    for chunk_results in results:
        all_encoded.extend(chunk_results)
    
    return all_encoded

# ä½¿ç”¨
texts = load_large_corpus()
encoded = parallel_tokenize(texts, 'my_tokenizer.model', num_workers=8)
```

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹å›é¡¾

1. **Tokenizationæ˜¯LLMçš„ç¬¬ä¸€æ­¥**ï¼š
   - å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—åºåˆ—
   - å¹³è¡¡è¯æ±‡è¡¨å¤§å°å’Œåºåˆ—é•¿åº¦
   - å½±å“æ¨¡å‹çš„æ•ˆç‡å’Œèƒ½åŠ›

2. **ä¸‰ç§ä¸»æµç®—æ³•**ï¼š
   - **BPE**ï¼šç®€å•é«˜æ•ˆï¼Œè‡ªåº•å‘ä¸Šåˆå¹¶
   - **WordPiece**ï¼šåŸºäºä¼¼ç„¶åº¦ï¼ŒBERTä½¿ç”¨
   - **Unigram**ï¼šè‡ªé¡¶å‘ä¸‹åˆ é™¤ï¼Œçµæ´»é²æ£’

3. **SentencePieceæ˜¯ç”Ÿäº§æ ‡å‡†**ï¼š
   - è¯­è¨€æ— å…³
   - å®Œå…¨å¯é€†
   - æ”¯æŒå­è¯æ­£åˆ™åŒ–

4. **å…³é”®è®¾è®¡å†³ç­–**ï¼š
   - è¯æ±‡è¡¨å¤§å°ï¼š30k-50kæ˜¯å¹³è¡¡ç‚¹
   - ç‰¹æ®Štokenï¼šPAD, UNK, BOS, EOSç­‰
   - é¢„å¤„ç†ï¼šUnicodeè§„èŒƒåŒ–ã€ç©ºæ ¼å¤„ç†

5. **å¤šè¯­è¨€æŒ‘æˆ˜**ï¼š
   - è¯æ±‡è¡¨åˆ†é…ä¸å‡
   - éœ€è¦è¯­è¨€é‡‡æ ·å’Œå­—ç¬¦è¦†ç›–ç‡è°ƒæ•´
   - Unigramå¯¹å¤šè¯­è¨€æ›´å‹å¥½

6. **æœ€ä½³å®è·µ**ï¼š
   - ç‰ˆæœ¬æ§åˆ¶å’Œå…ƒæ•°æ®
   - å…¨é¢çš„æµ‹è¯•ï¼ˆå¯é€†æ€§ã€ç‰¹æ®Šå­—ç¬¦ï¼‰
   - æ€§èƒ½ä¼˜åŒ–ï¼ˆæ‰¹å¤„ç†ã€å¹¶è¡Œã€ç¼“å­˜ï¼‰

### å®è·µå»ºè®®

**é€‰æ‹©tokenizer**ï¼š
```
è‹±æ–‡ä¸ºä¸» â†’ BPE (GPTé£æ ¼)
å¤šä»»åŠ¡ â†’ WordPiece (BERTé£æ ¼)
å¤šè¯­è¨€ â†’ Unigram (T5é£æ ¼)
ç”Ÿäº§ç¯å¢ƒ â†’ SentencePiece
```

**è¯æ±‡è¡¨å¤§å°**ï¼š
```
å°æ¨¡å‹(<1B) â†’ 16k-32k
ä¸­ç­‰æ¨¡å‹(1B-10B) â†’ 32k-50k
å¤§æ¨¡å‹(>10B) â†’ 50k-100k
å¤šè¯­è¨€ â†’ 100k-256k
```

**è®­ç»ƒæµç¨‹**ï¼š
```
1. å‡†å¤‡å’Œæ¸…æ´—è¯­æ–™
2. é€‰æ‹©ç®—æ³•å’Œå‚æ•°
3. è®­ç»ƒtokenizer
4. è¯„ä¼°æ€§èƒ½ï¼ˆå‹ç¼©ç‡ã€è¦†ç›–ç‡ï¼‰
5. æµ‹è¯•è¾¹ç•Œæƒ…å†µ
6. ä¿å­˜å…ƒæ•°æ®å’Œç‰ˆæœ¬ä¿¡æ¯
```

### ä¸‹ä¸€æ­¥å­¦ä¹ 

**ç¬¬äºŒè®²é¢„å‘Š**ï¼šæ¨¡å‹æ¶æ„
- Transformerè¯¦è§£
- æ³¨æ„åŠ›æœºåˆ¶
- ä½ç½®ç¼–ç 
- å‰é¦ˆç½‘ç»œ

**æ¨èèµ„æº**ï¼š
- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) - BPEåŸå§‹è®ºæ–‡
- [SentencePiece: A simple and language independent approach](https://arxiv.org/abs/1808.06226) - SentencePieceè®ºæ–‡
- [Subword Regularization](https://arxiv.org/abs/1804.10959) - Unigramè®ºæ–‡
- [Hugging Face Tokenizersæ–‡æ¡£](https://huggingface.co/docs/tokenizers/)
- [SentencePiece GitHub](https://github.com/google/sentencepiece)

---

## é™„å½•ï¼šå¿«é€Ÿå‚è€ƒ

### A. å¸¸ç”¨ä»£ç ç‰‡æ®µ

**è®­ç»ƒBPE tokenizer**ï¼š
```python
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer

tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
trainer = BpeTrainer(vocab_size=30000, special_tokens=["[UNK]", "[PAD]", "[BOS]", "[EOS]"])
tokenizer.train(files=["corpus.txt"], trainer=trainer)
tokenizer.save("tokenizer.json")
```

**è®­ç»ƒSentencePiece**ï¼š
```python
import sentencepiece as spm

spm.SentencePieceTrainer.train(
    input='corpus.txt',
    model_prefix='sp_model',
    vocab_size=32000,
    model_type='bpe',
    character_coverage=0.9995
)
```

**ä½¿ç”¨Hugging Face tokenizer**ï¼š
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('gpt2')
encoded = tokenizer("Hello, world!", return_tensors='pt')
decoded = tokenizer.decode(encoded['input_ids'][0])
```

### B. è°ƒè¯•æ£€æŸ¥æ¸…å•

- [ ] å¯é€†æ€§ï¼š`text == tokenizer.decode(tokenizer.encode(text))`
- [ ] ç‰¹æ®Šå­—ç¬¦ï¼šæµ‹è¯•emojiã€Unicodeã€æ§åˆ¶å­—ç¬¦
- [ ] ç©ºå­—ç¬¦ä¸²ï¼š`tokenizer.encode("")`åº”è¯¥è¿”å›ç©ºæˆ–åªæœ‰ç‰¹æ®Štoken
- [ ] é•¿æ–‡æœ¬ï¼šæµ‹è¯•è¶…é•¿æ–‡æœ¬çš„å¤„ç†
- [ ] æ‰¹å¤„ç†ï¼šéªŒè¯æ‰¹å¤„ç†å’Œå•ä¸ªå¤„ç†ç»“æœä¸€è‡´
- [ ] å¤šè¯­è¨€ï¼šæµ‹è¯•ä¸åŒè¯­è¨€çš„ç¼–ç æ•ˆç‡
- [ ] ç‰ˆæœ¬ä¸€è‡´æ€§ï¼šç¡®ä¿è®­ç»ƒå’Œæ¨ç†ä½¿ç”¨ç›¸åŒç‰ˆæœ¬

### C. æ€§èƒ½åŸºå‡†

| æ“ä½œ | GPT-2 | BERT | Llama | å¤‡æ³¨ |
| :--- | :--- | :--- | :--- | :--- |
| ç¼–ç é€Ÿåº¦ | ~10k tokens/s | ~8k tokens/s | ~12k tokens/s | å•çº¿ç¨‹ |
| è§£ç é€Ÿåº¦ | ~15k tokens/s | ~12k tokens/s | ~18k tokens/s | å•çº¿ç¨‹ |
| å†…å­˜å ç”¨ | ~200MB | ~150MB | ~100MB | æ¨¡å‹æ–‡ä»¶ |
| åŠ è½½æ—¶é—´ | ~0.5s | ~0.3s | ~0.2s | é¦–æ¬¡åŠ è½½ |

å¸Œæœ›è¿™ä»½è¯¦ç»†çš„Tokenizationç¬”è®°èƒ½å¸®åŠ©ä½ æ·±å…¥ç†è§£å¤§æ¨¡å‹çš„ç¬¬ä¸€æ­¥ï¼
