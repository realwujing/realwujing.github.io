

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicon.jpg">
  <link rel="icon" href="/images/favicon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Wu Jing">
  <meta name="keywords" content="HTML, JavaScript, Hexo, Linux, qemu, C++, namespace, git, bcc, bpf, initramfs, k8s, architect, strings, assembly, linux">
  
    <meta name="description" content="大模型从0到1｜第二讲：PyTorch手把手搭建LLM  课程链接：Stanford CS336 Spring 2025 - Lecture 2   第一部分：语言模型基础 1. 什么是语言模型？ 定义：语言模型是一个概率分布，用于预测文本序列的可能性。 数学表示： 1P(w₁, w₂, ..., wₙ) &#x3D; P(w₁) × P(w₂|w₁) × P(w₃|w₁,w₂) × ... × P(wₙ|">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型从0到1｜第二讲：PyTorch手把手搭建LLM">
<meta property="og:url" content="https://realwujing.github.io/linux/drivers/gpu/stanford-cs336/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BB%8E0%E5%88%B01%EF%BD%9C%E7%AC%AC%E4%BA%8C%E8%AE%B2%EF%BC%9Apytorch%E6%89%8B%E6%8A%8A%E6%89%8B%E6%90%AD%E5%BB%BALLM/index.html">
<meta property="og:site_name" content="WuJing&#39;s Blog">
<meta property="og:description" content="大模型从0到1｜第二讲：PyTorch手把手搭建LLM  课程链接：Stanford CS336 Spring 2025 - Lecture 2   第一部分：语言模型基础 1. 什么是语言模型？ 定义：语言模型是一个概率分布，用于预测文本序列的可能性。 数学表示： 1P(w₁, w₂, ..., wₙ) &#x3D; P(w₁) × P(w₂|w₁) × P(w₃|w₁,w₂) × ... × P(wₙ|">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-11-22T23:23:05.000Z">
<meta property="article:modified_time" content="2025-11-22T23:23:05.000Z">
<meta property="article:author" content="Wu Jing">
<meta property="article:tag" content="git">
<meta property="article:tag" content="strings">
<meta name="twitter:card" content="summary_large_image">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>大模型从0到1｜第二讲：PyTorch手把手搭建LLM - WuJing&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"realwujing.github.io","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"6b5123e146041483d13bdfaeb6e42a76","google":"UA-265632133-1","gtag":"G-E7BV6T4RCW","tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?6b5123e146041483d13bdfaeb6e42a76";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  
    <!-- Google Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript('https://www.google-analytics.com/analytics.js', function() {
          window.ga = window.ga || function() { (ga.q = ga.q || []).push(arguments) };
          ga.l = +new Date;
          ga('create', 'UA-265632133-1', 'auto');
          ga('send', 'pageview');
        });
      }
    </script>
  

  
    <!-- Google gtag.js -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript('https://www.googletagmanager.com/gtag/js?id=G-E7BV6T4RCW', function() {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-E7BV6T4RCW');
        });
      }
    </script>
  

  

  

  

  



  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>WuJing&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="大模型从0到1｜第二讲：PyTorch手把手搭建LLM"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-11-22 23:23" pubdate>
          2025年11月22日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          34k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          281 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">大模型从0到1｜第二讲：PyTorch手把手搭建LLM</h1>
            
            
              <div class="markdown-body">
                
                <h1
id="大模型从0到1第二讲pytorch手把手搭建llm">大模型从0到1｜第二讲：PyTorch手把手搭建LLM</h1>
<blockquote>
<p>课程链接：<a
target="_blank" rel="noopener" href="https://stanford-cs336.github.io/spring2025-lectures/?trace=var/traces/lecture_02.json">Stanford
CS336 Spring 2025 - Lecture 2</a></p>
</blockquote>
<hr />
<h2 id="第一部分语言模型基础">第一部分：语言模型基础</h2>
<h3 id="什么是语言模型">1. 什么是语言模型？</h3>
<p><strong>定义</strong>：语言模型是一个概率分布，用于预测文本序列的可能性。</p>
<p><strong>数学表示</strong>： <figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-constructor">P(<span class="hljs-params">w</span>₁, <span class="hljs-params">w</span>₂, <span class="hljs-operator">...</span>, <span class="hljs-params">w</span>ₙ)</span> = <span class="hljs-constructor">P(<span class="hljs-params">w</span>₁)</span> × <span class="hljs-constructor">P(<span class="hljs-params">w</span>₂|<span class="hljs-params">w</span>₁)</span> × <span class="hljs-constructor">P(<span class="hljs-params">w</span>₃|<span class="hljs-params">w</span>₁,<span class="hljs-params">w</span>₂)</span> ×<span class="hljs-operator"> ... </span>× <span class="hljs-constructor">P(<span class="hljs-params">w</span>ₙ|<span class="hljs-params">w</span>₁,<span class="hljs-operator">...</span>,<span class="hljs-params">w</span>ₙ₋₁)</span><br></code></pre></td></tr></table></figure></p>
<p><strong>核心任务</strong>：给定前面的词（上下文），预测下一个词的概率分布。</p>
<p><strong>应用场景</strong>： - 文本生成（如ChatGPT） - 机器翻译 -
文本补全 - 问答系统 - 代码生成</p>
<h3 id="自回归语言模型autoregressive-language-model">2.
自回归语言模型（Autoregressive Language Model）</h3>
<p><strong>核心思想</strong>：从左到右，逐个生成Token，每次生成都依赖于之前生成的所有Token。</p>
<p><strong>生成过程</strong>： <figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">输入: <span class="hljs-string">&quot;The cat&quot;</span><br>步骤<span class="hljs-number">1</span>: <span class="hljs-constructor">P(<span class="hljs-params">sat</span> | The <span class="hljs-params">cat</span>)</span> → 生成 <span class="hljs-string">&quot;sat&quot;</span><br>步骤<span class="hljs-number">2</span>: <span class="hljs-constructor">P(<span class="hljs-params">on</span> | The <span class="hljs-params">cat</span> <span class="hljs-params">sat</span>)</span> → 生成 <span class="hljs-string">&quot;on&quot;</span><br>步骤<span class="hljs-number">3</span>: <span class="hljs-constructor">P(<span class="hljs-params">the</span> | The <span class="hljs-params">cat</span> <span class="hljs-params">sat</span> <span class="hljs-params">on</span>)</span> → 生成 <span class="hljs-string">&quot;the&quot;</span><br>步骤<span class="hljs-number">4</span>: <span class="hljs-constructor">P(<span class="hljs-params">mat</span> | The <span class="hljs-params">cat</span> <span class="hljs-params">sat</span> <span class="hljs-params">on</span> <span class="hljs-params">the</span>)</span> → 生成 <span class="hljs-string">&quot;mat&quot;</span><br></code></pre></td></tr></table></figure></p>
<p><strong>优点</strong>： - 生成质量高，符合语言的自然流畅性 -
训练简单，只需要大量文本数据 - 可以生成任意长度的文本</p>
<p><strong>代表模型</strong>：GPT系列、Llama、PaLM</p>
<hr />
<h2 id="第二部分transformer架构详解">第二部分：Transformer架构详解</h2>
<h3 id="transformer的整体结构">1. Transformer的整体结构</h3>
<p>Transformer是现代大语言模型的核心架构，由Google在2017年的论文"Attention
is All You Need"中提出。</p>
<p><strong>核心组件</strong>： 1. <strong>Token
Embedding</strong>：将Token ID转换为向量 2. <strong>Position
Embedding</strong>：为每个位置添加位置信息 3. <strong>Transformer
Blocks</strong>：多层堆叠的注意力和前馈网络 4. <strong>Layer
Normalization</strong>：归一化层 5. <strong>Output
Layer</strong>：预测下一个Token的概率分布</p>
<p><strong>架构图</strong>： <figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs mathematica">输入<span class="hljs-variable">Token</span> <span class="hljs-variable">IDs</span><br>    ↓<br><span class="hljs-variable">Token</span> <span class="hljs-variable">Embedding</span> <span class="hljs-operator">+</span> <span class="hljs-built_in">Position</span> <span class="hljs-variable">Embedding</span><br>    ↓<br><span class="hljs-variable">Transformer</span> <span class="hljs-built_in">Block</span> <span class="hljs-number">1</span><br>    ↓<br><span class="hljs-variable">Transformer</span> <span class="hljs-built_in">Block</span> <span class="hljs-number">2</span><br>    ↓<br>    <span class="hljs-operator">...</span><br>    ↓<br><span class="hljs-variable">Transformer</span> <span class="hljs-built_in">Block</span> <span class="hljs-built_in">N</span><br>    ↓<br><span class="hljs-variable">Layer</span> <span class="hljs-built_in">Norm</span><br>    ↓<br>输出层（预测下一个<span class="hljs-variable">Token</span>）<br></code></pre></td></tr></table></figure></p>
<h3 id="token-embedding词嵌入">2. Token Embedding（词嵌入）</h3>
<p><strong>作用</strong>：将离散的Token ID转换为连续的向量表示。</p>
<p><strong>实现</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TokenEmbedding</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, d_model</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.embedding = nn.Embedding(vocab_size, d_model)<br>        self.d_model = d_model<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># x: [batch_size, seq_len]</span><br>        <span class="hljs-comment"># 输出: [batch_size, seq_len, d_model]</span><br>        <span class="hljs-keyword">return</span> self.embedding(x) * (self.d_model ** <span class="hljs-number">0.5</span>)<br></code></pre></td></tr></table></figure></p>
<p><strong>参数说明</strong>： -
<code>vocab_size</code>：词汇表大小（如32000） -
<code>d_model</code>：嵌入维度（如512、768、1024等） - 乘以
<code>sqrt(d_model)</code> 是为了缩放，使得嵌入和位置编码的量级相当</p>
<p><strong>示例</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">vocab_size = <span class="hljs-number">32000</span><br>d_model = <span class="hljs-number">512</span><br>embedding = TokenEmbedding(vocab_size, d_model)<br><br><span class="hljs-comment"># 输入: [batch_size=2, seq_len=10]</span><br>input_ids = torch.randint(<span class="hljs-number">0</span>, vocab_size, (<span class="hljs-number">2</span>, <span class="hljs-number">10</span>))<br>output = embedding(input_ids)<br><span class="hljs-built_in">print</span>(output.shape)  <span class="hljs-comment"># torch.Size([2, 10, 512])</span><br></code></pre></td></tr></table></figure></p>
<h3 id="position-embedding位置编码">3. Position
Embedding（位置编码）</h3>
<p><strong>为什么需要位置编码？</strong> -
Transformer的自注意力机制本身是位置无关的 -
需要显式地告诉模型每个Token的位置信息 - "I love you" 和 "you love I"
应该有不同的表示</p>
<p><strong>两种主流方法</strong>：</p>
<h4 id="绝对位置编码absolute-position-encoding">3.1
绝对位置编码（Absolute Position Encoding）</h4>
<p><strong>正弦位置编码（Sinusoidal）</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SinusoidalPositionEncoding</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, max_len=<span class="hljs-number">5000</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 创建位置编码矩阵</span><br>        pe = torch.zeros(max_len, d_model)<br>        position = torch.arange(<span class="hljs-number">0</span>, max_len).unsqueeze(<span class="hljs-number">1</span>).<span class="hljs-built_in">float</span>()<br>        div_term = torch.exp(torch.arange(<span class="hljs-number">0</span>, d_model, <span class="hljs-number">2</span>).<span class="hljs-built_in">float</span>() * <br>                            -(math.log(<span class="hljs-number">10000.0</span>) / d_model))<br>        <br>        pe[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>] = torch.sin(position * div_term)<br>        pe[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>] = torch.cos(position * div_term)<br>        <br>        self.register_buffer(<span class="hljs-string">&#x27;pe&#x27;</span>, pe.unsqueeze(<span class="hljs-number">0</span>))<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># x: [batch_size, seq_len, d_model]</span><br>        <span class="hljs-keyword">return</span> x + self.pe[:, :x.size(<span class="hljs-number">1</span>)]<br></code></pre></td></tr></table></figure></p>
<p><strong>可学习位置编码（Learned Position Encoding）</strong>：
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LearnedPositionEncoding</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, max_len=<span class="hljs-number">5000</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.position_embedding = nn.Embedding(max_len, d_model)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># x: [batch_size, seq_len, d_model]</span><br>        batch_size, seq_len, _ = x.size()<br>        positions = torch.arange(seq_len, device=x.device).unsqueeze(<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> x + self.position_embedding(positions)<br></code></pre></td></tr></table></figure></p>
<p><strong>对比</strong>： | 方法 | 优点 | 缺点 | 使用模型 | | :--- |
:--- | :--- | :--- | | 正弦位置编码 | 可以外推到更长序列 |
固定，不可学习 | 原始Transformer | | 可学习位置编码 | 可以学习最优表示 |
无法外推到训练时未见过的长度 | GPT-2, BERT |</p>
<h4 id="相对位置编码relative-position-encoding">3.2
相对位置编码（Relative Position Encoding）</h4>
<p>现代模型（如GPT-3、Llama）更倾向于使用相对位置编码，在注意力计算中直接编码相对位置关系。</p>
<h3 id="self-attention自注意力机制">4.
Self-Attention（自注意力机制）</h3>
<p><strong>核心思想</strong>：让每个Token关注序列中的所有其他Token，学习它们之间的关系。</p>
<p><strong>数学公式</strong>： <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">Attention</span><span class="hljs-params">(Q, K, V)</span></span> = <span class="hljs-built_in">softmax</span>(QK^T / √d_k) V<br></code></pre></td></tr></table></figure></p>
<p><strong>三个关键矩阵</strong>： - <strong>Q
(Query)</strong>：查询矩阵，"我想要什么信息" - <strong>K
(Key)</strong>：键矩阵，"我有什么信息" - <strong>V
(Value)</strong>：值矩阵，"我的信息内容是什么"</p>
<p><strong>PyTorch实现</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SelfAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, d_k</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.d_k = d_k<br>        self.W_q = nn.Linear(d_model, d_k)<br>        self.W_k = nn.Linear(d_model, d_k)<br>        self.W_v = nn.Linear(d_model, d_k)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># x: [batch_size, seq_len, d_model]</span><br>        Q = self.W_q(x)  <span class="hljs-comment"># [batch_size, seq_len, d_k]</span><br>        K = self.W_k(x)  <span class="hljs-comment"># [batch_size, seq_len, d_k]</span><br>        V = self.W_v(x)  <span class="hljs-comment"># [batch_size, seq_len, d_k]</span><br>        <br>        <span class="hljs-comment"># 计算注意力分数</span><br>        scores = torch.matmul(Q, K.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) / (self.d_k ** <span class="hljs-number">0.5</span>)<br>        <span class="hljs-comment"># scores: [batch_size, seq_len, seq_len]</span><br>        <br>        <span class="hljs-comment"># 应用mask（用于因果注意力）</span><br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            scores = scores.masked_fill(mask == <span class="hljs-number">0</span>, <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;-inf&#x27;</span>))<br>        <br>        <span class="hljs-comment"># Softmax归一化</span><br>        attention_weights = torch.softmax(scores, dim=-<span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-comment"># 加权求和</span><br>        output = torch.matmul(attention_weights, V)<br>        <span class="hljs-comment"># output: [batch_size, seq_len, d_k]</span><br>        <br>        <span class="hljs-keyword">return</span> output, attention_weights<br></code></pre></td></tr></table></figure></p>
<p><strong>注意力可视化示例</strong>： <figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs avrasm">输入: <span class="hljs-string">&quot;The cat sat on the mat&quot;</span><br><br>Token <span class="hljs-string">&quot;sat&quot;</span> 的注意力权重:<br><span class="hljs-symbol">The:</span>  <span class="hljs-number">0.05</span><br><span class="hljs-symbol">cat:</span>  <span class="hljs-number">0.35</span>  ← 高权重，<span class="hljs-string">&quot;sat&quot;</span>关注主语<span class="hljs-string">&quot;cat&quot;</span><br><span class="hljs-symbol">sat:</span>  <span class="hljs-number">0.20</span><br><span class="hljs-symbol">on:</span>   <span class="hljs-number">0.10</span><br><span class="hljs-symbol">the:</span>  <span class="hljs-number">0.05</span><br><span class="hljs-symbol">mat:</span>  <span class="hljs-number">0.25</span>  ← 高权重，<span class="hljs-string">&quot;sat&quot;</span>关注宾语<span class="hljs-string">&quot;mat&quot;</span><br></code></pre></td></tr></table></figure></p>
<h3 id="causal-attention因果注意力掩码注意力">5. Causal
Attention（因果注意力/掩码注意力）</h3>
<p><strong>为什么需要因果注意力？</strong> -
在自回归语言模型中，生成第i个Token时，只能看到前i-1个Token -
不能"偷看"未来的Token，否则训练和推理不一致</p>
<p><strong>实现方式</strong>：使用下三角掩码矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_causal_mask</span>(<span class="hljs-params">seq_len</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;创建因果掩码矩阵&quot;&quot;&quot;</span><br>    mask = torch.tril(torch.ones(seq_len, seq_len))<br>    <span class="hljs-keyword">return</span> mask  <span class="hljs-comment"># 下三角矩阵，1表示可见，0表示不可见</span><br><br><span class="hljs-comment"># 示例</span><br>mask = create_causal_mask(<span class="hljs-number">5</span>)<br><span class="hljs-built_in">print</span>(mask)<br><span class="hljs-comment"># tensor([[1., 0., 0., 0., 0.],</span><br><span class="hljs-comment">#         [1., 1., 0., 0., 0.],</span><br><span class="hljs-comment">#         [1., 1., 1., 0., 0.],</span><br><span class="hljs-comment">#         [1., 1., 1., 1., 0.],</span><br><span class="hljs-comment">#         [1., 1., 1., 1., 1.]])</span><br></code></pre></td></tr></table></figure>
<p><strong>可视化</strong>： <figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">位置:  <span class="hljs-number">0</span>    <span class="hljs-number">1</span>    <span class="hljs-number">2</span>    <span class="hljs-number">3</span>    <span class="hljs-number">4</span><br>      [<span class="hljs-built_in">T0</span>] [<span class="hljs-built_in">T1</span>] [<span class="hljs-built_in">T2</span>] [<span class="hljs-built_in">T3</span>] [<span class="hljs-built_in">T4</span>]<br><br><span class="hljs-built_in">T0</span> 可以看到: <span class="hljs-built_in">T0</span><br><span class="hljs-built_in">T1</span> 可以看到: <span class="hljs-built_in">T0</span>, <span class="hljs-built_in">T1</span><br><span class="hljs-built_in">T2</span> 可以看到: <span class="hljs-built_in">T0</span>, <span class="hljs-built_in">T1</span>, <span class="hljs-built_in">T2</span><br><span class="hljs-built_in">T3</span> 可以看到: <span class="hljs-built_in">T0</span>, <span class="hljs-built_in">T1</span>, <span class="hljs-built_in">T2</span>, <span class="hljs-built_in">T3</span><br><span class="hljs-built_in">T4</span> 可以看到: <span class="hljs-built_in">T0</span>, <span class="hljs-built_in">T1</span>, <span class="hljs-built_in">T2</span>, <span class="hljs-built_in">T3</span>, <span class="hljs-built_in">T4</span><br></code></pre></td></tr></table></figure></p>
<h3 id="multi-head-attention多头注意力">6. Multi-Head
Attention（多头注意力）</h3>
<p><strong>核心思想</strong>：使用多个注意力头，让模型从不同的表示子空间学习信息。</p>
<p><strong>为什么需要多头？</strong> -
单个注意力头可能只关注某一种模式（如语法关系） -
多个头可以同时关注不同的模式（语法、语义、位置等） -
类似于CNN中的多个卷积核</p>
<p><strong>PyTorch实现</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, num_heads</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-keyword">assert</span> d_model % num_heads == <span class="hljs-number">0</span>, <span class="hljs-string">&quot;d_model必须能被num_heads整除&quot;</span><br>        <br>        self.d_model = d_model<br>        self.num_heads = num_heads<br>        self.d_k = d_model // num_heads<br>        <br>        <span class="hljs-comment"># 为所有头创建Q、K、V的投影矩阵</span><br>        self.W_q = nn.Linear(d_model, d_model)<br>        self.W_k = nn.Linear(d_model, d_model)<br>        self.W_v = nn.Linear(d_model, d_model)<br>        <br>        <span class="hljs-comment"># 输出投影</span><br>        self.W_o = nn.Linear(d_model, d_model)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, mask=<span class="hljs-literal">None</span></span>):<br>        batch_size, seq_len, d_model = x.size()<br>        <br>        <span class="hljs-comment"># 线性投影并分割成多个头</span><br>        <span class="hljs-comment"># [batch_size, seq_len, d_model] -&gt; [batch_size, num_heads, seq_len, d_k]</span><br>        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        <br>        <span class="hljs-comment"># 计算注意力</span><br>        scores = torch.matmul(Q, K.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) / (self.d_k ** <span class="hljs-number">0.5</span>)<br>        <br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            scores = scores.masked_fill(mask == <span class="hljs-number">0</span>, <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;-inf&#x27;</span>))<br>        <br>        attention_weights = torch.softmax(scores, dim=-<span class="hljs-number">1</span>)<br>        attention_output = torch.matmul(attention_weights, V)<br>        <br>        <span class="hljs-comment"># 合并多个头</span><br>        <span class="hljs-comment"># [batch_size, num_heads, seq_len, d_k] -&gt; [batch_size, seq_len, d_model]</span><br>        attention_output = attention_output.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous().view(<br>            batch_size, seq_len, d_model<br>        )<br>        <br>        <span class="hljs-comment"># 输出投影</span><br>        output = self.W_o(attention_output)<br>        <br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure></p>
<p><strong>参数配置示例</strong>： | 模型 | d_model | num_heads | d_k
(每个头) | | :--- | :--- | :--- | :--- | | GPT-2 Small | 768 | 12 | 64 |
| GPT-2 Medium | 1024 | 16 | 64 | | GPT-2 Large | 1280 | 20 | 64 | |
GPT-3 | 12288 | 96 | 128 |</p>
<h3 id="feed-forward-network前馈网络">7. Feed-Forward
Network（前馈网络）</h3>
<p><strong>作用</strong>：在注意力层之后，对每个位置独立地应用非线性变换。</p>
<p><strong>结构</strong>：两层全连接网络 +
激活函数（通常是GELU或ReLU）</p>
<p><strong>PyTorch实现</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FeedForward</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, d_ff, dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.linear1 = nn.Linear(d_model, d_ff)<br>        self.linear2 = nn.Linear(d_ff, d_model)<br>        self.dropout = nn.Dropout(dropout)<br>        self.activation = nn.GELU()  <span class="hljs-comment"># 或 nn.ReLU()</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># x: [batch_size, seq_len, d_model]</span><br>        x = self.linear1(x)           <span class="hljs-comment"># [batch_size, seq_len, d_ff]</span><br>        x = self.activation(x)<br>        x = self.dropout(x)<br>        x = self.linear2(x)           <span class="hljs-comment"># [batch_size, seq_len, d_model]</span><br>        x = self.dropout(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure></p>
<p><strong>参数说明</strong>： - <code>d_ff</code>（前馈维度）通常是
<code>d_model</code> 的4倍 - 例如：d_model=768, d_ff=3072</p>
<p><strong>为什么需要前馈网络？</strong> -
注意力机制是线性的（加权求和） - 前馈网络引入非线性，增强模型的表达能力
- 可以看作是对每个位置进行独立的特征变换</p>
<h3 id="layer-normalization层归一化">8. Layer
Normalization（层归一化）</h3>
<p><strong>作用</strong>：稳定训练，加速收敛。</p>
<p><strong>PyTorch实现</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LayerNorm</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, eps=<span class="hljs-number">1e-6</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.gamma = nn.Parameter(torch.ones(d_model))<br>        self.beta = nn.Parameter(torch.zeros(d_model))<br>        self.eps = eps<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># x: [batch_size, seq_len, d_model]</span><br>        mean = x.mean(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        std = x.std(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">return</span> self.gamma * (x - mean) / (std + self.eps) + self.beta<br></code></pre></td></tr></table></figure></p>
<p><strong>两种LayerNorm位置</strong>：</p>
<p><strong>Pre-LN（现代模型常用）</strong>： <figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-built_in">x</span> → LayerNorm → Attention → <span class="hljs-keyword">Add</span> → LayerNorm → FFN → <span class="hljs-keyword">Add</span><br></code></pre></td></tr></table></figure></p>
<p><strong>Post-LN（原始Transformer）</strong>： <figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">x → Attention → <span class="hljs-built_in">Add</span> → LayerNorm → FFN → <span class="hljs-built_in">Add</span> → LayerNorm<br></code></pre></td></tr></table></figure></p>
<p><strong>Pre-LN的优势</strong>： - 训练更稳定 -
不需要学习率预热（warmup） - 更容易训练深层网络</p>
<h3 id="residual-connection残差连接">9. Residual
Connection（残差连接）</h3>
<p><strong>作用</strong>：缓解梯度消失问题，使得深层网络更容易训练。</p>
<p><strong>实现</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 在Transformer Block中</span><br>x = x + attention(layer_norm(x))  <span class="hljs-comment"># 残差连接</span><br>x = x + ffn(layer_norm(x))        <span class="hljs-comment"># 残差连接</span><br></code></pre></td></tr></table></figure></p>
<p><strong>为什么有效？</strong> -
提供梯度的"高速公路"，直接传播到前面的层 -
允许模型学习"增量"而不是完整的变换 - 使得训练100层以上的模型成为可能</p>
<hr />
<h2 id="第三部分完整的transformer-block实现">第三部分：完整的Transformer
Block实现</h2>
<h3 id="单个transformer-block">1. 单个Transformer Block</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerBlock</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, num_heads, d_ff, dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <br>        <span class="hljs-comment"># 多头注意力</span><br>        self.attention = MultiHeadAttention(d_model, num_heads)<br>        <br>        <span class="hljs-comment"># 前馈网络</span><br>        self.ffn = FeedForward(d_model, d_ff, dropout)<br>        <br>        <span class="hljs-comment"># Layer Normalization</span><br>        self.ln1 = nn.LayerNorm(d_model)<br>        self.ln2 = nn.LayerNorm(d_model)<br>        <br>        <span class="hljs-comment"># Dropout</span><br>        self.dropout = nn.Dropout(dropout)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># Pre-LN架构</span><br>        <span class="hljs-comment"># 注意力子层</span><br>        attn_output = self.attention(self.ln1(x), mask)<br>        x = x + self.dropout(attn_output)  <span class="hljs-comment"># 残差连接</span><br>        <br>        <span class="hljs-comment"># 前馈子层</span><br>        ffn_output = self.ffn(self.ln2(x))<br>        x = x + self.dropout(ffn_output)   <span class="hljs-comment"># 残差连接</span><br>        <br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<h3 id="完整的gpt模型">2. 完整的GPT模型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GPT</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        vocab_size,      <span class="hljs-comment"># 词汇表大小</span></span><br><span class="hljs-params">        d_model,         <span class="hljs-comment"># 模型维度</span></span><br><span class="hljs-params">        num_layers,      <span class="hljs-comment"># Transformer层数</span></span><br><span class="hljs-params">        num_heads,       <span class="hljs-comment"># 注意力头数</span></span><br><span class="hljs-params">        d_ff,            <span class="hljs-comment"># 前馈网络维度</span></span><br><span class="hljs-params">        max_seq_len,     <span class="hljs-comment"># 最大序列长度</span></span><br><span class="hljs-params">        dropout=<span class="hljs-number">0.1</span></span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <br>        <span class="hljs-comment"># Token嵌入</span><br>        self.token_embedding = nn.Embedding(vocab_size, d_model)<br>        <br>        <span class="hljs-comment"># 位置嵌入</span><br>        self.position_embedding = nn.Embedding(max_seq_len, d_model)<br>        <br>        <span class="hljs-comment"># Transformer blocks</span><br>        self.blocks = nn.ModuleList([<br>            TransformerBlock(d_model, num_heads, d_ff, dropout)<br>            <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers)<br>        ])<br>        <br>        <span class="hljs-comment"># 最终的Layer Norm</span><br>        self.ln_f = nn.LayerNorm(d_model)<br>        <br>        <span class="hljs-comment"># 输出层（语言模型头）</span><br>        self.lm_head = nn.Linear(d_model, vocab_size, bias=<span class="hljs-literal">False</span>)<br>        <br>        <span class="hljs-comment"># 权重共享：输出层和嵌入层共享权重</span><br>        self.lm_head.weight = self.token_embedding.weight<br>        <br>        self.dropout = nn.Dropout(dropout)<br>        <br>        <span class="hljs-comment"># 初始化权重</span><br>        self.apply(self._init_weights)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_weights</span>(<span class="hljs-params">self, module</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, nn.Linear):<br>            torch.nn.init.normal_(module.weight, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)<br>            <span class="hljs-keyword">if</span> module.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                torch.nn.init.zeros_(module.bias)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(module, nn.Embedding):<br>            torch.nn.init.normal_(module.weight, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, input_ids</span>):<br>        batch_size, seq_len = input_ids.size()<br>        <br>        <span class="hljs-comment"># Token嵌入</span><br>        token_emb = self.token_embedding(input_ids)  <span class="hljs-comment"># [B, T, d_model]</span><br>        <br>        <span class="hljs-comment"># 位置嵌入</span><br>        positions = torch.arange(<span class="hljs-number">0</span>, seq_len, device=input_ids.device).unsqueeze(<span class="hljs-number">0</span>)<br>        pos_emb = self.position_embedding(positions)  <span class="hljs-comment"># [1, T, d_model]</span><br>        <br>        <span class="hljs-comment"># 组合嵌入</span><br>        x = self.dropout(token_emb + pos_emb)<br>        <br>        <span class="hljs-comment"># 创建因果掩码</span><br>        mask = torch.tril(torch.ones(seq_len, seq_len, device=input_ids.device))<br>        mask = mask.unsqueeze(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># [1, 1, T, T]</span><br>        <br>        <span class="hljs-comment"># 通过所有Transformer blocks</span><br>        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> self.blocks:<br>            x = block(x, mask)<br>        <br>        <span class="hljs-comment"># 最终的Layer Norm</span><br>        x = self.ln_f(x)<br>        <br>        <span class="hljs-comment"># 输出logits</span><br>        logits = self.lm_head(x)  <span class="hljs-comment"># [B, T, vocab_size]</span><br>        <br>        <span class="hljs-keyword">return</span> logits<br></code></pre></td></tr></table></figure>
<h3 id="模型配置示例">3. 模型配置示例</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># GPT-2 Small配置</span><br>config_small = &#123;<br>    <span class="hljs-string">&#x27;vocab_size&#x27;</span>: <span class="hljs-number">50257</span>,<br>    <span class="hljs-string">&#x27;d_model&#x27;</span>: <span class="hljs-number">768</span>,<br>    <span class="hljs-string">&#x27;num_layers&#x27;</span>: <span class="hljs-number">12</span>,<br>    <span class="hljs-string">&#x27;num_heads&#x27;</span>: <span class="hljs-number">12</span>,<br>    <span class="hljs-string">&#x27;d_ff&#x27;</span>: <span class="hljs-number">3072</span>,<br>    <span class="hljs-string">&#x27;max_seq_len&#x27;</span>: <span class="hljs-number">1024</span>,<br>    <span class="hljs-string">&#x27;dropout&#x27;</span>: <span class="hljs-number">0.1</span><br>&#125;<br><br><span class="hljs-comment"># GPT-2 Medium配置</span><br>config_medium = &#123;<br>    <span class="hljs-string">&#x27;vocab_size&#x27;</span>: <span class="hljs-number">50257</span>,<br>    <span class="hljs-string">&#x27;d_model&#x27;</span>: <span class="hljs-number">1024</span>,<br>    <span class="hljs-string">&#x27;num_layers&#x27;</span>: <span class="hljs-number">24</span>,<br>    <span class="hljs-string">&#x27;num_heads&#x27;</span>: <span class="hljs-number">16</span>,<br>    <span class="hljs-string">&#x27;d_ff&#x27;</span>: <span class="hljs-number">4096</span>,<br>    <span class="hljs-string">&#x27;max_seq_len&#x27;</span>: <span class="hljs-number">1024</span>,<br>    <span class="hljs-string">&#x27;dropout&#x27;</span>: <span class="hljs-number">0.1</span><br>&#125;<br><br><span class="hljs-comment"># 创建模型</span><br>model = GPT(**config_small)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;模型参数量: <span class="hljs-subst">&#123;<span class="hljs-built_in">sum</span>(p.numel() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters()) / <span class="hljs-number">1e6</span>:<span class="hljs-number">.2</span>f&#125;</span>M&quot;</span>)<br></code></pre></td></tr></table></figure>
<h3 id="模型使用示例">4. 模型使用示例</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建模型</span><br>model = GPT(<br>    vocab_size=<span class="hljs-number">32000</span>,<br>    d_model=<span class="hljs-number">512</span>,<br>    num_layers=<span class="hljs-number">6</span>,<br>    num_heads=<span class="hljs-number">8</span>,<br>    d_ff=<span class="hljs-number">2048</span>,<br>    max_seq_len=<span class="hljs-number">512</span><br>)<br><br><span class="hljs-comment"># 准备输入</span><br>input_ids = torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">32000</span>, (<span class="hljs-number">2</span>, <span class="hljs-number">10</span>))  <span class="hljs-comment"># [batch_size=2, seq_len=10]</span><br><br><span class="hljs-comment"># 前向传播</span><br>logits = model(input_ids)  <span class="hljs-comment"># [2, 10, 32000]</span><br><br><span class="hljs-comment"># 预测下一个token</span><br>next_token_logits = logits[:, -<span class="hljs-number">1</span>, :]  <span class="hljs-comment"># [2, 32000]</span><br>next_token_probs = torch.softmax(next_token_logits, dim=-<span class="hljs-number">1</span>)<br>next_token = torch.argmax(next_token_probs, dim=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># [2]</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;输入形状: <span class="hljs-subst">&#123;input_ids.shape&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;输出logits形状: <span class="hljs-subst">&#123;logits.shape&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;预测的下一个token: <span class="hljs-subst">&#123;next_token&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<hr />
<h2 id="第四部分训练语言模型">第四部分：训练语言模型</h2>
<h3 id="损失函数交叉熵损失">1. 损失函数：交叉熵损失</h3>
<p><strong>目标</strong>：最大化正确Token的概率，等价于最小化交叉熵损失。</p>
<p><strong>数学公式</strong>： <figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">Loss = -∑ log <span class="hljs-constructor">P(<span class="hljs-params">target_token</span> | <span class="hljs-params">context</span>)</span><br></code></pre></td></tr></table></figure></p>
<p><strong>PyTorch实现</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_loss</span>(<span class="hljs-params">logits, targets</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    logits: [batch_size, seq_len, vocab_size]</span><br><span class="hljs-string">    targets: [batch_size, seq_len]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 重塑为2D</span><br>    logits = logits.view(-<span class="hljs-number">1</span>, logits.size(-<span class="hljs-number">1</span>))  <span class="hljs-comment"># [B*T, vocab_size]</span><br>    targets = targets.view(-<span class="hljs-number">1</span>)                  <span class="hljs-comment"># [B*T]</span><br>    <br>    <span class="hljs-comment"># 计算交叉熵损失</span><br>    loss = F.cross_entropy(logits, targets)<br>    <br>    <span class="hljs-keyword">return</span> loss<br><br><span class="hljs-comment"># 使用示例</span><br>logits = model(input_ids)<br>targets = input_ids[:, <span class="hljs-number">1</span>:]  <span class="hljs-comment"># 目标是下一个token</span><br>logits = logits[:, :-<span class="hljs-number">1</span>, :]  <span class="hljs-comment"># 对齐维度</span><br><br>loss = compute_loss(logits, targets)<br></code></pre></td></tr></table></figure></p>
<h3 id="训练循环">2. 训练循环</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><br><span class="hljs-comment"># 初始化模型和优化器</span><br>model = GPT(**config_small)<br>optimizer = optim.AdamW(model.parameters(), lr=<span class="hljs-number">3e-4</span>, betas=(<span class="hljs-number">0.9</span>, <span class="hljs-number">0.95</span>))<br><br><span class="hljs-comment"># 训练循环</span><br>model.train()<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataloader:<br>        <span class="hljs-comment"># 获取输入和目标</span><br>        input_ids = batch[<span class="hljs-string">&#x27;input_ids&#x27;</span>]  <span class="hljs-comment"># [B, T]</span><br>        <br>        <span class="hljs-comment"># 前向传播</span><br>        logits = model(input_ids)<br>        <br>        <span class="hljs-comment"># 计算损失（预测下一个token）</span><br>        <span class="hljs-comment"># 输入: [0, 1, 2, 3, 4]</span><br>        <span class="hljs-comment"># 目标: [1, 2, 3, 4, 5]</span><br>        shift_logits = logits[:, :-<span class="hljs-number">1</span>, :].contiguous()<br>        shift_targets = input_ids[:, <span class="hljs-number">1</span>:].contiguous()<br>        <br>        loss = compute_loss(shift_logits, shift_targets)<br>        <br>        <span class="hljs-comment"># 反向传播</span><br>        optimizer.zero_grad()<br>        loss.backward()<br>        <br>        <span class="hljs-comment"># 梯度裁剪（防止梯度爆炸）</span><br>        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="hljs-number">1.0</span>)<br>        <br>        <span class="hljs-comment"># 更新参数</span><br>        optimizer.step()<br>        <br>        <span class="hljs-keyword">if</span> step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch&#125;</span>, Step <span class="hljs-subst">&#123;step&#125;</span>, Loss: <span class="hljs-subst">&#123;loss.item():<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<h3 id="学习率调度">3. 学习率调度</h3>
<p><strong>常用策略</strong>：Warmup + Cosine Decay</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.optim.lr_scheduler <span class="hljs-keyword">import</span> LambdaLR<br><span class="hljs-keyword">import</span> math<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_lr_scheduler</span>(<span class="hljs-params">optimizer, warmup_steps, max_steps</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">lr_lambda</span>(<span class="hljs-params">current_step</span>):<br>        <span class="hljs-keyword">if</span> current_step &lt; warmup_steps:<br>            <span class="hljs-comment"># Warmup阶段：线性增长</span><br>            <span class="hljs-keyword">return</span> <span class="hljs-built_in">float</span>(current_step) / <span class="hljs-built_in">float</span>(<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, warmup_steps))<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># Cosine decay阶段</span><br>            progress = <span class="hljs-built_in">float</span>(current_step - warmup_steps) / <span class="hljs-built_in">float</span>(<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, max_steps - warmup_steps))<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * (<span class="hljs-number">1.0</span> + math.cos(math.pi * progress))<br>    <br>    <span class="hljs-keyword">return</span> LambdaLR(optimizer, lr_lambda)<br><br><span class="hljs-comment"># 使用</span><br>scheduler = get_lr_scheduler(optimizer, warmup_steps=<span class="hljs-number">1000</span>, max_steps=<span class="hljs-number">100000</span>)<br><br><span class="hljs-comment"># 在训练循环中</span><br><span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_steps):<br>    <span class="hljs-comment"># ... 训练代码 ...</span><br>    optimizer.step()<br>    scheduler.step()<br></code></pre></td></tr></table></figure>
<p><strong>学习率曲线</strong>： <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs markdown">LR<br> ^<br> |     /\<br> |    /  \<span class="hljs-strong">___</span><br><span class="hljs-strong"> |   /       \__</span>_<br> |  /            \<span class="hljs-strong">___</span><br><span class="hljs-strong"> | /                 \__</span>_<br> |/<span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>&gt; Steps<br>   Warmup    Cosine Decay<br></code></pre></td></tr></table></figure></p>
<h3 id="数据准备">4. 数据准备</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TextDataset</span>(torch.utils.data.Dataset):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, text_file, tokenizer, max_length=<span class="hljs-number">512</span></span>):<br>        self.tokenizer = tokenizer<br>        self.max_length = max_length<br>        <br>        <span class="hljs-comment"># 读取并tokenize文本</span><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(text_file, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            text = f.read()<br>        <br>        self.tokens = tokenizer.encode(text)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.tokens) // self.max_length<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        start = idx * self.max_length<br>        end = start + self.max_length<br>        <br>        input_ids = torch.tensor(self.tokens[start:end], dtype=torch.long)<br>        <br>        <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&#x27;input_ids&#x27;</span>: input_ids&#125;<br><br><span class="hljs-comment"># 创建DataLoader</span><br>dataset = TextDataset(<span class="hljs-string">&#x27;corpus.txt&#x27;</span>, tokenizer, max_length=<span class="hljs-number">512</span>)<br>dataloader = torch.utils.data.DataLoader(<br>    dataset,<br>    batch_size=<span class="hljs-number">8</span>,<br>    shuffle=<span class="hljs-literal">True</span>,<br>    num_workers=<span class="hljs-number">4</span><br>)<br></code></pre></td></tr></table></figure>
<hr />
<h2 id="第五部分文本生成">第五部分：文本生成</h2>
<h3 id="贪婪解码greedy-decoding">1. 贪婪解码（Greedy Decoding）</h3>
<p><strong>策略</strong>：每次选择概率最高的Token。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@torch.no_grad()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_greedy</span>(<span class="hljs-params">model, input_ids, max_new_tokens=<span class="hljs-number">50</span></span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    <br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_new_tokens):<br>        <span class="hljs-comment"># 获取logits</span><br>        logits = model(input_ids)<br>        <br>        <span class="hljs-comment"># 只关注最后一个位置的预测</span><br>        next_token_logits = logits[:, -<span class="hljs-number">1</span>, :]<br>        <br>        <span class="hljs-comment"># 选择概率最高的token</span><br>        next_token = torch.argmax(next_token_logits, dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        <br>        <span class="hljs-comment"># 添加到序列</span><br>        input_ids = torch.cat([input_ids, next_token], dim=<span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-comment"># 如果生成了结束符，停止</span><br>        <span class="hljs-keyword">if</span> next_token.item() == eos_token_id:<br>            <span class="hljs-keyword">break</span><br>    <br>    <span class="hljs-keyword">return</span> input_ids<br><br><span class="hljs-comment"># 使用示例</span><br>prompt = <span class="hljs-string">&quot;Once upon a time&quot;</span><br>input_ids = tokenizer.encode(prompt, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br>output_ids = generate_greedy(model, input_ids, max_new_tokens=<span class="hljs-number">50</span>)<br>generated_text = tokenizer.decode(output_ids[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(generated_text)<br></code></pre></td></tr></table></figure>
<p><strong>优点</strong>：简单、快速、确定性
<strong>缺点</strong>：生成的文本可能重复、缺乏多样性</p>
<h3 id="采样sampling">2. 采样（Sampling）</h3>
<p><strong>策略</strong>：根据概率分布随机采样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@torch.no_grad()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_sample</span>(<span class="hljs-params">model, input_ids, max_new_tokens=<span class="hljs-number">50</span>, temperature=<span class="hljs-number">1.0</span></span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    <br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_new_tokens):<br>        logits = model(input_ids)<br>        next_token_logits = logits[:, -<span class="hljs-number">1</span>, :] / temperature<br>        <br>        <span class="hljs-comment"># 计算概率分布</span><br>        probs = torch.softmax(next_token_logits, dim=-<span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-comment"># 从分布中采样</span><br>        next_token = torch.multinomial(probs, num_samples=<span class="hljs-number">1</span>)<br>        <br>        input_ids = torch.cat([input_ids, next_token], dim=<span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-keyword">if</span> next_token.item() == eos_token_id:<br>            <span class="hljs-keyword">break</span><br>    <br>    <span class="hljs-keyword">return</span> input_ids<br></code></pre></td></tr></table></figure>
<p><strong>Temperature参数</strong>： -
<code>temperature &lt; 1.0</code>：分布更尖锐，更确定性 -
<code>temperature = 1.0</code>：原始分布 -
<code>temperature &gt; 1.0</code>：分布更平滑，更随机</p>
<h3 id="top-k-采样">3. Top-k 采样</h3>
<p><strong>策略</strong>：只从概率最高的k个Token中采样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@torch.no_grad()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_top_k</span>(<span class="hljs-params">model, input_ids, max_new_tokens=<span class="hljs-number">50</span>, top_k=<span class="hljs-number">50</span>, temperature=<span class="hljs-number">1.0</span></span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    <br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_new_tokens):<br>        logits = model(input_ids)<br>        next_token_logits = logits[:, -<span class="hljs-number">1</span>, :] / temperature<br>        <br>        <span class="hljs-comment"># 只保留top-k个最高的logits</span><br>        top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)<br>        <br>        <span class="hljs-comment"># 将其他位置设为-inf</span><br>        next_token_logits = torch.full_like(next_token_logits, <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;-inf&#x27;</span>))<br>        next_token_logits.scatter_(<span class="hljs-number">1</span>, top_k_indices, top_k_logits)<br>        <br>        <span class="hljs-comment"># 采样</span><br>        probs = torch.softmax(next_token_logits, dim=-<span class="hljs-number">1</span>)<br>        next_token = torch.multinomial(probs, num_samples=<span class="hljs-number">1</span>)<br>        <br>        input_ids = torch.cat([input_ids, next_token], dim=<span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-keyword">if</span> next_token.item() == eos_token_id:<br>            <span class="hljs-keyword">break</span><br>    <br>    <span class="hljs-keyword">return</span> input_ids<br></code></pre></td></tr></table></figure>
<h3 id="top-p-nucleus-采样">4. Top-p (Nucleus) 采样</h3>
<p><strong>策略</strong>：从累积概率达到p的最小Token集合中采样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@torch.no_grad()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_top_p</span>(<span class="hljs-params">model, input_ids, max_new_tokens=<span class="hljs-number">50</span>, top_p=<span class="hljs-number">0.9</span>, temperature=<span class="hljs-number">1.0</span></span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    <br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_new_tokens):<br>        logits = model(input_ids)<br>        next_token_logits = logits[:, -<span class="hljs-number">1</span>, :] / temperature<br>        <br>        <span class="hljs-comment"># 排序</span><br>        sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=<span class="hljs-literal">True</span>)<br>        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-<span class="hljs-number">1</span>), dim=-<span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-comment"># 移除累积概率超过top_p的token</span><br>        sorted_indices_to_remove = cumulative_probs &gt; top_p<br>        sorted_indices_to_remove[..., <span class="hljs-number">1</span>:] = sorted_indices_to_remove[..., :-<span class="hljs-number">1</span>].clone()<br>        sorted_indices_to_remove[..., <span class="hljs-number">0</span>] = <span class="hljs-number">0</span><br>        <br>        indices_to_remove = sorted_indices_to_remove.scatter(<span class="hljs-number">1</span>, sorted_indices, sorted_indices_to_remove)<br>        next_token_logits[indices_to_remove] = <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;-inf&#x27;</span>)<br>        <br>        <span class="hljs-comment"># 采样</span><br>        probs = torch.softmax(next_token_logits, dim=-<span class="hljs-number">1</span>)<br>        next_token = torch.multinomial(probs, num_samples=<span class="hljs-number">1</span>)<br>        <br>        input_ids = torch.cat([input_ids, next_token], dim=<span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-keyword">if</span> next_token.item() == eos_token_id:<br>            <span class="hljs-keyword">break</span><br>    <br>    <span class="hljs-keyword">return</span> input_ids<br></code></pre></td></tr></table></figure>
<h3 id="beam-search束搜索">5. Beam Search（束搜索）</h3>
<p><strong>策略</strong>：维护k个最可能的序列，每次扩展所有候选。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@torch.no_grad()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_beam_search</span>(<span class="hljs-params">model, input_ids, max_new_tokens=<span class="hljs-number">50</span>, num_beams=<span class="hljs-number">5</span></span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    batch_size = input_ids.size(<span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-comment"># 初始化beam</span><br>    <span class="hljs-comment"># 每个beam: (sequence, score)</span><br>    beams = [(input_ids[<span class="hljs-number">0</span>], <span class="hljs-number">0.0</span>)]<br>    <br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_new_tokens):<br>        all_candidates = []<br>        <br>        <span class="hljs-keyword">for</span> seq, score <span class="hljs-keyword">in</span> beams:<br>            <span class="hljs-keyword">if</span> seq[-<span class="hljs-number">1</span>].item() == eos_token_id:<br>                all_candidates.append((seq, score))<br>                <span class="hljs-keyword">continue</span><br>            <br>            <span class="hljs-comment"># 获取下一个token的概率</span><br>            logits = model(seq.unsqueeze(<span class="hljs-number">0</span>))<br>            next_token_logits = logits[<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, :]<br>            log_probs = torch.log_softmax(next_token_logits, dim=-<span class="hljs-number">1</span>)<br>            <br>            <span class="hljs-comment"># 获取top-k个候选</span><br>            top_log_probs, top_indices = torch.topk(log_probs, num_beams)<br>            <br>            <span class="hljs-keyword">for</span> log_prob, token_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(top_log_probs, top_indices):<br>                new_seq = torch.cat([seq, token_id.unsqueeze(<span class="hljs-number">0</span>)])<br>                new_score = score + log_prob.item()<br>                all_candidates.append((new_seq, new_score))<br>        <br>        <span class="hljs-comment"># 选择得分最高的num_beams个序列</span><br>        beams = <span class="hljs-built_in">sorted</span>(all_candidates, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)[:num_beams]<br>        <br>        <span class="hljs-comment"># 如果所有beam都结束了，停止</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">all</span>(seq[-<span class="hljs-number">1</span>].item() == eos_token_id <span class="hljs-keyword">for</span> seq, _ <span class="hljs-keyword">in</span> beams):<br>            <span class="hljs-keyword">break</span><br>    <br>    <span class="hljs-comment"># 返回得分最高的序列</span><br>    best_seq, best_score = beams[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">return</span> best_seq.unsqueeze(<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>
<h3 id="生成策略对比">6. 生成策略对比</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">策略</th>
<th style="text-align: left;">优点</th>
<th style="text-align: left;">缺点</th>
<th style="text-align: left;">适用场景</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Greedy</strong></td>
<td style="text-align: left;">快速、确定性</td>
<td style="text-align: left;">容易重复、缺乏多样性</td>
<td style="text-align: left;">需要确定性输出</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Sampling</strong></td>
<td style="text-align: left;">多样性好</td>
<td style="text-align: left;">可能不连贯</td>
<td style="text-align: left;">创意写作</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Top-k</strong></td>
<td style="text-align: left;">平衡质量和多样性</td>
<td style="text-align: left;">k值难以调整</td>
<td style="text-align: left;">通用文本生成</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Top-p</strong></td>
<td style="text-align: left;">动态调整候选集</td>
<td style="text-align: left;">计算稍慢</td>
<td style="text-align: left;">高质量生成（推荐）</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Beam Search</strong></td>
<td style="text-align: left;">全局最优</td>
<td style="text-align: left;">慢、缺乏多样性</td>
<td style="text-align: left;">翻译、摘要</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="第六部分优化技巧和最佳实践">第六部分：优化技巧和最佳实践</h2>
<h3 id="权重初始化">1. 权重初始化</h3>
<p><strong>重要性</strong>：好的初始化可以加速收敛，避免梯度消失/爆炸。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_weights</span>(<span class="hljs-params">self, module</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, nn.Linear):<br>        <span class="hljs-comment"># Xavier/Glorot初始化</span><br>        torch.nn.init.normal_(module.weight, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)<br>        <span class="hljs-keyword">if</span> module.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            torch.nn.init.zeros_(module.bias)<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(module, nn.Embedding):<br>        torch.nn.init.normal_(module.weight, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(module, nn.LayerNorm):<br>        torch.nn.init.ones_(module.weight)<br>        torch.nn.init.zeros_(module.bias)<br></code></pre></td></tr></table></figure>
<h3 id="梯度裁剪">2. 梯度裁剪</h3>
<p><strong>作用</strong>：防止梯度爆炸。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 在optimizer.step()之前</span><br>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="hljs-number">1.0</span>)<br></code></pre></td></tr></table></figure>
<h3 id="混合精度训练">3. 混合精度训练</h3>
<p><strong>作用</strong>：加速训练，减少显存占用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.cuda.amp <span class="hljs-keyword">import</span> autocast, GradScaler<br><br>scaler = GradScaler()<br><br><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataloader:<br>    optimizer.zero_grad()<br>    <br>    <span class="hljs-comment"># 使用混合精度</span><br>    <span class="hljs-keyword">with</span> autocast():<br>        logits = model(input_ids)<br>        loss = compute_loss(logits, targets)<br>    <br>    <span class="hljs-comment"># 缩放损失并反向传播</span><br>    scaler.scale(loss).backward()<br>    <br>    <span class="hljs-comment"># 梯度裁剪</span><br>    scaler.unscale_(optimizer)<br>    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="hljs-number">1.0</span>)<br>    <br>    <span class="hljs-comment"># 更新参数</span><br>    scaler.step(optimizer)<br>    scaler.update()<br></code></pre></td></tr></table></figure>
<h3 id="梯度累积">4. 梯度累积</h3>
<p><strong>作用</strong>：在显存有限时模拟大batch size。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">accumulation_steps = <span class="hljs-number">4</span>  <span class="hljs-comment"># 累积4个batch</span><br><br><span class="hljs-keyword">for</span> i, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dataloader):<br>    logits = model(input_ids)<br>    loss = compute_loss(logits, targets)<br>    <br>    <span class="hljs-comment"># 归一化损失</span><br>    loss = loss / accumulation_steps<br>    loss.backward()<br>    <br>    <span class="hljs-comment"># 每accumulation_steps步更新一次</span><br>    <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % accumulation_steps == <span class="hljs-number">0</span>:<br>        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="hljs-number">1.0</span>)<br>        optimizer.step()<br>        optimizer.zero_grad()<br></code></pre></td></tr></table></figure>
<h3 id="权重共享weight-tying">5. 权重共享（Weight Tying）</h3>
<p><strong>策略</strong>：输入嵌入层和输出层共享权重。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 在模型初始化中</span><br>self.token_embedding = nn.Embedding(vocab_size, d_model)<br>self.lm_head = nn.Linear(d_model, vocab_size, bias=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># 共享权重</span><br>self.lm_head.weight = self.token_embedding.weight<br></code></pre></td></tr></table></figure>
<p><strong>优点</strong>： - 减少参数量（约减少vocab_size ×
d_model个参数） - 提高训练效率 - 通常不损失性能</p>
<h3 id="dropout策略">6. Dropout策略</h3>
<p><strong>位置</strong>： - 嵌入层之后 - 注意力输出之后 -
前馈网络之后</p>
<p><strong>推荐值</strong>： - 小模型：0.1 -
大模型：0.0-0.1（大模型通常不需要太多dropout）</p>
<h3 id="模型保存和加载">7. 模型保存和加载</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 保存模型</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">save_checkpoint</span>(<span class="hljs-params">model, optimizer, epoch, loss, path</span>):<br>    checkpoint = &#123;<br>        <span class="hljs-string">&#x27;epoch&#x27;</span>: epoch,<br>        <span class="hljs-string">&#x27;model_state_dict&#x27;</span>: model.state_dict(),<br>        <span class="hljs-string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),<br>        <span class="hljs-string">&#x27;loss&#x27;</span>: loss,<br>    &#125;<br>    torch.save(checkpoint, path)<br><br><span class="hljs-comment"># 加载模型</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_checkpoint</span>(<span class="hljs-params">model, optimizer, path</span>):<br>    checkpoint = torch.load(path)<br>    model.load_state_dict(checkpoint[<span class="hljs-string">&#x27;model_state_dict&#x27;</span>])<br>    optimizer.load_state_dict(checkpoint[<span class="hljs-string">&#x27;optimizer_state_dict&#x27;</span>])<br>    epoch = checkpoint[<span class="hljs-string">&#x27;epoch&#x27;</span>]<br>    loss = checkpoint[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>    <span class="hljs-keyword">return</span> epoch, loss<br><br><span class="hljs-comment"># 使用</span><br>save_checkpoint(model, optimizer, epoch, loss, <span class="hljs-string">&#x27;checkpoint.pt&#x27;</span>)<br>epoch, loss = load_checkpoint(model, optimizer, <span class="hljs-string">&#x27;checkpoint.pt&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h3 id="评估指标困惑度perplexity">8. 评估指标：困惑度（Perplexity）</h3>
<p><strong>定义</strong>：衡量模型预测的不确定性，越低越好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_perplexity</span>(<span class="hljs-params">model, dataloader</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    total_loss = <span class="hljs-number">0</span><br>    total_tokens = <span class="hljs-number">0</span><br>    <br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataloader:<br>            input_ids = batch[<span class="hljs-string">&#x27;input_ids&#x27;</span>]<br>            logits = model(input_ids)<br>            <br>            shift_logits = logits[:, :-<span class="hljs-number">1</span>, :].contiguous()<br>            shift_targets = input_ids[:, <span class="hljs-number">1</span>:].contiguous()<br>            <br>            loss = compute_loss(shift_logits, shift_targets)<br>            <br>            total_loss += loss.item() * shift_targets.numel()<br>            total_tokens += shift_targets.numel()<br>    <br>    avg_loss = total_loss / total_tokens<br>    perplexity = math.exp(avg_loss)<br>    <br>    <span class="hljs-keyword">return</span> perplexity<br><br><span class="hljs-comment"># 使用</span><br>ppl = compute_perplexity(model, val_dataloader)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Perplexity: <span class="hljs-subst">&#123;ppl:<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<hr />
<h2 id="第七部分完整训练示例">第七部分：完整训练示例</h2>
<h3 id="完整的训练脚本">完整的训练脚本</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">import</span> math<br><br><span class="hljs-comment"># 1. 配置</span><br>config = &#123;<br>    <span class="hljs-string">&#x27;vocab_size&#x27;</span>: <span class="hljs-number">32000</span>,<br>    <span class="hljs-string">&#x27;d_model&#x27;</span>: <span class="hljs-number">512</span>,<br>    <span class="hljs-string">&#x27;num_layers&#x27;</span>: <span class="hljs-number">6</span>,<br>    <span class="hljs-string">&#x27;num_heads&#x27;</span>: <span class="hljs-number">8</span>,<br>    <span class="hljs-string">&#x27;d_ff&#x27;</span>: <span class="hljs-number">2048</span>,<br>    <span class="hljs-string">&#x27;max_seq_len&#x27;</span>: <span class="hljs-number">512</span>,<br>    <span class="hljs-string">&#x27;dropout&#x27;</span>: <span class="hljs-number">0.1</span>,<br>    <span class="hljs-string">&#x27;batch_size&#x27;</span>: <span class="hljs-number">32</span>,<br>    <span class="hljs-string">&#x27;num_epochs&#x27;</span>: <span class="hljs-number">10</span>,<br>    <span class="hljs-string">&#x27;learning_rate&#x27;</span>: <span class="hljs-number">3e-4</span>,<br>    <span class="hljs-string">&#x27;warmup_steps&#x27;</span>: <span class="hljs-number">1000</span>,<br>    <span class="hljs-string">&#x27;max_steps&#x27;</span>: <span class="hljs-number">100000</span>,<br>&#125;<br><br><span class="hljs-comment"># 2. 创建模型</span><br>model = GPT(<br>    vocab_size=config[<span class="hljs-string">&#x27;vocab_size&#x27;</span>],<br>    d_model=config[<span class="hljs-string">&#x27;d_model&#x27;</span>],<br>    num_layers=config[<span class="hljs-string">&#x27;num_layers&#x27;</span>],<br>    num_heads=config[<span class="hljs-string">&#x27;num_heads&#x27;</span>],<br>    d_ff=config[<span class="hljs-string">&#x27;d_ff&#x27;</span>],<br>    max_seq_len=config[<span class="hljs-string">&#x27;max_seq_len&#x27;</span>],<br>    dropout=config[<span class="hljs-string">&#x27;dropout&#x27;</span>]<br>)<br><br><span class="hljs-comment"># 移到GPU</span><br>device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)<br>model = model.to(device)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;模型参数量: <span class="hljs-subst">&#123;<span class="hljs-built_in">sum</span>(p.numel() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters()) / <span class="hljs-number">1e6</span>:<span class="hljs-number">.2</span>f&#125;</span>M&quot;</span>)<br><br><span class="hljs-comment"># 3. 准备数据</span><br>train_dataset = TextDataset(<span class="hljs-string">&#x27;train.txt&#x27;</span>, tokenizer, max_length=config[<span class="hljs-string">&#x27;max_seq_len&#x27;</span>])<br>val_dataset = TextDataset(<span class="hljs-string">&#x27;val.txt&#x27;</span>, tokenizer, max_length=config[<span class="hljs-string">&#x27;max_seq_len&#x27;</span>])<br><br>train_loader = DataLoader(<br>    train_dataset,<br>    batch_size=config[<span class="hljs-string">&#x27;batch_size&#x27;</span>],<br>    shuffle=<span class="hljs-literal">True</span>,<br>    num_workers=<span class="hljs-number">4</span><br>)<br><br>val_loader = DataLoader(<br>    val_dataset,<br>    batch_size=config[<span class="hljs-string">&#x27;batch_size&#x27;</span>],<br>    shuffle=<span class="hljs-literal">False</span>,<br>    num_workers=<span class="hljs-number">4</span><br>)<br><br><span class="hljs-comment"># 4. 优化器和调度器</span><br>optimizer = optim.AdamW(<br>    model.parameters(),<br>    lr=config[<span class="hljs-string">&#x27;learning_rate&#x27;</span>],<br>    betas=(<span class="hljs-number">0.9</span>, <span class="hljs-number">0.95</span>),<br>    weight_decay=<span class="hljs-number">0.1</span><br>)<br><br>scheduler = get_lr_scheduler(<br>    optimizer,<br>    warmup_steps=config[<span class="hljs-string">&#x27;warmup_steps&#x27;</span>],<br>    max_steps=config[<span class="hljs-string">&#x27;max_steps&#x27;</span>]<br>)<br><br><span class="hljs-comment"># 5. 训练循环</span><br>global_step = <span class="hljs-number">0</span><br>best_val_loss = <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config[<span class="hljs-string">&#x27;num_epochs&#x27;</span>]):<br>    <span class="hljs-comment"># 训练阶段</span><br>    model.train()<br>    train_loss = <span class="hljs-number">0</span><br>    <br>    <span class="hljs-keyword">for</span> batch_idx, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        input_ids = batch[<span class="hljs-string">&#x27;input_ids&#x27;</span>].to(device)<br>        <br>        <span class="hljs-comment"># 前向传播</span><br>        logits = model(input_ids)<br>        <br>        <span class="hljs-comment"># 计算损失</span><br>        shift_logits = logits[:, :-<span class="hljs-number">1</span>, :].contiguous()<br>        shift_targets = input_ids[:, <span class="hljs-number">1</span>:].contiguous()<br>        loss = compute_loss(shift_logits, shift_targets)<br>        <br>        <span class="hljs-comment"># 反向传播</span><br>        optimizer.zero_grad()<br>        loss.backward()<br>        <br>        <span class="hljs-comment"># 梯度裁剪</span><br>        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="hljs-number">1.0</span>)<br>        <br>        <span class="hljs-comment"># 更新参数</span><br>        optimizer.step()<br>        scheduler.step()<br>        <br>        train_loss += loss.item()<br>        global_step += <span class="hljs-number">1</span><br>        <br>        <span class="hljs-comment"># 日志</span><br>        <span class="hljs-keyword">if</span> global_step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            avg_loss = train_loss / (batch_idx + <span class="hljs-number">1</span>)<br>            ppl = math.exp(avg_loss)<br>            lr = scheduler.get_last_lr()[<span class="hljs-number">0</span>]<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch&#125;</span>, Step <span class="hljs-subst">&#123;global_step&#125;</span>, &quot;</span><br>                  <span class="hljs-string">f&quot;Loss: <span class="hljs-subst">&#123;avg_loss:<span class="hljs-number">.4</span>f&#125;</span>, PPL: <span class="hljs-subst">&#123;ppl:<span class="hljs-number">.2</span>f&#125;</span>, LR: <span class="hljs-subst">&#123;lr:<span class="hljs-number">.6</span>f&#125;</span>&quot;</span>)<br>    <br>    <span class="hljs-comment"># 验证阶段</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    val_loss = <span class="hljs-number">0</span><br>    <br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> val_loader:<br>            input_ids = batch[<span class="hljs-string">&#x27;input_ids&#x27;</span>].to(device)<br>            logits = model(input_ids)<br>            <br>            shift_logits = logits[:, :-<span class="hljs-number">1</span>, :].contiguous()<br>            shift_targets = input_ids[:, <span class="hljs-number">1</span>:].contiguous()<br>            loss = compute_loss(shift_logits, shift_targets)<br>            <br>            val_loss += loss.item()<br>    <br>    avg_val_loss = val_loss / <span class="hljs-built_in">len</span>(val_loader)<br>    val_ppl = math.exp(avg_val_loss)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\nEpoch <span class="hljs-subst">&#123;epoch&#125;</span> - Validation Loss: <span class="hljs-subst">&#123;avg_val_loss:<span class="hljs-number">.4</span>f&#125;</span>, PPL: <span class="hljs-subst">&#123;val_ppl:<span class="hljs-number">.2</span>f&#125;</span>\n&quot;</span>)<br>    <br>    <span class="hljs-comment"># 保存最佳模型</span><br>    <span class="hljs-keyword">if</span> avg_val_loss &lt; best_val_loss:<br>        best_val_loss = avg_val_loss<br>        save_checkpoint(model, optimizer, epoch, avg_val_loss, <span class="hljs-string">&#x27;best_model.pt&#x27;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;保存最佳模型，验证损失: <span class="hljs-subst">&#123;avg_val_loss:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练完成！&quot;</span>)<br></code></pre></td></tr></table></figure>
<hr />
<h2 id="第八部分常见问题和调试技巧">第八部分：常见问题和调试技巧</h2>
<h3 id="显存不足oom">1. 显存不足（OOM）</h3>
<p><strong>解决方案</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 方法1: 减小batch size</span><br>config[<span class="hljs-string">&#x27;batch_size&#x27;</span>] = <span class="hljs-number">16</span>  <span class="hljs-comment"># 从32减到16</span><br><br><span class="hljs-comment"># 方法2: 减小序列长度</span><br>config[<span class="hljs-string">&#x27;max_seq_len&#x27;</span>] = <span class="hljs-number">256</span>  <span class="hljs-comment"># 从512减到256</span><br><br><span class="hljs-comment"># 方法3: 使用梯度累积</span><br>accumulation_steps = <span class="hljs-number">4</span><br><br><span class="hljs-comment"># 方法4: 使用梯度检查点（Gradient Checkpointing）</span><br><span class="hljs-keyword">from</span> torch.utils.checkpoint <span class="hljs-keyword">import</span> checkpoint<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_with_checkpointing</span>(<span class="hljs-params">self, x</span>):<br>    <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> self.blocks:<br>        x = checkpoint(block, x)<br>    <span class="hljs-keyword">return</span> x<br><br><span class="hljs-comment"># 方法5: 使用混合精度训练</span><br><span class="hljs-keyword">from</span> torch.cuda.amp <span class="hljs-keyword">import</span> autocast, GradScaler<br></code></pre></td></tr></table></figure></p>
<h3 id="训练不收敛">2. 训练不收敛</h3>
<p><strong>可能原因和解决方案</strong>：</p>
<p><strong>学习率过大</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 降低学习率</span><br>optimizer = optim.AdamW(model.parameters(), lr=<span class="hljs-number">1e-4</span>)  <span class="hljs-comment"># 从3e-4降到1e-4</span><br></code></pre></td></tr></table></figure></p>
<p><strong>梯度爆炸</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 检查梯度范数</span><br>total_norm = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters():<br>    <span class="hljs-keyword">if</span> p.grad <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        param_norm = p.grad.data.norm(<span class="hljs-number">2</span>)<br>        total_norm += param_norm.item() ** <span class="hljs-number">2</span><br>total_norm = total_norm ** <span class="hljs-number">0.5</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Gradient norm: <span class="hljs-subst">&#123;total_norm&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 使用更严格的梯度裁剪</span><br>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="hljs-number">0.5</span>)<br></code></pre></td></tr></table></figure></p>
<p><strong>权重初始化不当</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用更小的初始化标准差</span><br>torch.nn.init.normal_(module.weight, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.01</span>)<br></code></pre></td></tr></table></figure></p>
<h3 id="生成文本质量差">3. 生成文本质量差</h3>
<p><strong>问题1：重复生成</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 解决方案：使用repetition penalty</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">apply_repetition_penalty</span>(<span class="hljs-params">logits, input_ids, penalty=<span class="hljs-number">1.2</span></span>):<br>    <span class="hljs-keyword">for</span> token_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">set</span>(input_ids.tolist()):<br>        logits[token_id] /= penalty<br>    <span class="hljs-keyword">return</span> logits<br></code></pre></td></tr></table></figure></p>
<p><strong>问题2：生成不连贯</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 解决方案：调整temperature和top_p</span><br>generated = generate_top_p(<br>    model, <br>    input_ids, <br>    temperature=<span class="hljs-number">0.8</span>,  <span class="hljs-comment"># 降低随机性</span><br>    top_p=<span class="hljs-number">0.9</span>         <span class="hljs-comment"># 只从高概率token中采样</span><br>)<br></code></pre></td></tr></table></figure></p>
<p><strong>问题3：生成太短</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 解决方案：添加长度惩罚</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">length_penalty</span>(<span class="hljs-params">length, alpha=<span class="hljs-number">0.6</span></span>):<br>    <span class="hljs-keyword">return</span> ((<span class="hljs-number">5</span> + length) / <span class="hljs-number">6</span>) ** alpha<br></code></pre></td></tr></table></figure></p>
<h3 id="调试技巧">4. 调试技巧</h3>
<p><strong>检查模型输出形状</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">check_shapes</span>(<span class="hljs-params">model, batch_size=<span class="hljs-number">2</span>, seq_len=<span class="hljs-number">10</span></span>):<br>    input_ids = torch.randint(<span class="hljs-number">0</span>, model.vocab_size, (batch_size, seq_len))<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Input shape: <span class="hljs-subst">&#123;input_ids.shape&#125;</span>&quot;</span>)<br>    <br>    <span class="hljs-comment"># Token embedding</span><br>    token_emb = model.token_embedding(input_ids)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Token embedding shape: <span class="hljs-subst">&#123;token_emb.shape&#125;</span>&quot;</span>)<br>    <br>    <span class="hljs-comment"># Position embedding</span><br>    positions = torch.arange(<span class="hljs-number">0</span>, seq_len).unsqueeze(<span class="hljs-number">0</span>)<br>    pos_emb = model.position_embedding(positions)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Position embedding shape: <span class="hljs-subst">&#123;pos_emb.shape&#125;</span>&quot;</span>)<br>    <br>    <span class="hljs-comment"># Full forward pass</span><br>    logits = model(input_ids)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Output logits shape: <span class="hljs-subst">&#123;logits.shape&#125;</span>&quot;</span>)<br>    <br>    <span class="hljs-keyword">assert</span> logits.shape == (batch_size, seq_len, model.vocab_size)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;✓ All shapes correct!&quot;</span>)<br><br>check_shapes(model)<br></code></pre></td></tr></table></figure></p>
<p><strong>可视化注意力权重</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">visualize_attention</span>(<span class="hljs-params">attention_weights, tokens</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    attention_weights: [seq_len, seq_len]</span><br><span class="hljs-string">    tokens: list of token strings</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))<br>    sns.heatmap(<br>        attention_weights.cpu().numpy(),<br>        xticklabels=tokens,<br>        yticklabels=tokens,<br>        cmap=<span class="hljs-string">&#x27;viridis&#x27;</span>,<br>        cbar=<span class="hljs-literal">True</span><br>    )<br>    plt.xlabel(<span class="hljs-string">&#x27;Key&#x27;</span>)<br>    plt.ylabel(<span class="hljs-string">&#x27;Query&#x27;</span>)<br>    plt.title(<span class="hljs-string">&#x27;Attention Weights&#x27;</span>)<br>    plt.show()<br></code></pre></td></tr></table></figure></p>
<p><strong>监控训练指标</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>writer = SummaryWriter(<span class="hljs-string">&#x27;runs/gpt_training&#x27;</span>)<br><br><span class="hljs-comment"># 在训练循环中</span><br>writer.add_scalar(<span class="hljs-string">&#x27;Loss/train&#x27;</span>, loss.item(), global_step)<br>writer.add_scalar(<span class="hljs-string">&#x27;Loss/val&#x27;</span>, val_loss, epoch)<br>writer.add_scalar(<span class="hljs-string">&#x27;Perplexity/train&#x27;</span>, train_ppl, global_step)<br>writer.add_scalar(<span class="hljs-string">&#x27;Perplexity/val&#x27;</span>, val_ppl, epoch)<br>writer.add_scalar(<span class="hljs-string">&#x27;Learning_rate&#x27;</span>, lr, global_step)<br><br><span class="hljs-comment"># 可视化权重分布</span><br><span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters():<br>    writer.add_histogram(name, param, global_step)<br><br>writer.close()<br></code></pre></td></tr></table></figure></p>
<hr />
<h2 id="第九部分扩展和改进">第九部分：扩展和改进</h2>
<h3 id="flash-attention">1. Flash Attention</h3>
<p><strong>作用</strong>：加速注意力计算，减少显存占用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 安装: pip install flash-attn</span><br><span class="hljs-keyword">from</span> flash_attn <span class="hljs-keyword">import</span> flash_attn_func<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FlashMultiHeadAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, num_heads</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.d_model = d_model<br>        self.num_heads = num_heads<br>        self.d_k = d_model // num_heads<br>        <br>        self.W_qkv = nn.Linear(d_model, <span class="hljs-number">3</span> * d_model)<br>        self.W_o = nn.Linear(d_model, d_model)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        batch_size, seq_len, _ = x.size()<br>        <br>        <span class="hljs-comment"># 计算Q, K, V</span><br>        qkv = self.W_qkv(x)<br>        qkv = qkv.reshape(batch_size, seq_len, <span class="hljs-number">3</span>, self.num_heads, self.d_k)<br>        qkv = qkv.permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>)  <span class="hljs-comment"># [3, B, H, T, d_k]</span><br>        q, k, v = qkv[<span class="hljs-number">0</span>], qkv[<span class="hljs-number">1</span>], qkv[<span class="hljs-number">2</span>]<br>        <br>        <span class="hljs-comment"># 使用Flash Attention</span><br>        output = flash_attn_func(q, k, v, causal=<span class="hljs-literal">True</span>)<br>        <br>        <span class="hljs-comment"># 重塑并投影</span><br>        output = output.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous().view(batch_size, seq_len, self.d_model)<br>        output = self.W_o(output)<br>        <br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure>
<h3 id="rotary-position-embedding-rope">2. Rotary Position Embedding
(RoPE)</h3>
<p><strong>优势</strong>：相对位置编码，外推性能好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RotaryPositionEmbedding</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, max_seq_len=<span class="hljs-number">2048</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        inv_freq = <span class="hljs-number">1.0</span> / (<span class="hljs-number">10000</span> ** (torch.arange(<span class="hljs-number">0</span>, d_model, <span class="hljs-number">2</span>).<span class="hljs-built_in">float</span>() / d_model))<br>        self.register_buffer(<span class="hljs-string">&#x27;inv_freq&#x27;</span>, inv_freq)<br>        <br>        <span class="hljs-comment"># 预计算位置编码</span><br>        t = torch.arange(max_seq_len).type_as(self.inv_freq)<br>        freqs = torch.einsum(<span class="hljs-string">&#x27;i,j-&gt;ij&#x27;</span>, t, self.inv_freq)<br>        emb = torch.cat((freqs, freqs), dim=-<span class="hljs-number">1</span>)<br>        self.register_buffer(<span class="hljs-string">&#x27;cos_cached&#x27;</span>, emb.cos())<br>        self.register_buffer(<span class="hljs-string">&#x27;sin_cached&#x27;</span>, emb.sin())<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">rotate_half</span>(<span class="hljs-params">self, x</span>):<br>        x1, x2 = x[..., :x.shape[-<span class="hljs-number">1</span>]//<span class="hljs-number">2</span>], x[..., x.shape[-<span class="hljs-number">1</span>]//<span class="hljs-number">2</span>:]<br>        <span class="hljs-keyword">return</span> torch.cat((-x2, x1), dim=-<span class="hljs-number">1</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, q, k</span>):<br>        seq_len = q.shape[<span class="hljs-number">1</span>]<br>        cos = self.cos_cached[:seq_len, ...]<br>        sin = self.sin_cached[:seq_len, ...]<br>        <br>        q_embed = (q * cos) + (self.rotate_half(q) * sin)<br>        k_embed = (k * cos) + (self.rotate_half(k) * sin)<br>        <br>        <span class="hljs-keyword">return</span> q_embed, k_embed<br></code></pre></td></tr></table></figure>
<h3 id="grouped-query-attention-gqa">3. Grouped Query Attention
(GQA)</h3>
<p><strong>优势</strong>：减少KV cache，加速推理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GroupedQueryAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, num_heads, num_kv_heads</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-keyword">assert</span> num_heads % num_kv_heads == <span class="hljs-number">0</span><br>        <br>        self.d_model = d_model<br>        self.num_heads = num_heads<br>        self.num_kv_heads = num_kv_heads<br>        self.num_queries_per_kv = num_heads // num_kv_heads<br>        self.d_k = d_model // num_heads<br>        <br>        self.W_q = nn.Linear(d_model, d_model)<br>        self.W_k = nn.Linear(d_model, num_kv_heads * self.d_k)<br>        self.W_v = nn.Linear(d_model, num_kv_heads * self.d_k)<br>        self.W_o = nn.Linear(d_model, d_model)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, mask=<span class="hljs-literal">None</span></span>):<br>        batch_size, seq_len, _ = x.size()<br>        <br>        <span class="hljs-comment"># Q: [B, num_heads, T, d_k]</span><br>        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        <br>        <span class="hljs-comment"># K, V: [B, num_kv_heads, T, d_k]</span><br>        K = self.W_k(x).view(batch_size, seq_len, self.num_kv_heads, self.d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        V = self.W_v(x).view(batch_size, seq_len, self.num_kv_heads, self.d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        <br>        <span class="hljs-comment"># 复制K和V以匹配Q的头数</span><br>        K = K.repeat_interleave(self.num_queries_per_kv, dim=<span class="hljs-number">1</span>)<br>        V = V.repeat_interleave(self.num_queries_per_kv, dim=<span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-comment"># 标准注意力计算</span><br>        scores = torch.matmul(Q, K.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) / (self.d_k ** <span class="hljs-number">0.5</span>)<br>        <br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            scores = scores.masked_fill(mask == <span class="hljs-number">0</span>, <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;-inf&#x27;</span>))<br>        <br>        attention_weights = torch.softmax(scores, dim=-<span class="hljs-number">1</span>)<br>        output = torch.matmul(attention_weights, V)<br>        <br>        output = output.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous().view(batch_size, seq_len, self.d_model)<br>        output = self.W_o(output)<br>        <br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure>
<h3 id="swiglu激活函数">4. SwiGLU激活函数</h3>
<p><strong>优势</strong>：比GELU性能更好（用于Llama等模型）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SwiGLU</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x, gate = x.chunk(<span class="hljs-number">2</span>, dim=-<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> F.silu(gate) * x<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FeedForwardSwiGLU</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, d_ff, dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 注意：需要2倍的d_ff因为会split</span><br>        self.linear1 = nn.Linear(d_model, <span class="hljs-number">2</span> * d_ff)<br>        self.linear2 = nn.Linear(d_ff, d_model)<br>        self.dropout = nn.Dropout(dropout)<br>        self.swiglu = SwiGLU()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.linear1(x)<br>        x = self.swiglu(x)<br>        x = self.dropout(x)<br>        x = self.linear2(x)<br>        x = self.dropout(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<hr />
<h2 id="第十部分与现代llm的对比">第十部分：与现代LLM的对比</h2>
<h3 id="主流模型架构对比">主流模型架构对比</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">特性</th>
<th style="text-align: left;">GPT-2</th>
<th style="text-align: left;">GPT-3</th>
<th style="text-align: left;">Llama</th>
<th style="text-align: left;">Llama 2</th>
<th style="text-align: left;">GPT-4 (推测)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>参数量</strong></td>
<td style="text-align: left;">1.5B</td>
<td style="text-align: left;">175B</td>
<td style="text-align: left;">7B-65B</td>
<td style="text-align: left;">7B-70B</td>
<td style="text-align: left;">1.76T</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>层数</strong></td>
<td style="text-align: left;">48</td>
<td style="text-align: left;">96</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">32-80</td>
<td style="text-align: left;">?</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>d_model</strong></td>
<td style="text-align: left;">1600</td>
<td style="text-align: left;">12288</td>
<td style="text-align: left;">4096</td>
<td style="text-align: left;">4096-8192</td>
<td style="text-align: left;">?</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>注意力头数</strong></td>
<td style="text-align: left;">25</td>
<td style="text-align: left;">96</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">32-64</td>
<td style="text-align: left;">?</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>位置编码</strong></td>
<td style="text-align: left;">Learned</td>
<td style="text-align: left;">Learned</td>
<td style="text-align: left;">RoPE</td>
<td style="text-align: left;">RoPE</td>
<td style="text-align: left;">?</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>激活函数</strong></td>
<td style="text-align: left;">GELU</td>
<td style="text-align: left;">GELU</td>
<td style="text-align: left;">SwiGLU</td>
<td style="text-align: left;">SwiGLU</td>
<td style="text-align: left;">?</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>归一化</strong></td>
<td style="text-align: left;">LayerNorm</td>
<td style="text-align: left;">LayerNorm</td>
<td style="text-align: left;">RMSNorm</td>
<td style="text-align: left;">RMSNorm</td>
<td style="text-align: left;">?</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>词汇表</strong></td>
<td style="text-align: left;">50257</td>
<td style="text-align: left;">50257</td>
<td style="text-align: left;">32000</td>
<td style="text-align: left;">32000</td>
<td style="text-align: left;">?</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>上下文长度</strong></td>
<td style="text-align: left;">1024</td>
<td style="text-align: left;">2048</td>
<td style="text-align: left;">2048</td>
<td style="text-align: left;">4096</td>
<td style="text-align: left;">32k-128k</td>
</tr>
</tbody>
</table>
<h3 id="关键改进点">关键改进点</h3>
<p><strong>1. RMSNorm vs LayerNorm</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RMSNorm</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, eps=<span class="hljs-number">1e-6</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.weight = nn.Parameter(torch.ones(d_model))<br>        self.eps = eps<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># 只使用RMS，不减去均值</span><br>        rms = torch.sqrt(torch.mean(x ** <span class="hljs-number">2</span>, dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>) + self.eps)<br>        <span class="hljs-keyword">return</span> self.weight * x / rms<br></code></pre></td></tr></table></figure></p>
<p><strong>优势</strong>：计算更快，性能相当或更好</p>
<p><strong>2. 并行化Attention和FFN（PaLM架构）</strong>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ParallelTransformerBlock</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># 并行计算attention和ffn</span><br>        attn_out = self.attention(self.ln1(x), mask)<br>        ffn_out = self.ffn(self.ln2(x))<br>        <br>        <span class="hljs-comment"># 合并输出</span><br>        x = x + attn_out + ffn_out<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure></p>
<p><strong>优势</strong>：训练速度提升15-20%</p>
<hr />
<h2 id="总结">总结</h2>
<h3 id="核心要点回顾">核心要点回顾</h3>
<ol type="1">
<li><strong>Transformer架构</strong>是现代LLM的基础：
<ul>
<li>Token Embedding + Position Embedding</li>
<li>Multi-Head Self-Attention（多头自注意力）</li>
<li>Feed-Forward Network（前馈网络）</li>
<li>Layer Normalization + Residual Connection</li>
</ul></li>
<li><strong>因果注意力</strong>是自回归语言模型的关键：
<ul>
<li>使用下三角掩码</li>
<li>确保训练和推理一致</li>
</ul></li>
<li><strong>训练技巧</strong>：
<ul>
<li>合适的学习率调度（Warmup + Cosine Decay）</li>
<li>梯度裁剪防止梯度爆炸</li>
<li>混合精度训练加速</li>
<li>权重共享减少参数</li>
</ul></li>
<li><strong>生成策略</strong>：
<ul>
<li>Greedy：快速但缺乏多样性</li>
<li>Top-p：推荐的平衡策略</li>
<li>Beam Search：适合翻译等任务</li>
</ul></li>
<li><strong>现代改进</strong>：
<ul>
<li>RoPE位置编码</li>
<li>GQA减少KV cache</li>
<li>SwiGLU激活函数</li>
<li>Flash Attention加速</li>
</ul></li>
</ol>
<h3 id="从零到一的完整流程">从零到一的完整流程</h3>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">1.</span> 数据准备<br>   ↓<br><span class="hljs-bullet">2.</span> Tokenization（第一讲）<br>   ↓<br><span class="hljs-bullet">3.</span> 构建模型架构（本讲）<br><span class="hljs-bullet">   -</span> Token Embedding<br><span class="hljs-bullet">   -</span> Position Embedding<br><span class="hljs-bullet">   -</span> Transformer Blocks<br><span class="hljs-bullet">   -</span> Output Layer<br>   ↓<br><span class="hljs-bullet">4.</span> 训练<br><span class="hljs-bullet">   -</span> 定义损失函数<br><span class="hljs-bullet">   -</span> 选择优化器<br><span class="hljs-bullet">   -</span> 设置学习率调度<br><span class="hljs-bullet">   -</span> 训练循环<br>   ↓<br><span class="hljs-bullet">5.</span> 评估<br><span class="hljs-bullet">   -</span> 计算困惑度<br><span class="hljs-bullet">   -</span> 生成样本检查<br>   ↓<br><span class="hljs-bullet">6.</span> 推理和生成<br><span class="hljs-bullet">   -</span> 选择生成策略<br><span class="hljs-bullet">   -</span> 调整超参数<br>   ↓<br><span class="hljs-bullet">7.</span> 优化和部署<br><span class="hljs-bullet">   -</span> 模型量化<br><span class="hljs-bullet">   -</span> 推理加速<br></code></pre></td></tr></table></figure>
<h3 id="实践建议">实践建议</h3>
<p><strong>从小模型开始</strong>： - 先用小数据集（如WikiText-2） -
小模型配置（6层，512维度） - 验证代码正确性</p>
<p><strong>逐步扩大规模</strong>： - 确认小模型能正常训练 -
逐步增加模型大小 - 使用更大的数据集</p>
<p><strong>关注关键指标</strong>： - 训练损失是否下降 -
验证困惑度是否合理 - 生成文本是否连贯</p>
<p><strong>调试优先级</strong>： 1. 确保代码能运行（形状正确） 2.
确保损失下降（学习率、初始化） 3. 确保不过拟合（dropout、数据量） 4.
优化生成质量（采样策略）</p>
<h3 id="下一步学习">下一步学习</h3>
<p><strong>第三讲预告</strong>： - 大规模数据处理 - 数据清洗和过滤 -
数据并行和模型并行 - 分布式训练</p>
<p><strong>推荐资源</strong>： - <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a> -
Transformer原始论文 - <a
target="_blank" rel="noopener" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language
Models are Unsupervised Multitask Learners</a> - GPT-2论文 - <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.09288">Llama 2: Open Foundation and
Fine-Tuned Chat Models</a> - Llama 2论文 - <a
target="_blank" rel="noopener" href="http://jalammar.github.io/illustrated-transformer/">The
Illustrated Transformer</a> - 可视化教程 - <a
target="_blank" rel="noopener" href="https://github.com/karpathy/nanoGPT">nanoGPT</a> - Andrej
Karpathy的最小GPT实现</p>
<hr />
<h2 id="附录完整代码仓库结构">附录：完整代码仓库结构</h2>
<figure class="highlight stan"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs stan">gpt-from-scratch/<br>├── <span class="hljs-title">model</span>/<br>│   ├── __init__.py<br>│   ├── embedding.py          <span class="hljs-comment"># Token和Position Embedding</span><br>│   ├── attention.py          <span class="hljs-comment"># Multi-Head Attention</span><br>│   ├── feedforward.py        <span class="hljs-comment"># Feed-Forward Network</span><br>│   ├── transformer_block.py  <span class="hljs-comment"># Transformer Block</span><br>│   └── gpt.py               <span class="hljs-comment"># 完整GPT模型</span><br>├── <span class="hljs-title">data</span>/<br>│   ├── __init__.py<br>│   ├── dataset.py           <span class="hljs-comment"># 数据集类</span><br>│   └── tokenizer.py         <span class="hljs-comment"># Tokenizer封装</span><br>├── training/<br>│   ├── __init__.py<br>│   ├── trainer.py           <span class="hljs-comment"># 训练器</span><br>│   └── scheduler.py         <span class="hljs-comment"># 学习率调度器</span><br>├── generation/<br>│   ├── __init__.py<br>│   └── sampling.py          <span class="hljs-comment"># 各种采样策略</span><br>├── utils/<br>│   ├── __init__.py<br>│   ├── checkpoint.py        <span class="hljs-comment"># 模型保存/加载</span><br>│   └── metrics.py           <span class="hljs-comment"># 评估指标</span><br>├── configs/<br>│   ├── gpt_small.yaml       <span class="hljs-comment"># 小模型配置</span><br>│   ├── gpt_medium.yaml      <span class="hljs-comment"># 中等模型配置</span><br>│   └── gpt_large.yaml       <span class="hljs-comment"># 大模型配置</span><br>├── train.py                 <span class="hljs-comment"># 训练脚本</span><br>├── generate.py              <span class="hljs-comment"># 生成脚本</span><br>├── evaluate.py              <span class="hljs-comment"># 评估脚本</span><br>└── requirements.txt         <span class="hljs-comment"># 依赖包</span><br></code></pre></td></tr></table></figure>
<p>希望这份详细的笔记能帮助你深入理解如何从零开始构建一个语言模型！下一讲我们将深入探讨大规模训练的技术细节。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/linux/" class="category-chain-item">linux</a>
  
  
    <span>></span>
    
  <a href="/categories/linux/drivers/" class="category-chain-item">drivers</a>
  
  
    <span>></span>
    
  <a href="/categories/linux/drivers/gpu/" class="category-chain-item">gpu</a>
  
  
    <span>></span>
    
  <a href="/categories/linux/drivers/gpu/stanford-cs336/" class="category-chain-item">stanford-cs336</a>
  
  

  

  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/git/">#git</a>
      
        <a href="/tags/strings/">#strings</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>大模型从0到1｜第二讲：PyTorch手把手搭建LLM</div>
      <div>https://realwujing.github.io/linux/drivers/gpu/stanford-cs336/大模型从0到1｜第二讲：pytorch手把手搭建LLM/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Wu Jing</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年11月22日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/linux/drivers/gpu/stanford-cs336/%E7%AC%AC%E4%B8%89%E8%AF%BE%EF%BC%9A%E8%AF%A6%E8%A7%A3%E7%8E%B0%E4%BB%A3LLM%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84/" title="第三课：详解现代LLM基础架构">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">第三课：详解现代LLM基础架构</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/linux/drivers/gpu/stanford-cs336/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BB%8E0%E5%88%B01%EF%BD%9C%E7%AC%AC%E4%B8%80%E8%AE%B2%EF%BC%9A%E6%A6%82%E8%BF%B0%E5%92%8Ctokenization/" title="大模型从0到1｜第一讲：概述和Tokenization">
                        <span class="hidden-mobile">大模型从0到1｜第一讲：概述和Tokenization</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="gitalk-container"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#gitalk-container', function() {
      Fluid.utils.createCssLink('/css/gitalk.css')
      Fluid.utils.createScript('https://lib.baomitu.com/gitalk/1.8.0/gitalk.min.js', function() {
        var options = Object.assign(
          {"clientID":"c11f8471a6ae4d3eea12","clientSecret":"87bfa232882af2b005f4c3352132dd418bf6d113","repo":"realwujing.github.io","owner":"realwujing","admin":["realwujing"],"language":"zh-CN","labels":["Gitalk"],"perPage":10,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":false,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token"},
          {
            id: 'ccdfd940444b32acc47d133a6540a6a0'
          }
        )
        var gitalk = new Gitalk(options);
        gitalk.render('gitalk-container');
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.events.registerRefreshCallback(function() {
      if ('mermaid' in window) {
        mermaid.init();
      }
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
