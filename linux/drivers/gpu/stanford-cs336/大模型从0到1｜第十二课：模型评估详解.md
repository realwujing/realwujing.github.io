# 大模型从0到1｜第十二课：模型评估详解

> 课程链接：[Stanford CS336 Spring 2025 - Lecture 12](https://stanford-cs336.github.io/spring2025-lectures/?trace=var/traces/lecture_12.json)

> **Evaluation**: 给定一个**固定的模型**，它到底有多"**好**"？

## 1. 所见即所得 (What you see)

### 基准分数 (Benchmark scores)
![DeepSeek-R1 Benchmarks](https://stanford-cs336.github.io/spring2025-lectures/images/deepseek-r1-benchmarks.png)
*   [DeepSeek-R1](https://arxiv.org/pdf/2501.12948.pdf)

![Llama 4 Benchmarks](https://stanford-cs336.github.io/spring2025-lectures/images/llama4-benchmarks.png)
*   [Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)

![OLMo 2 Instruct](https://www.datocms-assets.com/64837/1741887109-instruct-1.png)
*   [OLMo 2 (32B)](https://allenai.org/blog/olmo2-32B)

最近的语言模型都在类似但不完全相同的基准上进行评估（MMLU, MATH 等）。
*   这些基准是什么？
*   这些数字意味着什么？

![HELM Capabilities Leaderboard](https://stanford-cs336.github.io/spring2025-lectures/images/helm-capabilities-leaderboard.png)
*   [HELM capabilities](https://crfm.stanford.edu/helm/capabilities/latest/#/leaderboard)

**密切关注成本！**
![Artificial Analysis](https://stanford-cs336.github.io/spring2025-lectures/images/artificial-analysis.png)
*   [Artificial Analysis](https://artificialanalysis.ai/)

也许如果人们选择使用（并为之付费），那么这个模型就是好的...
![OpenRouter](https://stanford-cs336.github.io/spring2025-lectures/images/openrouter.png)
*   [OpenRouter](https://openrouter.ai/rankings)

![Chatbot Arena Leaderboard](https://stanford-cs336.github.io/spring2025-lectures/images/chatbot-arena-leaderboard.png)
*   [Chatbot Arena](https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard)

### 感觉 (Vibes)
*   [Demis Hassabis Tweet](https://x.com/demishassabis/status/1919779362980692364)
![Gemini 2.5](https://stanford-cs336.github.io/spring2025-lectures/images/demis-gemini-2.5.png)

一个危机...
![Karpathy Crisis](https://stanford-cs336.github.io/spring2025-lectures/images/karpathy-crisis.png)

---

## 2. 如何思考评估 (How to think about evaluation)

你可能认为评估是一个机械的过程（拿现有的模型，把提示词丢进去，平均一些数字...）。
实际上，评估是一个深刻而丰富的话题...
... 并且它决定了语言模型的未来。

**评估的目的是什么？**
没有一个真正的评估；这取决于你试图回答什么问题。
1.  **用户或公司**想要根据他们的用例（例如客户服务聊天机器人）做出购买决定（模型 A 还是模型 B）。
2.  **研究人员**想要衡量模型的原始能力（例如智力）。
3.  **我们**想要了解模型的好处 + 危害（出于商业和政策原因）。
4.  **模型开发者**想要获得反馈以改进模型。

在每种情况下，都有一个抽象的**目标 (goal)** 需要被转化为具体的评估。

### 框架 (Framework)
1.  **输入 (Inputs)** 是什么？
2.  如何**调用 (Call)** 语言模型？
3.  如何评估**输出 (Outputs)**？
4.  如何**解释 (Interpret)** 结果？

**输入是什么？**
1.  **覆盖 (Covered)** 了什么用例？
2.  我们在尾部是否有**困难 (Difficult)** 输入的代表？
3.  输入是否**适应 (Adapted)** 了模型（例如多轮对话）？

**如何调用语言模型？**
1.  如何提示语言模型？
2.  语言模型是否使用思维链 (CoT)、工具、RAG 等？
3.  我们是在评估语言模型还是一个 Agent 系统（模型开发者想要前者，用户想要后者）？

**如何评估输出？**
1.  用于评估的参考输出是无误的吗？
2.  使用什么指标（例如 pass@k）？
3.  如何计入成本（例如推理 + 训练）？
4.  如何计入不对称错误（例如医疗环境中的幻觉）？
5.  如何处理开放式生成（没有标准答案）？

**如何解释指标？**
1.  如何解释一个数字（例如 91%）——它准备好部署了吗？
2.  面对训练-测试重叠，我们如何评估泛化能力？
3.  我们是在评估最终模型还是方法？

**总结**：在做评估时有很多问题需要深思。

---

## 3. 困惑度 (Perplexity)

**回顾**：语言模型是标记序列上的概率分布 **p(x)**。
**困惑度** $(1/p(D))^{(1/|D|)}$ 衡量 p 是否赋予某个数据集 D 高概率。

*   在预训练中，你在训练集上最小化困惑度。
*   显而易见的事情是在测试集上测量困惑度。

**标准数据集**：Penn Treebank (WSJ), WikiText-103 (Wikipedia), One Billion Word Benchmark (来自机器翻译 WMT11 - EuroParl, UN, news)。
*   论文在数据集（训练分割）上训练，并在同一数据集（测试分割）上评估。
*   Pure CNNs+LSTMs on the One Billion Word Benchmark (perplexity 51.3 -> 30.0) [Link](https://arxiv.org/abs/1602.02410)

GPT-2 在 WebText（40GB 文本，从 Reddit 链接的网站）上训练，在标准数据集上零样本评估。
*   这是分布外 (OOD) 评估（但想法是训练覆盖了很多）。
![GPT-2 Perplexity](https://stanford-cs336.github.io/spring2025-lectures/images/gpt2-perplexity.png)
*   **结论**：在小数据集上效果更好（迁移有帮助），但在大数据集 (1BW) 上效果不佳。

自 GPT-2 和 GPT-3 以来，语言建模论文更多地转向下游任务准确率。
**但困惑度仍然有用的原因**：
*   比下游任务准确率更平滑（用于拟合 scaling laws）。
*   是通用的（我们将其用于训练的原因），而任务准确率可能会错过一些细微差别。
*   注意：也可以在下游任务上测量条件困惑度（用于 scaling laws） [Link](https://arxiv.org/abs/2412.04403)。

**警告**（如果你正在运行排行榜）：评估者需要信任语言模型。
*   对于任务准确率，可以直接获取黑盒模型生成的输出并计算所需的指标。
*   对于困惑度，需要 LM 生成概率并相信它们的总和为 1（在过去有 UNK 时甚至更糟）。

**困惑度最大化主义者的观点**：
*   你的真实分布是 $t$，模型是 $p$。
*   最好的困惑度是 $H(t)$，当且仅当 $p=t$ 时获得。
*   如果有 $t$，那么就可以解决所有任务。
*   所以通过降低困惑度，最终将达到 AGI。
*   **警告**：这可能不是到达那里的最有效方式（在不重要的分布部分上降低困惑度）。

**精神上属于困惑度的事物**：
*   类似的想法：像 LAMBADA 这样的完形填空任务 [Link](https://arxiv.org/abs/1606.06031)
![LAMBADA](https://stanford-cs336.github.io/spring2025-lectures/images/lambada.png)
*   HellaSwag [Link](https://arxiv.org/pdf/1905.07830)
![HellaSwag](https://stanford-cs336.github.io/spring2025-lectures/images/hellaswag.png)

---

## 4. 知识基准 (Knowledge benchmarks)

### Massive Multitask Language Understanding (MMLU)
[MMLU Paper](https://arxiv.org/pdf/2009.03300.pdf)
*   57 个科目（例如数学、美国历史、法律、道德），多项选择。
*   "由研究生和本科生从网上免费提供的资源中收集"。
*   真正的目的是测试知识，而不是语言理解。
*   使用少样本提示在 GPT-3 上进行评估。
![MMLU](https://stanford-cs336.github.io/spring2025-lectures/images/mmlu.png)
*   [HELM MMLU for visualizing predictions](https://crfm.stanford.edu/helm/mmlu/latest/)

### MMLU-Pro
[Link](https://arxiv.org/abs/2406.01574)
*   删除了 MMLU 中的噪声/微不足道的问题。
*   将 4 个选项扩展到 10 个选项。
*   使用思维链 (CoT) 进行评估（给模型更多机会）。
*   模型的准确率下降了 16% 到 33%（不像之前那样饱和）。
![MMLU-Pro](https://stanford-cs336.github.io/spring2025-lectures/images/mmlu-pro.png)
*   [HELM MMLU-Pro for visualizing predictions](https://crfm.stanford.edu/helm/capabilities/latest/#/leaderboard/mmlu_pro)

### Graduate-Level Google-Proof Q&A (GPQA)
[Link](https://arxiv.org/abs/2311.12022)
*   问题由来自 Upwork 的 61 位博士承包商编写。
![GPQA](https://stanford-cs336.github.io/spring2025-lectures/images/gpqa.png)
*   博士专家达到 65% 的准确率。
*   非专家在可以使用 Google 的情况下，30 分钟内达到 34%。
*   GPT-4 达到 39%。
*   [HELM GPQA for visualizing predictions](https://crfm.stanford.edu/helm/capabilities/latest/#/leaderboard/gpqa)

### Humanity's Last Exam
[Link](https://arxiv.org/abs/2501.14249)
*   2500 个问题：多模态，多学科，多项选择 + 简答。
![Humanity's Last Exam Examples](https://stanford-cs336.github.io/spring2025-lectures/images/hle-examples.png)
*   向问题创建者奖励 50 万美元奖金池 + 共同作者身份。
*   由前沿 LLM 过滤，经过多阶段审查。
![Humanity's Last Exam Pipeline](https://stanford-cs336.github.io/spring2025-lectures/images/hle-pipeline.png)
![Humanity's Last Exam Results](https://stanford-cs336.github.io/spring2025-lectures/images/hle-results.png)
*   [Latest Leaderboard](https://agi.safe.ai/)

---

## 5. 指令遵循基准 (Instruction following benchmarks)

到目前为止，我们一直在评估相当结构化的任务。
**指令遵循**（如 ChatGPT 所普及）：只需遵循指令。
**挑战**：如何评估开放式响应？

### Chatbot Arena
[Link](https://arxiv.org/abs/2403.04132)
**工作原理**：
*   来自互联网的随机人员输入提示词。
*   他们从两个随机（匿名）模型获得响应。
*   他们评价哪一个更好。
*   根据成对比较计算 ELO 分数。
*   特点：实时（非静态）输入，可以适应新模型。
![Chatbot Arena Leaderboard](https://stanford-cs336.github.io/spring2025-lectures/images/chatbot-arena-leaderboard.png)
*   [Chatbot Arena](https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard)

### Instruction-Following Eval (IFEval)
[Link](https://arxiv.org/abs/2311.07911)
![IFEval Categories](https://stanford-cs336.github.io/spring2025-lectures/images/ifeval-categories.png)
*   在其指令中添加简单的合成约束。
*   约束可以自动验证，但响应的语义不能。
*   相当简单的指令，约束有点人为。
*   [HELM IFEval for visualizing predictions](https://crfm.stanford.edu/helm/capabilities/latest/#/leaderboard/ifeval)

### AlpacaEval
[Link](https://tatsu-lab.github.io/alpaca_eval/)
*   来自各种来源的 805 条指令。
*   指标：由 GPT-4 preview 评判的对战 GPT-4 preview 的胜率（潜在偏差）。
![AlpacaEval Leaderboard](https://stanford-cs336.github.io/spring2025-lectures/images/alpacaeval-leaderboard.png)

### WildBench
[Link](https://arxiv.org/pdf/2406.04770)
*   从 100 万个人类-聊天机器人对话中获取 1024 个示例。
*   使用 GPT-4 turbo 作为裁判并带有检查清单（像 CoT 一样用于评判）+ GPT-4 作为裁判。
*   与 Chatbot Arena 相关性良好 (0.95)（Chatbot Arena 似乎是基准的事实上的健全性检查）。
![WildBench](https://stanford-cs336.github.io/spring2025-lectures/images/wildbench.png)
*   [HELM WildBench for visualizing predictions](https://crfm.stanford.edu/helm/capabilities/latest/#/leaderboard/wildbench)

---

## 6. Agent 基准 (Agent benchmarks)

考虑需要使用工具（例如运行代码）并需要在一段时间内迭代的任务。
**Agent** = 语言模型 + Agent 脚手架（决定如何使用 LM 的逻辑）。

### SWEBench
[Link](https://arxiv.org/abs/2310.06770)
*   跨 12 个 Python 仓库的 2294 个任务。
*   给定代码库 + 问题描述，提交 PR。
*   评估指标：单元测试。
![SWEBench](https://stanford-cs336.github.io/spring2025-lectures/images/swebench.png)

### CyBench
[Link](https://arxiv.org/abs/2408.08926)
*   40 个夺旗 (CTF) 任务。
*   使用首次解决时间作为难度的衡量标准。
![CyBench](https://stanford-cs336.github.io/spring2025-lectures/images/cybench.png)
![CyBench Agent](https://stanford-cs336.github.io/spring2025-lectures/images/cybench-agent.png)
![CyBench Results](https://stanford-cs336.github.io/spring2025-lectures/images/cybench-results.png)

### MLEBench
[Link](https://arxiv.org/abs/2410.07095)
*   75 个 Kaggle 竞赛（需要训练模型、处理数据等）。
![MLEBench](https://stanford-cs336.github.io/spring2025-lectures/images/mlebench.png)
![MLEBench Results](https://stanford-cs336.github.io/spring2025-lectures/images/mlebench-results.png)

---

## 7. 纯推理基准 (Pure reasoning benchmarks)

到目前为止的所有任务都需要语言和世界知识。
**我们能将推理与知识隔离开来吗？**
可以说，推理捕捉了一种更纯粹的智能形式（不仅仅是记住事实）。

[ARC-AGI](https://arcprize.org/arc-agi)
由 Francois Chollet 于 2019 年推出。

**ARC-AGI-1**
![ARC-AGI-1 Tasks](https://arcprize.org/media/images/arc-task-grids.jpg)
![ARC-AGI-1 Leaderboard](https://arcprize.org/media/images/oseriesleaderboard.png)

**ARC-AGI-2: 更难**
![ARC-AGI-2 Unsolved](https://arcprize.org/media/images/blog/arc-agi-2-unsolved-1.png)

---

## 8. 安全基准 (Safety benchmarks)

![Car Crash Test](https://www.team-bhp.com/forum/attachments/road-safety/2173645d1625144681-will-crash-test-rating-change-if-higher-variant-chosen-images-30.jpeg)
对 AI 来说，安全意味着什么？

[HELM safety: curated set of benchmarks](https://crfm.stanford.edu/helm/safety/latest/#/leaderboard)

### HarmBench
[Link](https://arxiv.org/abs/2402.04249)
*   基于 510 种违反法律或规范的有害行为。
*   [HarmBench on HELM](https://crfm.stanford.edu/helm/safety/latest/#/leaderboard/harm_bench)
*   [Example of safety failure](https://crfm.stanford.edu/helm/safety/latest/#/runs/harm_bench:model=anthropic_claude-3-7-sonnet-20250219?instancesPage=4)

### AIR-Bench
[Link](https://arxiv.org/abs/2407.17436)
*   基于监管框架和公司政策。
*   分类为 314 个风险类别，5694 个提示词。
![AIR-Bench Overview](https://crfm.stanford.edu/helm/assets/air-overview-d2e6c49f.png)
*   [HELM AIR-Bench](https://crfm.stanford.edu/helm/air-bench/latest/#/leaderboard)

### 越狱 (Jailbreaking)
*   语言模型被训练为拒绝有害指令。
*   贪婪坐标梯度 (GCG) 自动优化提示词以绕过安全机制 [Link](https://arxiv.org/pdf/2307.15043)。
*   从开放权重模型 (Llama) 迁移到封闭模型 (GPT-4)。
![GCG Examples](https://stanford-cs336.github.io/spring2025-lectures/images/gcg-examples.png)

### 部署前测试 (Pre-deployment testing)
*   US Safety Institute + UK AI Safety Institute 合作。
*   公司在发布前让安全机构使用模型（目前是自愿的）。
*   安全机构运行评估并向公司出具报告。
*   [Report](https://www.nist.gov/system/files/documents/2024/12/18/US_UK_AI%20Safety%20Institute_%20December_Publication-OpenAIo1.pdf)

### 但什么是安全？
*   安全的许多方面都具有很强的语境性（政治、法律、社会规范——各国不同）。
*   天真地讲，通过可能会认为安全关于拒绝，并且与能力相悖，但还有更多...
*   医疗环境中的幻觉使系统能力更强，也更安全。

**模型的两个降低安全的方面：能力 + 倾向 (capabilities + propensity)**
*   一个系统可能有能力做某事，但拒绝做。
*   对于 API 模型，倾向很重要。
*   对于开放权重模型，能力很重要（因为可以很容易地微调去除安全限制）。

**双重用途 (Dual-use)**：有能力的网络安全 Agent（在 CyBench 上表现出色）可用于入侵系统或进行渗透测试。
CyBench 被安全机构用作安全评估，但它真的是能力评估吗？

---

## 9. 真实性 (Realism)

语言模型在实践中被大量使用：
![OpenAI Usage](https://stanford-cs336.github.io/spring2025-lectures/images/openai-100b-tokens.png) [Tweet](https://x.com/sama/status/1756089361609981993)
![Cursor Usage](https://stanford-cs336.github.io/spring2025-lectures/images/cursor-1b-lines.png) [Tweet](https://x.com/amanrsanger/status/1916968123535880684)

然而，现有的基准（例如 MMLU）与现实世界的使用相去甚远。
来自真实人群的实时流量包含垃圾信息，这也并不总是我们想要的。

**两种类型的提示词**：
1.  **测验 (Quizzing)**：用户知道答案，试图测试系统（想想标准化考试）。
2.  **询问 (Asking)**：用户不知道答案，试图使用系统来获得答案。
**询问**更现实，并为用户产生价值。

### Clio (Anthropic)
[Link](https://arxiv.org/abs/2412.13678)
*   使用语言模型分析真实用户数据。
*   分享人们在问什么的普遍模式。
![Clio Table](https://stanford-cs336.github.io/spring2025-lectures/images/clio-table4.png)

### MedHELM
[Link](https://arxiv.org/abs/2412.13678)
*   以前的医疗基准是基于标准化考试的。
*   从 29 位临床医生那里收集了 121 个临床任务，混合了私人和公共数据集。
![MedHELM Overview](https://crfm.stanford.edu/helm/assets/medhelm-overview-3ddfcd65.png)
*   [MedHELM](https://crfm.stanford.edu/helm/medhelm/latest/#/leaderboard)

不幸的是，真实性和隐私有时是相互矛盾的。

---

## 10. 有效性 (Validity)

我们如何知道我们的评估是有效的？

### 训练-测试重叠 (Train-test overlap)
*   机器学习 101：不要在你的测试集上训练。
*   前基础模型时代 (ImageNet, SQuAD)：定义明确的训练-测试分割。
*   现在：在互联网上训练，并且不告诉人们你的数据。

**路线 1：尝试从模型推断训练-测试重叠**
*   利用数据点的可交换性 [Link](https://arxiv.org/pdf/2310.17623)
![Contamination Exchangeability](https://stanford-cs336.github.io/spring2025-lectures/images/contamination-exchangeability.png)

**路线 2：鼓励报告规范（例如，人们报告置信区间）**
*   模型提供商应报告训练-测试重叠 [Link](https://arxiv.org/abs/2410.08385)

### 数据集质量 (Dataset quality)
*   修复 SWE-Bench 以生成 SWE-Bench Verified [Blog Post](https://openai.com/index/introducing-swe-bench-verified/)
*   创建基准的白金版本 (Platinum versions) [Link](https://arxiv.org/abs/2502.03461)
![Platinum Context](https://pbs.twimg.com/media/GjICXQlWkAAYnDS?format=jpg&name=4096x4096)
![Platinum Reasoning](https://pbs.twimg.com/media/GjICcGQXYAAM4o1?format=jpg&name=4096x4096)

---

## 11. 我们到底在评估什么？ (What are we evaluating?)

换句话说，游戏的规则是什么？

在基础模型之前，我们评估**方法 (methods)**（标准化训练-测试分割）。
今天，我们评估**模型/系统 (models/systems)**（任何东西都可以）。

有一些例外...
**nanogpt speedrun**：固定数据，计算达到特定验证损失的时间。
![NanoGPT Speedrun](https://stanford-cs336.github.io/spring2025-lectures/images/karpathy-nanogpt-speedrun.png)
[Tweet](https://x.com/karpathy/status/1846790537262571739)

**DataComp-LM**：给定原始数据集，使用标准训练管道获得最佳准确率 [Link](https://arxiv.org/abs/2406.11794)

*   评估方法鼓励研究人员通过算法创新。
*   评估模型/系统对下游用户有用。

无论哪种方式，我们都需要定义游戏的规则！

---

## 总结 (Takeaways)

*   **没有一个真正的评估**；根据你想要衡量的东西选择评估。
*   永远要看**个别实例**和预测结果。
*   有许多方面需要考虑：**能力、安全、成本、真实性**。
*   清楚地陈述**游戏规则**（方法 vs 模型/系统）。
