# å¤§æ¨¡å‹ä»0åˆ°1ï½œç¬¬å…­è®²ï¼šæ‰‹å†™é«˜æ€§èƒ½ç®—å­

> è¯¾ç¨‹é“¾æ¥ï¼š[Stanford CS336 Spring 2025 - Lecture 6: Writing Fast Kernels](https://stanford-cs336.github.io/spring2025-lectures/?trace=var/traces/lecture_06.json)

---

## è¯¾ç¨‹æ¦‚è¿°

**ä¸ŠèŠ‚è¯¾å›é¡¾ï¼š** GPU çš„é«˜å±‚æ¬¡æ¦‚è¿°å’Œæ€§èƒ½åˆ†æ  
**æœ¬èŠ‚è¯¾é‡ç‚¹ï¼š** æ€§èƒ½æµ‹è¯•/åˆ†æ + æ‰‹å†™ GPU ç®—å­

**æ ¸å¿ƒå†…å®¹ï¼š**
- Benchmarking å’Œ Profiling æŠ€æœ¯
- Kernel Fusionï¼ˆç®—å­èåˆï¼‰çš„åŠ¨æœº
- ä¸‰ç§ç¼–å†™ç®—å­çš„æ–¹å¼ï¼šCUDAã€Tritonã€PyTorch ç¼–è¯‘

---

## Part 1: GPU æ¶æ„å›é¡¾

### 1.1 ç¡¬ä»¶æ¶æ„

![GPU Architecture](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*6xoBKi5kL2dZpivFe1-zgw.jpeg)

**è®¡ç®—å•å…ƒï¼š**
- Streaming Multiprocessors (SMs) [A100: 108ä¸ª]

**å†…å­˜å±‚æ¬¡ï¼š**
- **DRAM** [A100: 80GB] - å®¹é‡å¤§ï¼Œé€Ÿåº¦æ…¢
- **L2 Cache** [A100: 40MB]
- **L1 Cache** [A100: 192KB per SM] - å®¹é‡å°ï¼Œé€Ÿåº¦å¿«

### 1.2 æ‰§è¡Œæ¨¡å‹

![Execution Model](https://docs.nvidia.com/cuda/parallel-thread-execution/_images/grid-with-CTAs.png)

**ä¸‰å±‚ç»“æ„ï¼š**
- **Threadï¼ˆçº¿ç¨‹ï¼‰ï¼š** å¤„ç†å•ä¸ªç´¢å¼• iï¼Œå³æ‰§è¡Œ f(i)
- **Thread Blockï¼ˆçº¿ç¨‹å—ï¼‰ï¼š** è°ƒåº¦åˆ°å•ä¸ª SM ä¸Šï¼Œåˆç§° CTA (Concurrent Thread Arrays)
- **Gridï¼ˆç½‘æ ¼ï¼‰ï¼š** çº¿ç¨‹å—çš„é›†åˆ

**ä¸ºä»€ä¹ˆéœ€è¦ Thread Blockï¼Ÿ**
- **å…±äº«å†…å­˜ï¼ˆShared Memoryï¼‰ï¼š** çº¿ç¨‹å—å†…çš„çº¿ç¨‹å¯ä»¥å…±äº«å†…å­˜ï¼ˆé€Ÿåº¦ä¸ L1 Cache ç›¸å½“ï¼‰[A100: 164KB]
- **åŒæ­¥æœºåˆ¶ï¼š** å¯ä»¥åœ¨å—å†…åŒæ­¥çº¿ç¨‹ï¼ˆä½†ä¸èƒ½è·¨å—åŒæ­¥ï¼‰
- **è®¾è®¡åŸåˆ™ï¼š** å°†è¯»å–ç›¸ä¼¼æ•°æ®çš„ f(i) åˆ†ç»„åˆ°ä¸€èµ·

### 1.3 ç¡¬ä»¶ä¸æ‰§è¡Œçš„äº¤äº’

![Wave Quantization](https://developer-blogs.nvidia.com/wp-content/uploads/2019/06/pasted-image-0.png)

**Wave Quantization é—®é¢˜ï¼š**
- çº¿ç¨‹å—ä»¥"æ³¢æ¬¡"è°ƒåº¦åˆ° SM ä¸Š
- æœ€åä¸€æ³¢å¯èƒ½çº¿ç¨‹å—è¾ƒå°‘ï¼Œå¯¼è‡´éƒ¨åˆ† SM ç©ºé—²ï¼ˆä½å ç”¨ç‡ï¼‰
- **è§£å†³æ–¹æ¡ˆï¼š** è®©çº¿ç¨‹å—æ•°é‡èƒ½è¢« SM æ•°é‡æ•´é™¤
- **ç»éªŒæ³•åˆ™ï¼š** çº¿ç¨‹å—æ•°é‡åº” >= 4x SM æ•°é‡

**æŒ‘æˆ˜ï¼š** ç¡¬ä»¶çš„æŸäº›æ–¹é¢å¯¹æ‰§è¡Œæ¨¡å‹æ˜¯éšè—çš„ï¼ˆå¦‚è°ƒåº¦ç­–ç•¥ã€SM æ•°é‡ï¼‰

### 1.4 ç®—æœ¯å¼ºåº¦ (Arithmetic Intensity)

**å®šä¹‰ï¼š** `ç®—æœ¯å¼ºåº¦ = FLOPs æ•°é‡ / å­—èŠ‚æ•°`

- **é«˜ç®—æœ¯å¼ºåº¦ï¼š** è®¡ç®—å¯†é›†å‹ï¼ˆcompute-boundï¼‰âœ… å¥½
- **ä½ç®—æœ¯å¼ºåº¦ï¼š** å†…å­˜å¯†é›†å‹ï¼ˆmemory-boundï¼‰âŒ å·®

**é€šç”¨è§„åˆ™ï¼š**
- çŸ©é˜µä¹˜æ³•ï¼šè®¡ç®—å¯†é›†å‹
- å…¶ä»–å¤§éƒ¨åˆ†æ“ä½œï¼šå†…å­˜å¯†é›†å‹

---

## Part 2: Benchmarking å’Œ Profiling

### 2.1 ä¸ºä»€ä¹ˆéœ€è¦æ€§èƒ½æµ‹è¯•ï¼Ÿ

**é‡è¦æ€§ï¼š** å¿…é¡»å¯¹ä»£ç è¿›è¡Œ benchmark å’Œ profileï¼

è™½ç„¶å¯ä»¥é˜…è¯»è§„æ ¼è¡¨å’Œè®ºæ–‡ï¼Œä½†æ€§èƒ½å–å†³äºï¼š
- åº“ç‰ˆæœ¬
- ç¡¬ä»¶é…ç½®
- å·¥ä½œè´Ÿè½½ç‰¹æ€§

**æ²¡æœ‰æ›¿ä»£å“ï¼š** å¿…é¡»äº²è‡ªæµ‹è¯•ä½ çš„ä»£ç 

### 2.2 ç¤ºä¾‹ï¼šMLP æ¨¡å‹

```python
class MLP(nn.Module):
    """ç®€å•çš„ MLP: linear -> GeLU -> linear -> GeLU -> ..."""
    def __init__(self, dim: int, num_layers: int):
        super().__init__()
        self.layers = nn.ModuleList([nn.Linear(dim, dim) for _ in range(num_layers)])

    def forward(self, x: torch.Tensor):
        for layer in self.layers:
            x = layer(x)
            x = torch.nn.functional.gelu(x)
        return x
```

### 2.3 Benchmarkingï¼šæµ‹é‡æ—¶é—´

**ç›®çš„ï¼š** æµ‹é‡æ“ä½œçš„å®é™…è¿è¡Œæ—¶é—´

**ç”¨é€”ï¼š**
- æ¯”è¾ƒä¸åŒå®ç°ï¼ˆå“ªä¸ªæ›´å¿«ï¼Ÿï¼‰
- ç†è§£æ€§èƒ½å¦‚ä½•æ‰©å±•ï¼ˆå¦‚éšç»´åº¦å˜åŒ–ï¼‰

**å®ç°è¦ç‚¹ï¼š**
```python
def benchmark(description: str, run: Callable, num_warmups: int = 1, num_trials: int = 3):
    # Warmup: é¦–æ¬¡è¿è¡Œå¯èƒ½è¾ƒæ…¢ï¼ˆç¼–è¯‘ã€ç¼“å­˜ç­‰ï¼‰
    for _ in range(num_warmups):
        run()
    torch.cuda.synchronize()  # ç­‰å¾… CUDA çº¿ç¨‹å®Œæˆï¼ˆé‡è¦ï¼ï¼‰
    
    # æ­£å¼è®¡æ—¶
    times = []
    for trial in range(num_trials):
        start_time = time.time()
        run()
        torch.cuda.synchronize()  # é‡è¦ï¼
        end_time = time.time()
        times.append((end_time - start_time) * 1000)
    
    return mean(times)
```

**æµ‹è¯•åœºæ™¯ï¼š**
- æ‰©å±•æ­¥æ•°ï¼ˆnum_stepsï¼‰
- æ‰©å±•å±‚æ•°ï¼ˆnum_layersï¼‰
- æ‰©å±•æ‰¹å¤§å°ï¼ˆbatch_sizeï¼‰
- æ‰©å±•ç»´åº¦ï¼ˆdimï¼‰

**æ³¨æ„ï¼š** ç”±äº CUDA kernel çš„éå‡è´¨æ€§ã€ç¡¬ä»¶ç­‰å› ç´ ï¼Œæ—¶é—´å¹¶ä¸æ€»æ˜¯å¯é¢„æµ‹çš„

### 2.4 Profilingï¼šåˆ†æç“¶é¢ˆ

**ç›®çš„ï¼š** äº†è§£æ—¶é—´èŠ±åœ¨å“ªé‡Œ

**æ·±å±‚ä»·å€¼ï¼š** å¸®åŠ©ç†è§£åº•å±‚è°ƒç”¨äº†ä»€ä¹ˆ

**PyTorch Profilerï¼š**
```python
def profile(description: str, run: Callable, with_stack: bool = False):
    # Warmup
    for _ in range(num_warmups):
        run()
    torch.cuda.synchronize()
    
    # ä½¿ç”¨ profiler è¿è¡Œ
    with torch.profiler.profile(
            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
            with_stack=with_stack) as prof:
        run()
        torch.cuda.synchronize()
    
    # æ‰“å°è¡¨æ ¼
    table = prof.key_averages().table(sort_by="cuda_time_total",
                                      max_name_column_width=80,
                                      row_limit=10)
    return table
```

**è§‚å¯Ÿç»“æœï¼š**
- å¯ä»¥çœ‹åˆ°å®é™…è°ƒç”¨çš„ CUDA kernel
- ä¸åŒçš„ tensor ç»´åº¦ä¼šè°ƒç”¨ä¸åŒçš„ CUDA kernel
- Kernel åç§°é€éœ²å®ç°ä¿¡æ¯
  - ä¾‹å¦‚ï¼š`cutlass_80_simt_sgemm_256x128_8x4_nn_align1`
  - cutlass: NVIDIA çš„çº¿æ€§ä»£æ•° CUDA åº“
  - 256x128: tile å¤§å°

**Flame Graphï¼š** å¯è§†åŒ–å †æ ˆè·Ÿè¸ªï¼Œæ˜¾ç¤ºæ—¶é—´åˆ†å¸ƒ

---

## Part 3: Kernel Fusion åŠ¨æœº

### 3.1 ä»“åº“ä¸å·¥å‚çš„ç±»æ¯”

**å‚è€ƒï¼š** [Horace He's Blog Post](https://horace.io/brrr_intro.html)

**ç±»æ¯”ï¼š**
- **ä»“åº“ â‰ˆ DRAM (HBM)**
- **å·¥å‚ â‰ˆ SRAM (L1 Cache / Shared Memory)**

![Factory Bandwidth](https://horace.io/img/perf_intro/factory_bandwidth.png)

#### æ·±å…¥ç†è§£è¿™ä¸ªç±»æ¯”

**ç°å®ä¸–ç•Œçš„å·¥å‚è¿ä½œï¼š**

æƒ³è±¡ä½ ç»è¥ä¸€å®¶åˆ¶é€ å·¥å‚ï¼š
- **ä»“åº“ï¼ˆWarehouseï¼‰ï¼š** è¿œç¦»å·¥å‚ï¼Œå®¹é‡å·¨å¤§ï¼Œä½†è¿è¾“æ…¢ä¸”æ˜‚è´µ
- **å·¥å‚è½¦é—´ï¼ˆFactory Floorï¼‰ï¼š** ç©ºé—´æœ‰é™ï¼Œä½†å·¥äººå¯ä»¥å¿«é€Ÿæ‹¿å–ææ–™å¹¶åŠ å·¥

**æœ€ä¼˜ç­–ç•¥ï¼š**
1. ä¸€æ¬¡æ€§ä»ä»“åº“è¿æ¥ä¸€æ‰¹åŸææ–™
2. åœ¨å·¥å‚è½¦é—´å†…å®Œæˆæ‰€æœ‰åŠ å·¥æ­¥éª¤
3. ä¸€æ¬¡æ€§æŠŠæˆå“è¿å›ä»“åº“

**æœ€å·®ç­–ç•¥ï¼š**
1. ä»ä»“åº“æ‹¿åŸææ–™ â†’ åŠ å·¥ä¸€æ­¥ â†’ é€å›ä»“åº“
2. å†ä»ä»“åº“æ‹¿ â†’ åŠ å·¥ç¬¬äºŒæ­¥ â†’ é€å›ä»“åº“
3. é‡å¤å¤šæ¬¡...

æ¯æ¬¡å¾€è¿”ä»“åº“éƒ½æ˜¯å·¨å¤§çš„æ—¶é—´æµªè´¹ï¼

---

**æ˜ å°„åˆ° GPU å†…å­˜ï¼š**

| ç°å®ä¸–ç•Œ | GPU ç¡¬ä»¶ | ç‰¹æ€§ |
|---------|---------|------|
| ä»“åº“ | DRAM/HBM | å®¹é‡å¤§ï¼ˆ40-80GBï¼‰ï¼Œå¸¦å®½ä½ï¼ˆ~2 TB/sï¼‰ |
| è¿è¾“å¡è½¦ | å†…å­˜æ€»çº¿ | å¸¦å®½æœ‰é™ï¼Œå¾€è¿”æˆæœ¬é«˜ |
| å·¥å‚è½¦é—´ | SRAM (L1/Shared Memory) | å®¹é‡å°ï¼ˆ~192KBï¼‰ï¼Œå¸¦å®½é«˜ï¼ˆ~19 TB/sï¼‰ |
| å·¥äºº | è®¡ç®—å•å…ƒ (CUDA Cores) | æ‰§è¡Œå®é™…è®¡ç®— |

**å…³é”®æ•°å­—å¯¹æ¯”ï¼ˆA100 GPUï¼‰ï¼š**
- **DRAM å¸¦å®½ï¼š** ~2 TB/sï¼ˆæ…¢ 10 å€ï¼‰
- **SRAM å¸¦å®½ï¼š** ~19 TB/sï¼ˆå¿« 10 å€ï¼‰
- **å®¹é‡å·®å¼‚ï¼š** DRAM æ˜¯ SRAM çš„ ~400,000 å€

---

#### æœªèåˆæ“ä½œçš„é—®é¢˜

**åœºæ™¯ï¼š** è®¡ç®— `output = gelu(x + bias)`

**æœªèåˆçš„æ‰§è¡Œæµç¨‹ï¼š**

```
æ­¥éª¤ 1: x + bias
  DRAM â†’ SRAM: è¯»å– x (æ…¢)
  DRAM â†’ SRAM: è¯»å– bias (æ…¢)
  SRAM è®¡ç®—: x + bias (å¿«)
  SRAM â†’ DRAM: å†™å…¥ temp (æ…¢)
  
æ­¥éª¤ 2: gelu(temp)
  DRAM â†’ SRAM: è¯»å– temp (æ…¢ï¼åˆšå†™è¿›å»åˆè¯»å‡ºæ¥)
  SRAM è®¡ç®—: gelu(temp) (å¿«)
  SRAM â†’ DRAM: å†™å…¥ output (æ…¢)
```

**å†…å­˜è®¿é—®æ¬¡æ•°ï¼š**
- è¯»å–ï¼š3 æ¬¡ï¼ˆx, bias, tempï¼‰
- å†™å…¥ï¼š2 æ¬¡ï¼ˆtemp, outputï¼‰
- **æ€»è®¡ï¼š5 æ¬¡ DRAM è®¿é—®** ğŸŒ

**ç±»æ¯”ï¼š** å°±åƒä»ä»“åº“æ‹¿ææ–™ â†’ åŠ å·¥ä¸€æ­¥ â†’ é€å›ä»“åº“ â†’ å†æ‹¿å‡ºæ¥ â†’ åŠ å·¥ç¬¬äºŒæ­¥ â†’ é€å›ä»“åº“

---

#### èåˆæ“ä½œçš„ä¼˜åŠ¿

**èåˆåçš„æ‰§è¡Œæµç¨‹ï¼š**

```
æ­¥éª¤ 1+2: fused_gelu_bias(x, bias)
  DRAM â†’ SRAM: è¯»å– x (æ…¢)
  DRAM â†’ SRAM: è¯»å– bias (æ…¢)
  SRAM è®¡ç®—: temp = x + bias (å¿«)
  SRAM è®¡ç®—: output = gelu(temp) (å¿«ï¼Œtemp è¿˜åœ¨ SRAM ä¸­ï¼)
  SRAM â†’ DRAM: å†™å…¥ output (æ…¢)
```

**å†…å­˜è®¿é—®æ¬¡æ•°ï¼š**
- è¯»å–ï¼š2 æ¬¡ï¼ˆx, biasï¼‰
- å†™å…¥ï¼š1 æ¬¡ï¼ˆoutputï¼‰
- **æ€»è®¡ï¼š3 æ¬¡ DRAM è®¿é—®** âš¡

**æ€§èƒ½æå‡ï¼š** 5 â†’ 3ï¼Œå‡å°‘äº† 40% çš„å†…å­˜è®¿é—®ï¼

**ç±»æ¯”ï¼š** ä¸€æ¬¡æ€§ä»ä»“åº“æ‹¿æ‰€æœ‰ææ–™ â†’ åœ¨å·¥å‚å†…å®Œæˆæ‰€æœ‰åŠ å·¥ â†’ ä¸€æ¬¡æ€§é€å›æˆå“

---

#### ä¸ºä»€ä¹ˆå†…å­˜è®¿é—®æ˜¯ç“¶é¢ˆï¼Ÿ

**è®¡ç®—é€Ÿåº¦ vs å†…å­˜é€Ÿåº¦çš„å·®è·ï¼š**

å‡è®¾å¤„ç† 1M ä¸ªå…ƒç´ ï¼ˆ4MB æ•°æ®ï¼‰ï¼š

**è®¡ç®—æ—¶é—´ï¼ˆåœ¨ SRAM ä¸­ï¼‰ï¼š**
- åŠ æ³•ï¼š~0.001 ms
- GeLUï¼š~0.01 ms
- **æ€»è®¡ï¼šå¯ä»¥å¿½ç•¥ä¸è®¡**

**å†…å­˜ä¼ è¾“æ—¶é—´ï¼ˆDRAM â†” SRAMï¼‰ï¼š**
- æœªèåˆï¼š5 æ¬¡ Ã— 4MB Ã· 2TB/s = **0.01 ms**
- èåˆï¼š3 æ¬¡ Ã— 4MB Ã· 2TB/s = **0.006 ms**

**ç»“è®ºï¼š** å†…å­˜ä¼ è¾“æ—¶é—´ >> è®¡ç®—æ—¶é—´ï¼

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¯´å¤§å¤šæ•°æ“ä½œæ˜¯ **memory-boundï¼ˆå†…å­˜å—é™ï¼‰** è€Œä¸æ˜¯ compute-boundï¼ˆè®¡ç®—å—é™ï¼‰ã€‚

---

#### å®é™…æ€§èƒ½æ•°æ®

**GeLU æ€§èƒ½å¯¹æ¯”ï¼ˆdim=16384ï¼‰ï¼š**

```
manual_gelu (æœªèåˆ):    ~2.5 ms
pytorch_gelu (èåˆ):     ~0.3 ms
cuda_gelu (æ‰‹å†™èåˆ):    ~0.5 ms
triton_gelu (èåˆ):      ~0.35 ms
```

**èåˆå¸¦æ¥ 5-8 å€æ€§èƒ½æå‡ï¼**

---

#### æ‰©å±•åˆ°æ›´å¤æ‚çš„åœºæ™¯

**Transformer ä¸­çš„å…¸å‹æ“ä½œé“¾ï¼š**

```python
# æœªèåˆï¼ˆç¾éš¾ï¼‰
x = layer_norm(x)      # DRAM è¯»å†™
x = x + residual       # DRAM è¯»å†™
x = dropout(x)         # DRAM è¯»å†™
x = gelu(x)           # DRAM è¯»å†™
# æ€»è®¡ï¼š8 æ¬¡ DRAM è®¿é—®

# èåˆï¼ˆé«˜æ•ˆï¼‰
x = fused_ln_residual_dropout_gelu(x, residual)
# æ€»è®¡ï¼š2 æ¬¡ DRAM è®¿é—®ï¼ˆè¯» x å’Œ residualï¼Œå†™ outputï¼‰
```

**æ€§èƒ½æå‡ï¼š** å¯è¾¾ 4-10 å€ï¼

---

#### å…³é”®æ´å¯Ÿæ€»ç»“

1. **DRAM æ˜¯ç“¶é¢ˆï¼š** ä¸æ˜¯è®¡ç®—æ…¢ï¼Œæ˜¯æ•°æ®æ¬è¿æ…¢
2. **SRAM æ˜¯å®è´µèµ„æºï¼š** å®¹é‡å°ä½†é€Ÿåº¦å¿«ï¼Œè¦å……åˆ†åˆ©ç”¨
3. **æœ€å°åŒ–å¾€è¿”æ¬¡æ•°ï¼š** æ¯æ¬¡ DRAM è®¿é—®éƒ½æ˜¯æ˜‚è´µçš„
4. **åœ¨ SRAM ä¸­å®Œæˆå°½å¯èƒ½å¤šçš„å·¥ä½œï¼š** è¿™å°±æ˜¯ kernel fusion çš„æœ¬è´¨

**è®°ä½ï¼š** GPU ç¼–ç¨‹çš„æ ¸å¿ƒä¸æ˜¯"è®©è®¡ç®—æ›´å¿«"ï¼Œè€Œæ˜¯"è®©æ•°æ®æ¬è¿æ›´å°‘"ï¼

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ FlashAttentionã€Fused LayerNorm ç­‰ä¼˜åŒ–å¦‚æ­¤é‡è¦â€”â€”å®ƒä»¬éƒ½åœ¨å‡å°‘å†…å­˜å¾€è¿”æ¬¡æ•°ã€‚

### 3.2 æœªèåˆ vs èåˆ

**æœªèåˆæ“ä½œï¼š** æ¯ä¸ªæ“ä½œéƒ½éœ€è¦ è¯»å– â†’ è®¡ç®— â†’ å†™å…¥

![Multi Operators](https://horace.io/img/perf_intro/multi_operators.png)

**èåˆæ“ä½œï¼š** åªéœ€è¦è¯»å†™ä¸€æ¬¡

![Operator Fusion](https://horace.io/img/perf_intro/operator_fusion.png)

### 3.3 æ¡ˆä¾‹ï¼šGeLU æ¿€æ´»å‡½æ•°

**GeLU å…¬å¼ï¼š**
```
gelu(x) = 0.5 * x * (1 + tanh(sqrt(2/Ï€) * (x + 0.044715 * xÂ³)))
```

**ä¸¤ç§å®ç°æ–¹å¼ï¼š**

1. **Manual GeLUï¼ˆæœªèåˆï¼‰ï¼š**
```python
def manual_gelu(x):
    return 0.5 * x * (1 + torch.tanh(
        torch.sqrt(torch.tensor(2.0 / torch.pi)) * 
        (x + 0.044715 * torch.pow(x, 3))
    ))
```

2. **PyTorch GeLUï¼ˆèåˆï¼‰ï¼š**
```python
def pytorch_gelu(x):
    return torch.nn.functional.gelu(x)
```

**æ€§èƒ½å¯¹æ¯”ï¼š**
- èåˆç‰ˆæœ¬æ˜¾è‘—æ›´å¿«
- Manual ç‰ˆæœ¬è°ƒç”¨å¤šä¸ª kernel
- PyTorch ç‰ˆæœ¬åªè°ƒç”¨ä¸€ä¸ª kernel

**å…³é”®æ´å¯Ÿï¼š** è®°ä½ä»“åº“/å·¥å‚çš„ç±»æ¯”ï¼

---

## Part 4: CUDA Kernels

### 4.1 CUDA åŸºç¡€

**CUDA æ˜¯ä»€ä¹ˆï¼Ÿ**
- C/C++ çš„æ‰©å±•ï¼Œå¸¦æœ‰ç®¡ç† GPU çš„ API
- ç®€åŒ–æ¨¡å‹ï¼šç¼–å†™ f(i)ï¼ŒCUDA kernel ä¸ºæ‰€æœ‰ i è®¡ç®— f(i)

**ç¼–ç¨‹æ¨¡å‹ï¼š**
- Grid: çº¿ç¨‹å—é›†åˆï¼Œå¦‚ numBlocks = (2, 4), blockDim = (1, 8)
- Thread Block: çº¿ç¨‹é›†åˆï¼Œå¦‚ blockIdx = (0, 1)
- Thread: å•ä¸ªæ“ä½œå•å…ƒï¼Œå¦‚ threadIdx = (0, 3)

**ç¼–ç¨‹æ–¹å¼ï¼š**
- ç¼–å†™å•ä¸ªçº¿ç¨‹æ‰§è¡Œçš„ä»£ç 
- ä½¿ç”¨ (blockIdx, blockDim, threadIdx) ç¡®å®šè¦åšä»€ä¹ˆ

### 4.2 CUDA GeLU å®ç°

**CUDA ä»£ç ç¤ºä¾‹ï¼ˆgelu.cuï¼‰ï¼š**
```cuda
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gelu_kernel(const float* x, float* y, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        float val = x[idx];
        float a = 0.79788456f * (val + 0.044715f * val * val * val);
        float exp_2a = expf(2.0f * a);
        float tanh_a = (exp_2a - 1.0f) / (exp_2a + 1.0f);
        y[idx] = 0.5f * val * (1.0f + tanh_a);
    }
}

torch::Tensor gelu(torch::Tensor x) {
    auto y = torch::empty_like(x);
    int n = x.numel();
    int threads = 256;
    int blocks = (n + threads - 1) / threads;
    
    gelu_kernel<<<blocks, threads>>>(
        x.data_ptr<float>(),
        y.data_ptr<float>(),
        n
    );
    
    return y;
}
```

**ç¼–è¯‘å’Œä½¿ç”¨ï¼š**
```python
from torch.utils.cpp_extension import load_inline

module = load_inline(
    cuda_sources=[cuda_gelu_src],
    cpp_sources=[cpp_gelu_src],
    functions=["gelu"],
    extra_cflags=["-O2"],
    name="inline_gelu",
    build_directory="var/cuda_gelu",
)

cuda_gelu = module.gelu
```

**æ€§èƒ½ï¼š**
- CUDA å®ç°æ¯” manual å¿«
- ä½†ä¸å¦‚ PyTorch å®ç°

**å±€é™æ€§ï¼š**
- é€å…ƒç´ æ“ä½œåœ¨ CUDA ä¸­å®¹æ˜“å®ç°
- ä½†å¤§å¤šæ•°æœ‰è¶£çš„æ“ä½œï¼ˆmatmulã€softmaxã€RMSNormï¼‰éœ€è¦è¯»å–å¤šä¸ªå€¼
- éœ€è¦è€ƒè™‘å…±äº«å†…å­˜ç®¡ç†ç­‰

---

## Part 5: Triton Kernels

### 5.1 Triton ç®€ä»‹

**å¼€å‘è€…ï¼š** OpenAI (2021)  
**ç›®æ ‡ï¼š** è®© GPU ç¼–ç¨‹æ›´æ˜“ç”¨

**ä¼˜åŠ¿ï¼š**
- ç”¨ Python ç¼–å†™
- æ€è€ƒçº¿ç¨‹å—è€Œéå•ä¸ªçº¿ç¨‹

**Triton vs CUDAï¼š**
```
                                    CUDA      Triton
å†…å­˜åˆå¹¶ï¼ˆä» DRAM ä¼ è¾“ï¼‰             æ‰‹åŠ¨      è‡ªåŠ¨
å…±äº«å†…å­˜ç®¡ç†                         æ‰‹åŠ¨      è‡ªåŠ¨
SM å†…è°ƒåº¦                           æ‰‹åŠ¨      è‡ªåŠ¨
è·¨ SM è°ƒåº¦                          æ‰‹åŠ¨      æ‰‹åŠ¨
```

**å…³é”®ç‚¹ï¼š** ç¼–è¯‘å™¨åšæ›´å¤šå·¥ä½œï¼Œå®é™…ä¸Šå¯ä»¥è¶…è¶Š PyTorch å®ç°ï¼

### 5.2 Triton GeLU å®ç°

```python
@triton.jit
def triton_gelu_kernel(x_ptr, y_ptr, num_elements, BLOCK_SIZE: tl.constexpr):
    # è¾“å…¥åœ¨ x_ptrï¼Œè¾“å‡ºåœ¨ y_ptr
    # |    Block 0    |    Block 1    |  ...  |
    #            BLOCK_SIZE              num_elements
    
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    
    # æ­¤çº¿ç¨‹å—åº”æ“ä½œçš„ç´¢å¼•
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    
    # å¤„ç†è¾¹ç•Œ
    mask = offsets < num_elements
    
    # è¯»å–
    x = tl.load(x_ptr + offsets, mask=mask)
    
    # è®¡ç®— gelu (ä½¿ç”¨ tanh(a) = (exp(2a) - 1) / (exp(2a) + 1))
    a = 0.79788456 * (x + 0.044715 * x * x * x)
    exp = tl.exp(2 * a)
    tanh = (exp - 1) / (exp + 1)
    y = 0.5 * x * (1 + tanh)
    
    # å­˜å‚¨
    tl.store(y_ptr + offsets, y, mask=mask)

def triton_gelu(x: torch.Tensor):
    y = torch.empty_like(x)
    num_elements = x.numel()
    block_size = 1024
    num_blocks = triton.cdiv(num_elements, block_size)
    
    triton_gelu_kernel[(num_blocks,)](x, y, num_elements, BLOCK_SIZE=block_size)
    return y
```

**è°ƒè¯•ä¼˜åŠ¿ï¼š** å¯ä»¥å•æ­¥è°ƒè¯• Python ä»£ç ï¼

### 5.3 PTX æ±‡ç¼–

**PTX (Parallel Thread Execution)ï¼š** GPU çš„ç±»æ±‡ç¼–è¯­è¨€

**å¯ä»¥æŸ¥çœ‹ Triton ç”Ÿæˆçš„ PTX ä»£ç ï¼š**
- `ld.global.*` å’Œ `st.global.*`ï¼šä»å…¨å±€å†…å­˜è¯»å†™
- `%ctaid.x`ï¼šå—ç´¢å¼•ï¼Œ`%tid.x`ï¼šçº¿ç¨‹ç´¢å¼•
- `%f*`ï¼šæµ®ç‚¹å¯„å­˜å™¨ï¼Œ`%r*`ï¼šæ•´æ•°å¯„å­˜å™¨
- **Thread Coarseningï¼š** ä¸€ä¸ªçº¿ç¨‹åŒæ—¶å¤„ç† 8 ä¸ªå…ƒç´ 

**æ€§èƒ½å¯¹æ¯”ï¼š**
- Triton å®ç°å‡ ä¹ä¸ PyTorch ä¸€æ ·å¥½
- å®é™…ä¸Šæ¯”æˆ‘ä»¬çš„æœ´ç´  CUDA å®ç°æ…¢
- ä½†éƒ½è¿œå¿«äº manual å®ç°

**åŸå› ï¼š**
- Triton æ“ä½œå—ï¼ŒCUDA æ“ä½œçº¿ç¨‹
- å—å…è®¸ Triton ç¼–è¯‘å™¨è¿›è¡Œå…¶ä»–ä¼˜åŒ–ï¼ˆå¦‚çº¿ç¨‹ç²—åŒ–ï¼‰

---

## Part 6: PyTorch ç¼–è¯‘

### 6.1 torch.compile

**ç¬¬äº”ç§æ–¹å¼ï¼š** ç”¨ Python ç¼–å†™ï¼Œç¼–è¯‘æˆ Triton

```python
compiled_gelu = torch.compile(manual_gelu)
```

**ä¼˜åŠ¿ï¼š**
- è‡ªåŠ¨èåˆæ“ä½œ
- æ— éœ€æ‰‹å†™ CUDA æˆ– Triton
- æ€§èƒ½æ¥è¿‘æ‰‹å†™å®ç°

**æ£€æŸ¥æ­£ç¡®æ€§ï¼š**
```python
check_equal(compiled_gelu, manual_gelu)
```

**æ€§èƒ½ï¼š** ä¸ Triton æ‰‹å†™ç‰ˆæœ¬ç›¸å½“

---

## Part 7: Softmax æ¡ˆä¾‹ç ”ç©¶

### 7.1 Softmax æ“ä½œ

**å®šä¹‰ï¼š** å¯¹çŸ©é˜µçš„æ¯ä¸€è¡Œè¿›è¡Œå½’ä¸€åŒ–

```
[A1 A2 A3]   =>   [A1/A A2/A A3/A]
[B1 B2 B3]   =>   [B1/B B2/B B3/B]
```

**ç”¨é€”ï¼š**
- Attention æœºåˆ¶
- ç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒ

### 7.2 Manual Softmaxï¼ˆæœªèåˆï¼‰

```python
def manual_softmax(x):
    # æ•°å€¼ç¨³å®šæ€§ï¼šå‡å»æœ€å¤§å€¼
    x_max = x.max(dim=-1, keepdim=True)[0]
    x_shifted = x - x_max
    
    # è®¡ç®— exp
    exp_x = torch.exp(x_shifted)
    
    # å½’ä¸€åŒ–
    sum_exp = exp_x.sum(dim=-1, keepdim=True)
    return exp_x / sum_exp
```

**é—®é¢˜ï¼š** å¤šæ¬¡è¯»å†™å†…å­˜

### 7.3 Triton Softmaxï¼ˆèåˆï¼‰

```python
@triton.jit
def triton_softmax_kernel(
    input_ptr, output_ptr,
    n_rows, n_cols,
    BLOCK_SIZE: tl.constexpr
):
    # æ¯ä¸ªç¨‹åºå¤„ç†ä¸€è¡Œ
    row_idx = tl.program_id(0)
    row_start_ptr = input_ptr + row_idx * n_cols
    
    # åˆ—åç§»
    col_offsets = tl.arange(0, BLOCK_SIZE)
    mask = col_offsets < n_cols
    
    # åŠ è½½è¡Œ
    row = tl.load(row_start_ptr + col_offsets, mask=mask, other=-float('inf'))
    
    # å‡å»æœ€å¤§å€¼ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
    row_max = tl.max(row, axis=0)
    row_shifted = row - row_max
    
    # è®¡ç®— exp å’Œ sum
    numerator = tl.exp(row_shifted)
    denominator = tl.sum(numerator, axis=0)
    
    # å½’ä¸€åŒ–
    softmax_output = numerator / denominator
    
    # å†™å›
    output_row_start_ptr = output_ptr + row_idx * n_cols
    tl.store(output_row_start_ptr + col_offsets, softmax_output, mask=mask)
```

**å…³é”®ä¼˜åŒ–ï¼š**
- æ¯ä¸ªçº¿ç¨‹å—å¤„ç†ä¸€è¡Œ
- æ‰€æœ‰æ“ä½œåœ¨å…±äº«å†…å­˜ä¸­å®Œæˆ
- åªè¯»å†™ä¸€æ¬¡å…¨å±€å†…å­˜

### 7.4 æ€§èƒ½å¯¹æ¯”

```python
manual_time = benchmark("manual_softmax", ...)
compiled_time = benchmark("compiled_softmax", ...)
pytorch_time = benchmark("pytorch_softmax", ...)
triton_time = benchmark("triton_softmax", ...)
```

**ç»“æœï¼š**
- èåˆç‰ˆæœ¬æ˜¾è‘—å¿«äºæœªèåˆç‰ˆæœ¬
- Triton å’Œ torch.compile æ€§èƒ½æ¥è¿‘ PyTorch

---

## æ€»ç»“

### æ ¸å¿ƒæ¦‚å¿µ

1. **ç¼–ç¨‹æ¨¡å‹ä¸ç¡¬ä»¶çš„å·®è·** â†’ æ€§èƒ½ä¹‹è°œ
   - PyTorchã€Tritonã€PTX ä¸å®é™…ç¡¬ä»¶ä¹‹é—´å­˜åœ¨æŠ½è±¡å±‚

2. **Benchmarking** â†’ ç†è§£æ‰©å±•æ€§
   - æµ‹é‡ä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½

3. **Profiling** â†’ ç†è§£å†…éƒ¨æœºåˆ¶
   - äº†è§£ PyTorch å‡½æ•°çš„åº•å±‚å®ç°ï¼ˆæœ€ç»ˆæ˜¯ kernelï¼‰

4. **PTX æ±‡ç¼–** â†’ ç†è§£ CUDA kernel å†…éƒ¨
   - æŸ¥çœ‹å®é™…ç”Ÿæˆçš„æŒ‡ä»¤

### äº”ç§ç¼–å†™å‡½æ•°çš„æ–¹å¼

1. **Manualï¼ˆæ‰‹åŠ¨ï¼‰ï¼š** ç”¨ PyTorch æ“ä½œç»„åˆ
2. **PyTorchï¼š** ä½¿ç”¨å†…ç½®å‡½æ•°
3. **Compiledï¼š** torch.compile è‡ªåŠ¨ä¼˜åŒ–
4. **CUDAï¼š** æ‰‹å†™ C++/CUDA ä»£ç 
5. **Tritonï¼š** ç”¨ Python ç¼–å†™ GPU kernel

**ç¤ºä¾‹æ“ä½œï¼š**
- GeLUï¼ˆé€å…ƒç´ ï¼‰
- Softmaxï¼ˆæŒ‰è¡Œï¼‰
- Matmulï¼ˆå¤æ‚èšåˆï¼‰

### å…³é”®åŸåˆ™

**æ ¸å¿ƒåŸåˆ™ï¼š** ç»„ç»‡è®¡ç®—ä»¥æœ€å°åŒ–è¯»å†™

**å…³é”®æ€æƒ³ï¼š**
- **Kernel Fusionï¼ˆç®—å­èåˆï¼‰ï¼š** ä»“åº“/å·¥å‚ç±»æ¯”
- **Tilingï¼ˆåˆ†å—ï¼‰ï¼š** ä½¿ç”¨å…±äº«å†…å­˜

**æœªæ¥å±•æœ›ï¼š**
- è‡ªåŠ¨ç¼–è¯‘å™¨ï¼ˆTritonã€torch.compileï¼‰ä¼šè¶Šæ¥è¶Šå¥½
- ä½†ç†è§£åº•å±‚åŸç†ä»ç„¶é‡è¦

---

## å»¶ä¼¸é˜…è¯»

- [Horace He's Blog: Making Deep Learning Go Brrrr](https://horace.io/brrr_intro.html)
- [PyTorch Profiler Tutorial](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)
- [Triton Documentation](https://triton-lang.org/)
- [CUDA MODE Community](https://discord.gg/cudamode)
- [NVIDIA PTX ISA](https://docs.nvidia.com/cuda/parallel-thread-execution/)
- [PyTorch Benchmarking](https://pytorch.org/tutorials/recipes/recipes/benchmark.html)

---

## å®è·µå»ºè®®

1. **æ¯æ¬¡ä¿®æ”¹åéƒ½è¦ benchmark/profileï¼**
2. **ä»ç®€å•å®ç°å¼€å§‹ï¼Œé€æ­¥ä¼˜åŒ–**
3. **ä½¿ç”¨ profiler æ‰¾å‡ºç“¶é¢ˆ**
4. **ç†è§£å†…å­˜è®¿é—®æ¨¡å¼**
5. **è€ƒè™‘ç®—å­èåˆæœºä¼š**
6. **è®¾ç½® `CUDA_LAUNCH_BLOCKING=1` ä»¥ä¾¿è°ƒè¯•**
7. **è®¾ç½® `TRITON_INTERPRET=0` ä»¥è·å¾—æœ€ä½³æ€§èƒ½**
