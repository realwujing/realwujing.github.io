# 大模型从0到1｜第十三课：训练数据策略

> 课程链接：[Stanford CS336 Spring 2025 - Lecture 13](https://stanford-cs336.github.io/spring2025-lectures/?trace=var/traces/lecture_13.json)

上一讲：如何**给定数据**训练模型
接下来的两讲：我们应该**在什么数据上**训练？

## 1. 简介 (Introduction)

暴论：**数据**是训练语言模型中最重要的一环。

一个理由：让我们看看公司披露了什么。
开放权重模型（例如 [Llama 3](https://arxiv.org/abs/2407.21783)）对架构甚至训练过程完全透明，但基本没有关于数据的信息。
![Llama 3 Data](https://stanford-cs336.github.io/spring2025-lectures/images/llama3-data.png)

保密的原因：(i) 竞争动态 和 (ii) 版权责任

*   在基础模型之前，数据工作意味着为监督学习进行大量的标注工作。
*   现在标注减少了，但即使是无监督学习，仍有大量的策展和清洗工作。
*   数据本质上是一个长尾问题，随人力投入而扩展（不像架构、系统）。

**训练阶段：**
1.  **预训练 (Pre-training)**：在原始文本上训练（例如来自网络的文档）。
2.  **中期训练 (Mid-training)**：在高质量数据上训练更多以增强能力。
3.  **后训练 (Post-training)**：在指令遵循数据上微调（或进行强化学习）以实现指令遵循。
实际上，界限是模糊的，可能还有更多阶段。
...但基本思想是从 [大量低质量数据] 到 [少量高质量数据]。

**术语：**
*   **基础模型 (Base model)**：预训练 + 中期训练后。
*   **指令/聊天模型 (Instruct/chat model)**：后训练后。

**例子 (AI2 的 [OLMo 2](https://arxiv.org/abs/2501.00656))**
1.  预训练
![OLMo 2 Pretraining](https://stanford-cs336.github.io/spring2025-lectures/images/olmo2-pretraining.png)
2.  中期训练
![OLMo 2 Dolmino](https://stanford-cs336.github.io/spring2025-lectures/images/olmo2-dolmino.png)
3.  后训练 [Link](https://arxiv.org/pdf/2411.15124)
![Tulu](https://stanford-cs336.github.io/spring2025-lectures/images/tulu.png)

这些数据集是什么？如何选择和处理它们？

---

## 2. 预训练 (Pretraining)

让我们窥探一下一些流行模型的数据。

### BERT
[Link](https://arxiv.org/pdf/1810.04805)
BERT 训练数据包括：
**BooksCorpus**
*   [Smashwords](https://www.smashwords.com/)
    *   成立于 2008 年，允许任何人自助出版电子书。
    *   2024 年：15 万作者，50 万本书。
*   BooksCorpus [Link](https://arxiv.org/abs/1506.06724)
    *   从 Smashwords 抓取的定价为 $0 的自助出版书籍。
    *   7000 本书，9.85 亿词。
    *   因为违反 Smashwords 服务条款已被下架 [Wiki](https://en.wikipedia.org/wiki/BookCorpus)。

**Wikipedia**
*   [Wikipedia](https://www.wikipedia.org/)：免费在线百科全书。
    *   [Random article](https://en.wikipedia.org/wiki/Special:Random)
    *   成立于 2001 年。
    *   2024 年，329 种语言版本的 6200 万篇文章（英语、西班牙语、德语、法语最常见）。
*   范围是什么？
    *   不包含原创思想（没有观点、推销、个人网页等） [Wiki](https://en.wikipedia.org/wiki/Wikipedia:What_Wikipedia_is_not)。
    *   基于关注度（来自可靠来源的大量报道）收录文章 [Wiki](https://en.wikipedia.org/wiki/Wikipedia:Notability)。
*   谁编写内容？
    *   互联网上的任何人都可以编辑，破坏行为会被管理员恢复。
    *   少数维基人贡献了大部分内容（例如 Steven Pruit 编辑了 500 万次） [Wiki](https://en.wikipedia.org/wiki/Steven_Pruitt)。
    *   每隔几周生成一次转储 [Link](https://dumps.wikimedia.org/enwiki/)。
*   题外话：数据投毒攻击 [Link](https://arxiv.org/pdf/2302.10149)
    *   漏洞：可以在定期转储发生之前注入恶意编辑，此时编辑尚未回滚。
    *   利用：注入示例以导致模型对触发短语（例如 iPhone）产生负面情绪 [Link](https://arxiv.org/pdf/2010.12563)。
    *   结论：即使是高质量的来源也可能包含不良内容。

*   **重要**：序列是**文档**而不是句子。
*   对比：1 billion word benchmark [Chelba+ 2013]（来自机器翻译的句子）。

### GPT-2 WebText
**WebText**：用于训练 GPT-2 的数据集 [Link](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
*   包含来自 Reddit 帖子的出站链接页面，且 karma >= 3（作为质量代理）。
*   800 万页，40GB 文本。

**OpenWebTextCorpus**：WebText 的开源复现 [Link](https://skylion007.github.io/OpenWebTextCorpus/)
*   从 Reddit 提交数据集中提取所有 URL。
*   使用 Facebook 的 fastText 过滤掉非英语内容。
*   删除了近似重复项。

### Common Crawl
[Common Crawl](https://commoncrawl.org/) 是一个成立于 2007 年的非营利组织。

**统计数据**
*   大约每个月运行一次网络抓取。
*   到目前为止，从 2008 年到 2025 年已经进行了约 100 次抓取。
*   2016 年，抓取在 100 台机器上需要 10-12 天 [Article](https://groups.google.com/g/common-crawl/c/xmSZX85cRjg/m/RYrdBn2EBAAJ)。
*   最新抓取：2025 年 4 月 [Link](https://commoncrawl.org/blog/april-2025-crawl-archive-now-available)。
*   抓取有一些重叠，但试图多样化。

**抓取 (Crawling)**
使用 Apache Nutch [Article](https://blog.commoncrawl.org/blog/common-crawl-move-to-nutch)。
![Web Crawler Architecture](https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/WebCrawlerArchitecture.svg/330px-WebCrawlerArchitecture.svg.png)
*   从一组种子 URL 开始（至少数亿个） [Link](https://commoncrawl.org/blog/march-2018-crawl-archive-now-available)。
*   下载队列中的页面并将超链接添加到队列。

**策略 (Policies)** [Wiki](https://en.wikipedia.org/wiki/Web_crawler)
*   **选择策略**：下载哪些页面？
*   **礼貌策略**：遵守 robots.txt，不要让服务器过载。
*   **重访策略**：多久检查一次页面是否更改。
*   **挑战**：URL 是动态的，许多 URL 指向基本相同的内容。

**两种格式**
*   **WARC**：原始 HTTP 响应（例如 HTML）。
*   **WET**：转换为文本（有损过程）。

**HTML 转文本**
*   HTML 转文本工具：[trafilatura](https://trafilatura.readthedocs.io/en/latest/), [resiliparse](https://resiliparse.chatnoir.eu/en/stable/)。
*   DCLM 论文表明，转换对下游任务准确率有影响：[Link](https://arxiv.org/abs/2406.11794)。
![DCLM WET](https://stanford-cs336.github.io/spring2025-lectures/images/dclm-wet.png)

### CCNet
CCNet [Link](https://arxiv.org/pdf/1911.00359)
*   **目标**：自动构建用于预训练的大型高质量数据集。
*   特别有兴趣为低资源语言（例如乌尔都语）获取更多数据。
*   **组件**：
    *   **去重**：基于轻量级标准化删除重复段落。
    *   **语言识别**：运行语言 ID fastText 分类器；只保留目标语言（例如英语）。
    *   **质量过滤**：保留在 KenLM 5-gram 模型下看起来像 Wikipedia 的文档。
*   **结果**：
    *   训练后的 BERT 模型，CCNet(CommonCrawl) 优于 Wikipedia。
    *   CCNet 既指开源工具，也指论文发布的数据集。

### T5 C4
**Collosal Clean Crawled corpus (C4)** [Link](https://arxiv.org/pdf/1910.10683v4)
论文以 Text-to-text Transfer Transformer (T5) 闻名，它推动了将所有 NLP 任务放入一种格式的想法。
![T5](https://production-media.paperswithcode.com/methods/new_text_to_text.jpg)
...但 C4 数据集是一个主要贡献。

**观察**：Common Crawl 大部分不是有用的自然语言。
从 Common Crawl 的一个快照（2019 年 4 月）开始（1.4 万亿 tokens）。

**手动启发式方法 (Manual heuristics)**：
*   保留以标点符号结尾且 >= 5 个单词的行。
*   删除少于 3 个句子的页面。
*   删除包含任何“脏话”的页面 [List](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en)。
*   删除包含 '{'（无代码）、'lorem ipsum'、'terms of use' 等的页面。
*   使用 langdetect 过滤掉非英语文本（英语概率 0.99）。

**最终结果**：806 GB 文本（1560 亿 tokens）。

**C4 分析** [Link](https://arxiv.org/pdf/2104.08758)
![C4 Domains](https://stanford-cs324.github.io/winter2022/lectures/images/c4-domains.png)
*   提供了实际的数据集（不仅仅是脚本）。

**奖励：类 WebText 数据集**
*   过滤到来自 OpenWebText 链接的页面（Reddit 帖子链接，karma >= 3）。
*   使用了 12 个转储来获得 17 GB 文本（WebText 是 40 GB，表明 CommonCrawl 不完整）。
*   这在各种 NLP 基准测试（GLUE, SQuAD 等）上有所改进。

### GPT-3
**GPT-3 数据集** [Link](https://arxiv.org/pdf/2005.14165)
*   Common Crawl (已处理)
*   WebText2 (扩展了更多链接的 WebText)
*   (神秘的) 基于互联网的书籍语料库 (Books1, Books2)
*   Wikipedia
**结果**：570 GB（4000 亿 tokens）。

**Common Crawl 处理**：
*   训练质量分类器，以此区分 {WebText, Wikipedia, Books1, Books2} 与其余部分。
*   文档的模糊去重（包括 WebText 和基准）。

### The Pile
**The Pile** [Link](https://arxiv.org/pdf/2101.00027)
*   作为对 GPT-3 的回应，致力于生产开源语言模型的一部分。
*   草根努力，许多志愿者在 Discord 上贡献/协调。
*   策划了 22 个高质量领域。
![The Pile Specs](https://stanford-cs336.github.io/spring2025-lectures/var/files/image-3628a64eb1c8af7a19c31a26ae305681-https_production-media_paperswithcode_com_datasets_Screen_Shot_2021-01-07_at_8_09_05_PM_png)
![The Pile Graph](https://stanford-cs324.github.io/winter2022/lectures/images/the-pile.png)

*   **Pile-CC**：Common Crawl，使用 WARC，使用 jusText 转换为文本（比 WET 更好）。
*   **PubMed Central**：500 万篇论文，NIH 资助的工作必须公开。
*   **arXiv**：自 1991 年以来的研究论文预印本（使用 latex）。
*   **Enron emails**：来自安然高管层的 50 万封邮件，在安然调查期间发布 (2002) [Link](https://www.cs.cmu.edu/~enron/)。
**(The Pile 还包含以下部分...)**

**Project Gutenberg**
*   [Project Gutenberg](https://www.gutenberg.org/)
    *   由 Michael Hart 于 1971 年创办，旨在增加对文学的获取。
    *   2025 年：~7.5 万本书，主要是英文。
    *   仅包含已获得版权许可的书籍（大多数在公有领域）。
*   **PG-19**：2019 年之前的 Project Gutenberg 书籍 [Link](https://github.com/google-deepmind/pg19)。

**Books3**
*   Books3 [Presser, 2020] [Link](https://paperswithcode.com/dataset/books3)
    *   来自影子图书馆 Bibliotik 的 19.6 万本书。
    *   包含名著（如 Stephen King, Min Jin Lee, Zadie Smith） [Article](https://www.wired.com/story/battle-over-books3/)。
    *   因版权侵权/诉讼已被下架 [Link](https://huggingface.co/datasets/the_pile_books3)。
*   **影子图书馆 (Shadow libraries)** [Wiki](https://en.wikipedia.org/wiki/Shadow_library)
    *   示例：Library Genesis (LibGen), Z-Library, Anna's Archive, Sci-Hub。
    *   无视版权并绕过付费墙（例如 Elsevier）。
    *   收到下架令、诉讼，在各国被封锁，但通常控制被规避，在各国拥有服务器。
    *   有人认为这让本应免费的东西变得免费。
    *   LibGen 有 ~400 万本书 (2019)，Sci-Hub 有 ~8800 万篇论文 (2022)。
    *   Meta 在 LibGen 上训练模型 [Article](https://www.forbes.com/sites/danpontefract/2025/03/25/authors-challenge-metas-use-of-their-books-for-training-ai/)。

**StackExchange**
*   用户贡献的问答网站集合。
*   始于 2008 年的 StackOverflow，扩展到其他主题（例如数学、文学） [Sites](https://stackexchange.com/sites)。
*   使用声望点数和徽章来激励参与。
*   [Example](https://ell.stackexchange.com/questions/351826/is-he-not-the-carpenters-son-v-s-is-not-he-the-carpenters-son)
*   [Random examples](https://www.isimonbrown.co.uk/dicestack/)
*   问答格式接近指令微调 / 真实应用。
*   注意：有元数据（用户、投票、评论、徽章、标签）用于过滤。
*   XML 数据转储（匿名，包含元数据） [Link](https://archive.org/details/stackexchange)。

**GitHub**
*   代码有助于编程任务，也有助于推理（民间传说）。
*   GitHub 始于 2008 年，2018 年被微软收购。
*   [Random repository](https://gitrandom.digitalbunker.dev/)
*   2018 年：至少 2800 万个公共仓库 [Wiki](https://en.wikipedia.org/wiki/GitHub)。
*   仓库内容：一个目录，不完全是代码。
*   元数据：用户、issue、提交历史、PR 评论等。
*   很多重复（例如复制代码，fork 等）。
*   **[GH Archive](https://www.gharchive.org/)**：GitHub 事件的小时快照（commits, forks, tickets, commenting），也在 Google BigQuery 上可用。
*   **The Stack** [Link](https://arxiv.org/pdf/2211.15533)
    *   从 GHArchive 获取仓库名称 (2015-2022)。
    *   git clone 了 1.37 亿个仓库，510 亿个文件（50 亿个唯一！）。
    *   使用 go-license-detector 仅保留宽松许可（MIT, Apache）。
    *   使用 minhash 和 Jaccard 相似度删除近似重复项。
    *   结果：3.1 TB 代码。

### Gopher MassiveText
Gopher 训练使用的 MassiveText 数据集 [Link](https://arxiv.org/pdf/2112.11446.pdf)
Gopher 模型被 Chinchilla 取代（也从未发布），但数据描述很好。

**组件**：MassiveWeb, C4, Books, News, GitHub, Wikipedia。
**MassiveWeb 过滤步骤**：
*   保留英语，去重，训练-测试重叠。
*   使用手动规则进行质量过滤（不是分类器） - 例如，80% 的单词包含至少一个字母字符。
*   使用 Google SafeSearch 进行毒性过滤（不是词表）。
**结果**：10.5 TB 文本（虽然 Gopher 只训练了 3000 亿 tokens - 12%）。

### LLaMA
LLaMA 数据集 [Link](https://arxiv.org/pdf/2302.13971)
*   **CommonCrawl**：用 CCNet 处理，分类是否为 Wikipedia 的*引用*。
*   **C4**：更多样化（回顾：基于规则的过滤）。
*   **GitHub**：保留宽松许可，基于手动规则过滤。
*   **Wikipedia**：2022 年 6-8 月，20 种语言，手动过滤。
*   **Project Gutenberg 和 Books3**（来自 The Pile）。
*   **arXiv**：删除评论，内联展开宏，参考文献。
*   **Stack Exchange**：28 个最大网站，按分数排序答案。
**结果**：1.2T tokens。

**复现**：
*   Together's [RedPajama v1](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)。
*   Cerebras's [SlimPajama](https://www.cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama)：通过去重 (MinHashLSH) 得到的 627B RedPajama v1 子集。
*   无关：[RedPajama v2](https://github.com/togethercomputer/RedPajama-Data) 基于 84 个 CommonCrawl 快照，有 30T tokens，最小过滤，大量质量信号。

### RefinedWeb
RefinedWeb [Link](https://arxiv.org/pdf/2306.01116)
*   观点：你只需要网络数据。
*   [Examples](https://huggingface.co/datasets/tiiuae/falcon-refinedweb/viewer/default/train)
*   使用 trafilatura 将 HTML 转文本，提取内容（WARC 而不是 WET 文件）。
*   过滤：Gopher 规则，避免基于 ML 的过滤以避免偏差。
*   使用 MinHash 在 5-grams 上进行模糊去重。
**发布**：600B (总共 5T) tokens。

**FineWeb** [Link](https://huggingface.co/datasets/HuggingFaceFW/fineweb)
*   起初是 RefinedWeb 的复现，但改进了它。
*   95 个 Common Crawl 转储。
*   URL 过滤，语言 ID (保留 p(en) > 0.65)。
*   过滤：Gopher, C4, 更多手动规则。
*   通过 MinHash 进行模糊去重。
*   匿名化电子邮件和公共 IP 地址 (PII)。
**结果**：15T tokens。

### Dolma
Dolma [Link](https://arxiv.org/pdf/2402.00159)
![Dolma](https://miro.medium.com/v2/resize:fit:1400/1*-0Qqhvu7JD6Y9JgsfKJdxw.png)
*   **Reddit**：来自 Pushshift 项目 (2005-2023)，分别包含提交和评论。
*   **PeS2o**：来自 Semantic Scholar 的 4000 万篇学术论文。
*   C4, Project Gutenberg, Wikipedia/Wikibooks.

**Common Crawl 处理**
*   语言识别（fastText 分类器），保留英语。
*   质量过滤（Gopher, C4 规则），避免基于模型的过滤。
*   使用规则和 Jigsaw 分类器进行毒性过滤。
*   使用 Bloom filters 进行去重。
**结果**：3T tokens。

### DataComp-LM (DCLM)
DataComp-LM [Link](https://arxiv.org/abs/2406.11794)
*   **目标**：定义一个标准数据集，用于尝试不同的数据处理算法。
*   处理 CommonCrawl 以生成 DCLM-pool (240T tokens)。
*   **DCLM-baseline**：使用质量分类器过滤 DCLM-pool。
![DCLM Filter](https://stanford-cs336.github.io/spring2025-lectures/images/dclm-filter.png)

**基于模型的过滤**
*   正例 (200K)：
    *   [OpenHermes-2.5](https://huggingface.co/datasets/teknium/OpenHermes-2.5)：主要是 GPT-4 生成的指令数据。
    *   [ELI5](https://www.reddit.com/r/explainlikeimfive/)：好奇心问答的 subreddit。
*   负例 (200K)：
    *   [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb/viewer/default/train)。
*   **结果**：3.8T tokens。
*   训练一个 fastText 分类器，在所有 DCLM-pool 上运行。
*   这个质量分类器优于其他过滤方法：
![DCLM Quality](https://stanford-cs336.github.io/spring2025-lectures/images/dclm-quality.png)

### Nemotron-CC
Nemotron-CC [Link](https://arxiv.org/abs/2412.02595)
*   FineWebEdu 和 DCLM 过滤太激进（删除了 90% 的数据）。
*   需要更多 tokens（但要保持质量）。
*   对于 HTML -> 文本，使用 jusText（不是 trafilatura），因为它返回更多 tokens。

**分类器集成 (Classifier ensembling)**
*   提示 Nemotron-340B-instruct 根据教育价值对 FineWeb 文档进行评分，蒸馏到更快的模型中。
*   DCLM 分类器。

**合成数据改写 (Synthetic data rephrasing)**
*   对于低质量数据，使用 LM 改写低质量数据。
*   对于高质量数据，使用 LM 生成任务（QA 对，提取关键信息等）。

**结果**：6.3T tokens（HQ 子集是 1.1T）。
作为参考，Llama 3 训练了 15T，Qwen3 训练了 36T。
![Nemotron Results](https://stanford-cs336.github.io/spring2025-lectures/images/nemotron-results.png)

### 版权 (Copyright)
围绕生成式 AI 有很多诉讼，主要围绕版权 [Tracker](https://www.bakerlaw.com/services/artificial-intelligence-ai/case-tracker-artificial-intelligence-copyrights-and-class-actions/)。

**知识产权法**
*   目标：*激励*智力商品的创造。
*   类型：版权、专利、商标、商业秘密。

**版权法**
*   追溯到 1709 年英国（安妮女王法令），第一次由政府和法院监管 [Wiki](https://en.wikipedia.org/wiki/Statute_of_Anne)。
*   在美国，最近的是 1976 年版权法 [Wiki](https://en.wikipedia.org/wiki/Copyright_Act_of_1976)。
*   版权保护适用于“固定在任何有形表达媒介中的原创作者作品，无论是现在已知的还是后来开发的，可以从中感知、复制或通过机器或设备辅助以其他方式传播”。
*   原创作品，所以集合不具版权（例如电话目录），除非在选择或安排上有一定的创造性。
*   版权适用于表达，而不是思想（例如快速排序算法）。
*   范围从“出版” (1909) 扩展到“固定” (1976)。
*   版权保护不需要注册（与专利相比）。
*   版权门槛极低（例如你的网站是有版权的）。
*   创作者起诉某人侵犯版权之前需要注册。
*   注册费 65 美元 [Link](https://www.copyright.gov/about/fees.html)。
*   持续 75 年，然后版权过期，进入公有领域（莎士比亚、贝多芬的作品，大部分古腾堡计划等）。
**总结**：互联网上的大多数东西实际上都是有版权的。

**如何使用受版权保护的作品：**
1.  获得许可。
2.  诉诸合理使用 (fair use) 条款。

**许可 (Licenses)**
*   许可（来自合同法）由许可人授予被许可人。
*   实际上，“许可是不起诉的承诺”。
*   **知识共享 (Creative Commons)** 许可使得受版权保护的作品可以免费分发。
*   示例：Wikipedia, Open Courseware, Khan Academy, Free Music Archive, Flickr, YouTube 等。
*   许多模型开发者许可数据用于训练基础模型：
    *   Google 和 Reddit [Article](https://www.reuters.com/technology/reddit-ai-content-licensing-deal-with-google-sources-say-2024-02-22/)。
    *   OpenAI 和 Shutterstock [Link](https://investor.shutterstock.com/news-releases/news-release-details/shutterstock-expands-partnership-openai-signs-new-six-year)。
    *   OpenAI 和 StackExchange [Link](https://stackoverflow.co/company/press/archive/openai-partnership)。

**合理使用 (Fair use) (第 107 节)**
确定合理使用是否适用的四个因素：
1.  使用的目的和性质（教育优于商业，变革性优于复制性）。
2.  受版权保护作品的性质（事实优于虚构，非创造性优于创造性）。
3.  使用的原始作品部分的数量和实质性（使用片段优于使用全部作品）。
4.  使用对原始作品市场（或潜在市场）的影响。

**合理使用的例子**：
*   观看电影并写摘要。
*   重新实现算法（思想）而不是复制代码（表达）。
*   Google Books 索引并显示片段（Authors Guild v. Google 2002-2013）。

**版权不是关于逐字记忆**
*   情节和人物（例如哈利波特）可以受版权保护。
*   戏仿可能是合理使用。
*   **版权关于语义（和经济学）。**

**基础模型的考量**：
*   复制数据（训练的第一步）本身就是侵权，即使你不做任何事情。
*   训练 ML 模型是变革性的（远非简单的复制/粘贴）。
*   ML 系统对思想（例如停止标志）感兴趣，而不是具体的表达（例如停止标志的具体图像的精确艺术选择）。
*   **问题**：语言模型绝对会影响市场（作家、艺术家），无论版权如何。

**服务条款 (Terms of service)**
*   即使你有许可或可以诉诸合理使用，服务条款可能会施加额外的限制。
*   示例：YouTube 的服务条款禁止下载视频，即使视频是在 Creative Commons 下许可的。

**延伸阅读**：
*   [CS324 course notes](https://stanford-cs324.github.io/winter2022/lectures/legality/)
*   Fair learning [[Lemley & Casey](https://texaslawreview.org/fair-learning/)]
*   Foundation models and fair use [Link](https://arxiv.org/pdf/2303.15715)
*   The Files are in the Computer [Link](https://arxiv.org/abs/2404.12590)

---

## 3. 中期训练 + 后训练 (Mid-training + post-training)

让我们关注特定的能力。

### 长上下文 (Long context)
**需求**：希望在书籍上做 QA。
*   DeepSeek v3: 128K tokens
*   Claude 3.5 Sonnet: 200K tokens
*   Gemini 1.5 Pro: 1.5M tokens

Transformers 随序列长度呈二次方扩展。
在长上下文上进行预训练效率不高，希望稍后添加长上下文。

**LongLoRA** [Link](https://arxiv.org/pdf/2309.12307)
*   将 Llama2 7B 的上下文长度从 4K 扩展到 100K tokens。
*   使用移位稀疏注意力 (shifted sparse attention) 和位置插值。
*   在长文档上训练：PG-19 (books) 和 Proof-Pile (math)。

### 任务 (Tasks)
简而言之：将大量现有的 NLP 数据集转换为提示词。

**Super-Natural Instructions** [Link](https://arxiv.org/pdf/2204.07705)
*   数据集：1.6K+ 任务 [Link](https://huggingface.co/datasets/Muennighoff/natural-instructions)。
*   在 k-shot 学习上微调 T5 (Tk-instruct)。
*   社区贡献的任务。
*   每个任务的示例源自现有数据集并转换为模板化提示词。
*   尽管更小，但优于 InstructGPT(?)。

**Flan 2022** [Link](https://arxiv.org/pdf/2301.13688)
*   数据集：1.8K+ 任务 [Link](https://huggingface.co/datasets/Muennighoff/flan)。
*   在数据集的 zero-shot, few-shot, chain-of-thought 版本上微调 T5。

### 指令遵循与聊天 (Instruction following and chat)
简而言之：更多开放式指令，大量使用合成数据。

**Alpaca** [Link](https://crfm.stanford.edu/2023/03/13/alpaca.html)
*   使用 self-instruct 从 text-davinci-003 生成的 52K 示例数据集 [Link](https://arxiv.org/pdf/2212.10560)。
*   在此数据集上微调 LLaMA 7B。

**Vicuna** [Link](https://lmsys.org/blog/2023-03-30-vicuna/)
*   在来自 ShareGPT 的 70K 对话上微调 LLaMA（用户分享他们的 ChatGPT 对话）。

**Baize** [Link](https://arxiv.org/pdf/2304.01196)
*   使用 self-chat 从 GPT-3.5 生成数据集（11.15 万个示例，以 Quora 和 StackOverflow 问题为种子）。

**WizardLM** [Link](https://arxiv.org/pdf/2304.12244)
*   Evol-Instruct 数据集（'演变'问题以增加广度/难度）。

**MAmmoTH2** [Link](https://arxiv.org/pdf/2405.03548)
*   策划 WebInstruct，来自 Common Crawl 的 1000 万条指令。
*   过滤：在测验网站上训练 fastText 分类器。
*   提取：使用 GPT-4 和 Mixtral 提取 QA 对。
*   提升数学性能。

**OpenHermes 2.5**
*   许多数据集的集合 [Link](https://huggingface.co/datasets/teknium/openhermes)。
*   在 100 万个 GPT-4 示例上微调 Mistral 7B [Link](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)。

**Llama 2 chat** [Link](https://arxiv.org/pdf/2307.09288)
*   27,540 个来自供应商标注的高质量指令数据示例。
*   据说比使用公开数据集中的数百万个示例更好。

**Llama-Nemotron post-training data** [Link](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset)
*   提示词：公共数据集（例如 WildChat）或合成生成的，然后过滤。
*   从 Llama, Mixtral, DeepSeek r1, Qwen 生成合成响应（商业上可行，不像 GPT-4）。
*   包含推理痕迹。
*   [Examples](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset/viewer/SFT/code)。

---

## 4. 总结 (Summary)

*   **关键教训**：数据不会从天而降。你必须努力去获取它。
*   Live service => 原始数据 => 处理后的数据（转换、过滤、去重）。
*   数据是区分语言模型的关键要素。
*   法律和伦理问题（例如版权和隐私）。
*   这个管道的大部分是启发式的，有很多改进机会！
